{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# FinBERT Profiling: DistillBert\n",
        "\n",
        "This notebook is intentionally **thin**: it reuses the profiling utilities in `pipelines/finBERT/finbert/` (especially `finbert/finbert_profile.py` and `finbert/profile_utils.py`) instead of copying large code blocks.\n",
        "\n",
        "The purpose of this notebook is to profile finetuning Distilled Bert on financial sentiment analysis.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "from pathlib import Path\n",
        "import shutil\n",
        "import os\n",
        "import logging\n",
        "import sys\n",
        "sys.path.append('..')\n",
        "\n",
        "from textblob import TextBlob\n",
        "from pprint import pprint\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "from finbert.finbert import *\n",
        "from finbert.finbert_profile import *\n",
        "from finbert.profile_utils import get_model_size_mb, print_device_info, setup_nltk_data, timed_eval\n",
        "import finbert.utils as tools\n",
        "\n",
        "from finbert.distillFinBert import DistillFinBert\n",
        "\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "project_dir = Path.cwd().parent\n",
        "pd.set_option('max_colwidth', None)\n",
        "\n",
        "import wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "wandb.init(\n",
        "    entity=\"si2449-columbia-university\",\n",
        "    project=\"Project-Runs\",\n",
        "    name=\"finetuning-distilled_bert\",\n",
        "    group=\"knowledge-distillation\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cl_path = project_dir/'models'/'distillbert'\n",
        "cl_data_path = project_dir/'data'/'sentiment_data'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    shutil.rmtree(cl_path) \n",
        "except:\n",
        "    pass\n",
        "\n",
        "bertmodel = AutoModelForSequenceClassification.from_pretrained('distilbert-base-uncased', cache_dir=None, num_labels=3)\n",
        "\n",
        "\n",
        "config = Config(   data_dir=cl_data_path,\n",
        "                   bert_model=bertmodel,\n",
        "                   num_train_epochs=6,\n",
        "                   model_dir=cl_path,\n",
        "                   max_seq_length = 64,\n",
        "                   train_batch_size = 32,\n",
        "                   learning_rate =0.00001420326287435756,\n",
        "                   output_mode='classification',\n",
        "                   warm_up_proportion=0.14386028719686458,\n",
        "                   local_rank=-1,\n",
        "                   discriminate=True,\n",
        "                   gradual_unfreeze=False)\n",
        "\n",
        "config.profile_train_steps = 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "finbert = DistillFinBert(config)\n",
        "finbert.config.base_model = 'distilbert-base-uncased'\n",
        "finbert.config.discriminate=True\n",
        "finbert.config.gradual_unfreeze=False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "finbert.prepare_model(label_list=['positive','negative','neutral'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_data = finbert.get_data('train')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = finbert.create_the_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "start = time.perf_counter()\n",
        "trained_model = finbert.train(train_examples = train_data, model = model)\n",
        "train_wall_s = time.perf_counter() - start"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_data = finbert.get_data(\"test\")\n",
        "\n",
        "results = finbert.evaluate(examples=test_data, model=trained_model)\n",
        "\n",
        "eval_df, eval_timing = timed_eval(\n",
        "    finbert=finbert, model=trained_model, examples=test_data, use_amp=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def report(df, cols=['label','prediction','logits']):\n",
        "    #print('Validation loss:{0:.2f}'.format(metrics['best_validation_loss']))\n",
        "    cs = CrossEntropyLoss(weight=finbert.class_weights)\n",
        "    loss = cs(torch.tensor(list(df[cols[2]])),torch.tensor(list(df[cols[0]])))\n",
        "    print(\"Evaluation Loss:{0:.2f}\".format(loss))\n",
        "    print(\"Evaluation Accuracy:{0:.2f}\".format((df[cols[0]] == df[cols[1]]).sum() / df.shape[0]) )\n",
        "    print(\"\\nClassification Report:\")\n",
        "    return_val = classification_report(df[cols[0]], df[cols[1]], output_dict=True)\n",
        "    print(return_val)\n",
        "    return_val[\"Evalaution Loss\"] = loss\n",
        "    return_val[\"Evaluation Accuracy\"] = (df[cols[0]] == df[cols[1]]).sum() / df.shape[0]\n",
        "    return return_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results['prediction'] = results.predictions.apply(lambda x: np.argmax(x,axis=0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "wandb_report = report(results,cols=['labels','prediction','predictions'])\n",
        "\n",
        "summary = {\n",
        "        \"device\": str(finbert.device),\n",
        "        \"model_dir\": str(cl_path),\n",
        "        \"train_wall_s\": float(train_wall_s),\n",
        "        \"train_examples\": int(len(train_data)),\n",
        "        \"train_examples_per_s\": float((len(train_data) * finbert.config.num_train_epochs) / train_wall_s)\n",
        "        if train_wall_s > 0\n",
        "        else float(\"inf\"),\n",
        "        \"model_size_mb\": float(get_model_size_mb(trained_model)),\n",
        "        \"profile_train_steps\": finbert.config.profile_train_steps,\n",
        "        **(finbert.profile_results.get(\"training_summary\", {}) or {}),\n",
        "        **eval_timing,\n",
        "        **wandb_report,\n",
        "    }\n",
        "\n",
        "wandb.log(summary)\n",
        "wandb.finish()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
