{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Master Profile Notebook\n",
        "This notebook combines all the profiling experiments we have done so far into one notebook: uses the optimal hyperparameters, knowledge distillation, trains using AMP, and uses FP16 weights.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imports\n",
        "We import the necessary libraries and modules, including the custom `finbert` modules we have defined that allow for profiling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Helper utilities loaded\n"
          ]
        }
      ],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "from pathlib import Path\n",
        "import shutil\n",
        "import time\n",
        "import sys\n",
        "sys.path.append('..')\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from sklearn.metrics import classification_report\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "from finbert.finbert import *\n",
        "from finbert.finbert_profile import *\n",
        "from finbert.profile_utils import get_model_size_mb, print_device_info, setup_nltk_data\n",
        "import finbert.utils as tools\n",
        "from finbert.distillFinBert import *\n",
        "\n",
        "import wandb\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "project_dir = Path.cwd().parent\n",
        "pd.set_option('max_colwidth', None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Paths\n",
        "cl_path = project_dir / 'models' / 'master-profile'\n",
        "cl_path_baseline = project_dir / 'models' / 'master-profile' / 'baseline'\n",
        "cl_path_amp = project_dir / 'models' / 'master-profile' / 'amp'\n",
        "cl_data_path = project_dir / 'data' / 'sentiment_data'\n",
        "\n",
        "# Clean up previous run\n",
        "try:\n",
        "    shutil.rmtree(cl_path)\n",
        "except:\n",
        "    pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "12/17/2025 03:26:07 - WARNING - torchao -   Skipping import of cpp extensions due to incompatible torch version 2.9.1+cu128 for torchao version 0.14.1             Please see https://github.com/pytorch/ao/issues/2919 for more info\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "12/17/2025 03:26:09 - INFO - finbert.finbert -   device: cuda n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "12/17/2025 03:26:11 - INFO - finbert.utils -   *** Example ***\n",
            "12/17/2025 03:26:11 - INFO - finbert.utils -   guid: train-1\n",
            "12/17/2025 03:26:11 - INFO - finbert.utils -   tokens: [CLS] after the reporting period , bio ##tie north american licensing partner so ##max ##on pharmaceuticals announced positive results with na ##lm ##efe ##ne in a pilot phase 2 clinical trial for smoking ce ##ssa ##tion [SEP]\n",
            "12/17/2025 03:26:11 - INFO - finbert.utils -   input_ids: 101 2044 1996 7316 2558 1010 16012 9515 2167 2137 13202 4256 2061 17848 2239 24797 2623 3893 3463 2007 6583 13728 27235 2638 1999 1037 4405 4403 1016 6612 3979 2005 9422 8292 11488 3508 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/17/2025 03:26:11 - INFO - finbert.utils -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/17/2025 03:26:11 - INFO - finbert.utils -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/17/2025 03:26:11 - INFO - finbert.utils -   label: positive (id = 0)\n",
            "12/17/2025 03:26:11 - INFO - finbert.finbert -   ***** Loading data *****\n",
            "12/17/2025 03:26:11 - INFO - finbert.finbert -     Num examples = 3488\n",
            "12/17/2025 03:26:11 - INFO - finbert.finbert -     Batch size = 32\n",
            "12/17/2025 03:26:11 - INFO - finbert.finbert -     Num steps = 72\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "Starting Profiled Training\n",
            "Device: cuda\n",
            "Profiling activities: [<ProfilerActivity.CPU: 0>, <ProfilerActivity.CUDA: 2>]\n",
            "================================================================================\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Iteration:  17%|█▋        | 19/109 [00:14<01:10,  1.28it/s]\n",
            "Epoch:   0%|          | 0/6 [00:14<?, ?it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "Profiling complete for first epoch (20 steps)\n",
            "Continuing full training without profiling...\n",
            "================================================================================\n",
            "\n",
            "\n",
            "================================================================================\n",
            "PROFILING RESULTS - Training\n",
            "================================================================================\n",
            "\n",
            "\n",
            "By CPU Time:\n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                  cudaStreamSynchronize        76.15%       12.163s        76.15%       12.163s      76.018ms       0.000us         0.00%       0.000us       0.000us           0 B           0 B           0 B           0 B           160  \n",
            "                                             aten::item         0.09%      15.170ms        41.87%        6.687s     827.431us       0.000us         0.00%      77.308us       0.010us           0 B           0 B           0 B           0 B          8082  \n",
            "                              aten::_local_scalar_dense         0.05%       7.713ms        41.78%        6.672s     825.554us      77.308us         0.00%      77.308us       0.010us           0 B           0 B           0 B           0 B          8082  \n",
            "                                               aten::to         0.03%       4.556ms        34.60%        5.526s     997.312us       0.000us         0.00%     622.283us       0.112us           0 B           0 B      13.47 MB           0 B          5541  \n",
            "                                            aten::copy_         0.04%       5.946ms        34.58%        5.523s      12.553ms       1.766ms         0.01%       1.794ms       4.078us           0 B           0 B           0 B           0 B           440  \n",
            "                                         aten::_to_copy         0.01%       2.234ms        34.57%        5.522s      34.510ms       0.000us         0.00%     622.283us       3.889us           0 B           0 B      13.47 MB           0 B           160  \n",
            "                                       loss_calculation         0.08%      13.066ms        25.15%        4.018s     200.876ms       0.000us         0.00%     206.879us      10.344us           0 B           0 B      21.00 KB     -19.00 KB            20  \n",
            "                                          data_transfer         0.02%       3.928ms         9.65%        1.541s      77.062ms       0.000us         0.00%     296.916us      14.846us           0 B           0 B      49.00 KB    -931.00 KB            20  \n",
            "                                          backward_pass         4.91%     784.975ms         7.55%        1.205s      60.254ms       0.000us         0.00%      45.985us       2.299us      -3.75 KB      -3.75 KB     -15.30 GB     -15.28 GB            20  \n",
            "                                       cudaLaunchKernel         3.53%     564.322ms         6.31%        1.009s      57.039us       0.000us         0.00%      47.487ms       2.686us           0 B           0 B           0 B           0 B         17682  \n",
            "                       Runtime Triggered Module Loading         5.36%     856.201ms         5.36%     856.516ms      12.978ms      29.472ms         0.21%      29.478ms     446.638us           0 B           0 B           0 B           0 B            66  \n",
            "                                           forward_pass         0.79%     125.682ms         4.93%     786.675ms      39.334ms       0.000us         0.00%        4.487s     224.331ms       3.75 KB         -80 B      23.62 GB      -6.00 GB            20  \n",
            "                                         optimizer_step         0.34%      54.096ms         3.80%     606.976ms      30.349ms       0.000us         0.00%        1.932s      96.600ms         804 B           0 B      -7.49 GB      -2.03 MB            20  \n",
            "                              Optimizer.step#AdamW.step         1.04%     165.449ms         2.68%     427.503ms      21.375ms       0.000us         0.00%        1.742s      87.100ms         804 B      -2.34 KB     836.46 MB      -8.17 GB            20  \n",
            "autograd::engine::evaluate_function: EmbeddingBackwa...         0.00%     588.502us         2.21%     353.574ms       5.893ms       0.000us         0.00%      38.435ms     640.586us           0 B           0 B       1.67 GB    -123.75 MB            60  \n",
            "                                     EmbeddingBackward0         0.00%     252.458us         2.21%     352.985ms       5.883ms       0.000us         0.00%      38.435ms     640.586us           0 B           0 B       1.79 GB           0 B            60  \n",
            "                               aten::embedding_backward         0.00%     204.556us         2.21%     352.733ms       5.879ms       0.000us         0.00%      38.435ms     640.586us           0 B           0 B       1.79 GB           0 B            60  \n",
            "                         aten::embedding_dense_backward         0.00%     780.767us         2.21%     352.528ms       5.875ms      19.981ms         0.14%      38.435ms     640.586us           0 B           0 B       1.79 GB           0 B            60  \n",
            "                                           aten::linear         0.11%      17.228ms         2.19%     350.371ms     236.737us       0.000us         0.00%        3.978s       2.688ms           0 B           0 B      12.67 GB           0 B          1480  \n",
            "    autograd::engine::evaluate_function: AddmmBackward0         0.22%      34.690ms         1.93%     308.307ms     208.316us       0.000us         0.00%        6.499s       4.391ms           0 B           0 B      -2.05 GB     -21.23 GB          1480  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 15.971s\n",
            "Self CUDA time total: 13.995s\n",
            "\n",
            "\n",
            "By CUDA Time:\n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "    autograd::engine::evaluate_function: AddmmBackward0         0.22%      34.690ms         1.93%     308.307ms     208.316us       0.000us         0.00%        6.499s       4.391ms           0 B           0 B      -2.05 GB     -21.23 GB          1480  \n",
            "                                         AddmmBackward0         0.14%      22.763ms         1.22%     195.227ms     131.910us       0.000us         0.00%        6.310s       4.263ms           0 B           0 B      19.18 GB           0 B          1480  \n",
            "                                               aten::mm         0.54%      85.532ms         0.82%     130.458ms      44.074us        6.310s        45.08%        6.310s       2.132ms           0 B           0 B      19.18 GB      19.18 GB          2960  \n",
            "                                           forward_pass         0.00%       0.000us         0.00%       0.000us       0.000us        4.707s        33.63%        4.707s     235.339ms           0 B           0 B           0 B           0 B            20  \n",
            "                                           forward_pass         0.79%     125.682ms         4.93%     786.675ms      39.334ms       0.000us         0.00%        4.487s     224.331ms       3.75 KB         -80 B      23.62 GB      -6.00 GB            20  \n",
            "                                           aten::linear         0.11%      17.228ms         2.19%     350.371ms     236.737us       0.000us         0.00%        3.978s       2.688ms           0 B           0 B      12.67 GB           0 B          1480  \n",
            "                                            aten::addmm         0.46%      73.823ms         1.87%     297.879ms     201.270us        3.970s        28.37%        3.978s       2.688ms           0 B           0 B      12.67 GB      12.67 GB          1480  \n",
            "                                  volta_sgemm_128x64_tn         0.00%       0.000us         0.00%       0.000us       0.000us        2.689s        19.22%        2.689s       2.241ms           0 B           0 B           0 B           0 B          1200  \n",
            "                                  volta_sgemm_128x64_nn         0.00%       0.000us         0.00%       0.000us       0.000us        2.171s        15.51%        2.171s       1.809ms           0 B           0 B           0 B           0 B          1200  \n",
            "                                  volta_sgemm_128x64_nt         0.00%       0.000us         0.00%       0.000us       0.000us        2.061s        14.73%        2.061s       1.689ms           0 B           0 B           0 B           0 B          1220  \n",
            "                                         optimizer_step         0.34%      54.096ms         3.80%     606.976ms      30.349ms       0.000us         0.00%        1.932s      96.600ms         804 B           0 B      -7.49 GB      -2.03 MB            20  \n",
            "                              Optimizer.step#AdamW.step         0.00%       0.000us         0.00%       0.000us       0.000us        1.782s        12.73%        1.782s      89.109ms           0 B           0 B           0 B           0 B            20  \n",
            "                              Optimizer.step#AdamW.step         1.04%     165.449ms         2.68%     427.503ms      21.375ms       0.000us         0.00%        1.742s      87.100ms         804 B      -2.34 KB     836.46 MB      -8.17 GB            20  \n",
            "                                 volta_sgemm_128x128_tn         0.00%       0.000us         0.00%       0.000us       0.000us        1.277s         9.12%        1.277s       5.319ms           0 B           0 B           0 B           0 B           240  \n",
            "                                 volta_sgemm_128x128_nn         0.00%       0.000us         0.00%       0.000us       0.000us        1.049s         7.50%        1.049s       4.371ms           0 B           0 B           0 B           0 B           240  \n",
            "                                 volta_sgemm_128x128_nt         0.00%       0.000us         0.00%       0.000us       0.000us        1.025s         7.32%        1.025s       4.271ms           0 B           0 B           0 B           0 B           240  \n",
            "                                    aten::_foreach_mul_         0.11%      17.743ms         0.28%      44.702ms      48.589us     567.164ms         4.05%     588.660ms     639.848us           0 B           0 B           0 B           0 B           920  \n",
            "autograd::engine::evaluate_function: ScaledDotProduc...         0.04%       6.636ms         0.67%     106.630ms     444.293us       0.000us         0.00%     544.092ms       2.267ms      -3.75 KB      -3.75 KB      -2.82 GB      -7.06 GB           240  \n",
            "            ScaledDotProductEfficientAttentionBackward0         0.02%       3.007ms         0.63%      99.994ms     416.643us       0.000us         0.00%     544.092ms       2.267ms           0 B           0 B       4.24 GB           0 B           240  \n",
            "aten::_scaled_dot_product_efficient_attention_backwa...         0.04%       6.327ms         0.61%      96.987ms     404.113us       0.000us         0.00%     544.092ms       2.267ms           0 B           0 B       4.24 GB           0 B           240  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 15.971s\n",
            "Self CUDA time total: 13.995s\n",
            "\n",
            "\n",
            "================================================================================\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Iteration: 100%|██████████| 109/109 [01:17<00:00,  1.41it/s]\n",
            "12/17/2025 03:28:24 - INFO - finbert.utils -   *** Example ***\n",
            "12/17/2025 03:28:24 - INFO - finbert.utils -   guid: validation-1\n",
            "12/17/2025 03:28:24 - INFO - finbert.utils -   tokens: [CLS] our in - depth expertise extends to the fields of energy , industry , urban & mobility and water & environment [SEP]\n",
            "12/17/2025 03:28:24 - INFO - finbert.utils -   input_ids: 101 2256 1999 1011 5995 11532 8908 2000 1996 4249 1997 2943 1010 3068 1010 3923 1004 12969 1998 2300 1004 4044 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/17/2025 03:28:24 - INFO - finbert.utils -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/17/2025 03:28:24 - INFO - finbert.utils -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/17/2025 03:28:24 - INFO - finbert.utils -   label: neutral (id = 2)\n",
            "12/17/2025 03:28:24 - INFO - finbert.finbert -   ***** Loading data *****\n",
            "12/17/2025 03:28:24 - INFO - finbert.finbert -     Num examples = 388\n",
            "12/17/2025 03:28:24 - INFO - finbert.finbert -     Batch size = 32\n",
            "12/17/2025 03:28:24 - INFO - finbert.finbert -     Num steps = 72\n",
            "Validating: 100%|██████████| 13/13 [00:02<00:00,  4.68it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation losses: [0.7724753526540903]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Iteration: 100%|██████████| 109/109 [01:17<00:00,  1.41it/s]\n",
            "12/17/2025 03:29:45 - INFO - finbert.utils -   *** Example ***\n",
            "12/17/2025 03:29:45 - INFO - finbert.utils -   guid: validation-1\n",
            "12/17/2025 03:29:45 - INFO - finbert.utils -   tokens: [CLS] our in - depth expertise extends to the fields of energy , industry , urban & mobility and water & environment [SEP]\n",
            "12/17/2025 03:29:45 - INFO - finbert.utils -   input_ids: 101 2256 1999 1011 5995 11532 8908 2000 1996 4249 1997 2943 1010 3068 1010 3923 1004 12969 1998 2300 1004 4044 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/17/2025 03:29:45 - INFO - finbert.utils -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/17/2025 03:29:45 - INFO - finbert.utils -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/17/2025 03:29:45 - INFO - finbert.utils -   label: neutral (id = 2)\n",
            "12/17/2025 03:29:45 - INFO - finbert.finbert -   ***** Loading data *****\n",
            "12/17/2025 03:29:45 - INFO - finbert.finbert -     Num examples = 388\n",
            "12/17/2025 03:29:45 - INFO - finbert.finbert -     Batch size = 32\n",
            "12/17/2025 03:29:45 - INFO - finbert.finbert -     Num steps = 72\n",
            "Validating: 100%|██████████| 13/13 [00:02<00:00,  4.62it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation losses: [0.7724753526540903, 0.6911069934184735]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Iteration: 100%|██████████| 109/109 [01:17<00:00,  1.41it/s]\n",
            "12/17/2025 03:31:06 - INFO - finbert.utils -   *** Example ***\n",
            "12/17/2025 03:31:06 - INFO - finbert.utils -   guid: validation-1\n",
            "12/17/2025 03:31:06 - INFO - finbert.utils -   tokens: [CLS] our in - depth expertise extends to the fields of energy , industry , urban & mobility and water & environment [SEP]\n",
            "12/17/2025 03:31:06 - INFO - finbert.utils -   input_ids: 101 2256 1999 1011 5995 11532 8908 2000 1996 4249 1997 2943 1010 3068 1010 3923 1004 12969 1998 2300 1004 4044 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/17/2025 03:31:06 - INFO - finbert.utils -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/17/2025 03:31:06 - INFO - finbert.utils -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/17/2025 03:31:06 - INFO - finbert.utils -   label: neutral (id = 2)\n",
            "12/17/2025 03:31:06 - INFO - finbert.finbert -   ***** Loading data *****\n",
            "12/17/2025 03:31:06 - INFO - finbert.finbert -     Num examples = 388\n",
            "12/17/2025 03:31:06 - INFO - finbert.finbert -     Batch size = 32\n",
            "12/17/2025 03:31:06 - INFO - finbert.finbert -     Num steps = 72\n",
            "Validating: 100%|██████████| 13/13 [00:02<00:00,  4.60it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation losses: [0.7724753526540903, 0.6911069934184735, 0.6911069934184735]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Iteration: 100%|██████████| 109/109 [01:17<00:00,  1.41it/s]\n",
            "12/17/2025 03:32:27 - INFO - finbert.utils -   *** Example ***\n",
            "12/17/2025 03:32:27 - INFO - finbert.utils -   guid: validation-1\n",
            "12/17/2025 03:32:27 - INFO - finbert.utils -   tokens: [CLS] our in - depth expertise extends to the fields of energy , industry , urban & mobility and water & environment [SEP]\n",
            "12/17/2025 03:32:27 - INFO - finbert.utils -   input_ids: 101 2256 1999 1011 5995 11532 8908 2000 1996 4249 1997 2943 1010 3068 1010 3923 1004 12969 1998 2300 1004 4044 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/17/2025 03:32:27 - INFO - finbert.utils -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/17/2025 03:32:27 - INFO - finbert.utils -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/17/2025 03:32:27 - INFO - finbert.utils -   label: neutral (id = 2)\n",
            "12/17/2025 03:32:27 - INFO - finbert.finbert -   ***** Loading data *****\n",
            "12/17/2025 03:32:27 - INFO - finbert.finbert -     Num examples = 388\n",
            "12/17/2025 03:32:27 - INFO - finbert.finbert -     Batch size = 32\n",
            "12/17/2025 03:32:27 - INFO - finbert.finbert -     Num steps = 72\n",
            "Validating: 100%|██████████| 13/13 [00:02<00:00,  4.67it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation losses: [0.7724753526540903, 0.6911069934184735, 0.6911069934184735, 0.6911069934184735]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Iteration: 100%|██████████| 109/109 [01:04<00:00,  1.69it/s]\n",
            "12/17/2025 03:33:35 - INFO - finbert.utils -   *** Example ***\n",
            "12/17/2025 03:33:35 - INFO - finbert.utils -   guid: validation-1\n",
            "12/17/2025 03:33:35 - INFO - finbert.utils -   tokens: [CLS] our in - depth expertise extends to the fields of energy , industry , urban & mobility and water & environment [SEP]\n",
            "12/17/2025 03:33:35 - INFO - finbert.utils -   input_ids: 101 2256 1999 1011 5995 11532 8908 2000 1996 4249 1997 2943 1010 3068 1010 3923 1004 12969 1998 2300 1004 4044 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/17/2025 03:33:35 - INFO - finbert.utils -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/17/2025 03:33:35 - INFO - finbert.utils -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/17/2025 03:33:35 - INFO - finbert.utils -   label: neutral (id = 2)\n",
            "12/17/2025 03:33:35 - INFO - finbert.finbert -   ***** Loading data *****\n",
            "12/17/2025 03:33:35 - INFO - finbert.finbert -     Num examples = 388\n",
            "12/17/2025 03:33:35 - INFO - finbert.finbert -     Batch size = 32\n",
            "12/17/2025 03:33:35 - INFO - finbert.finbert -     Num steps = 72\n",
            "Validating: 100%|██████████| 13/13 [00:01<00:00,  9.62it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation losses: [0.7724753526540903, 0.6911069934184735, 0.6911069934184735, 0.6911069934184735, 0.6911069934184735]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Iteration: 100%|██████████| 109/109 [00:36<00:00,  2.97it/s]\n",
            "12/17/2025 03:34:14 - INFO - finbert.utils -   *** Example ***\n",
            "12/17/2025 03:34:14 - INFO - finbert.utils -   guid: validation-1\n",
            "12/17/2025 03:34:14 - INFO - finbert.utils -   tokens: [CLS] our in - depth expertise extends to the fields of energy , industry , urban & mobility and water & environment [SEP]\n",
            "12/17/2025 03:34:14 - INFO - finbert.utils -   input_ids: 101 2256 1999 1011 5995 11532 8908 2000 1996 4249 1997 2943 1010 3068 1010 3923 1004 12969 1998 2300 1004 4044 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/17/2025 03:34:14 - INFO - finbert.utils -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/17/2025 03:34:14 - INFO - finbert.utils -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/17/2025 03:34:14 - INFO - finbert.utils -   label: neutral (id = 2)\n",
            "12/17/2025 03:34:14 - INFO - finbert.finbert -   ***** Loading data *****\n",
            "12/17/2025 03:34:14 - INFO - finbert.finbert -     Num examples = 388\n",
            "12/17/2025 03:34:14 - INFO - finbert.finbert -     Batch size = 32\n",
            "12/17/2025 03:34:14 - INFO - finbert.finbert -     Num steps = 72\n",
            "Validating: 100%|██████████| 13/13 [00:01<00:00,  9.53it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation losses: [0.7724753526540903, 0.6911069934184735, 0.6911069934184735, 0.6911069934184735, 0.6911069934184735, 0.6911069934184735]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch: 100%|██████████| 6/6 [07:09<00:00, 71.59s/it]\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The anonymous setting has no effect and will be removed in a future version.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtfs2123\u001b[0m (\u001b[33msi2449-columbia-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.23.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/tfs2123/hpml-project/pipelines/finBERT/notebooks/wandb/run-20251217_033418-x292gz3q</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/si2449-columbia-university/finbert-experiments/runs/x292gz3q' target=\"_blank\">master-profile</a></strong> to <a href='https://wandb.ai/si2449-columbia-university/finbert-experiments' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/si2449-columbia-university/finbert-experiments' target=\"_blank\">https://wandb.ai/si2449-columbia-university/finbert-experiments</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/si2449-columbia-university/finbert-experiments/runs/x292gz3q' target=\"_blank\">https://wandb.ai/si2449-columbia-university/finbert-experiments/runs/x292gz3q</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "12/17/2025 03:34:19 - INFO - finbert.distillFinBert -   device: cuda n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "12/17/2025 03:34:20 - INFO - finbert.utils -   *** Example ***\n",
            "12/17/2025 03:34:20 - INFO - finbert.utils -   guid: train-1\n",
            "12/17/2025 03:34:20 - INFO - finbert.utils -   tokens: [CLS] after the reporting period , bio ##tie north american licensing partner so ##max ##on pharmaceuticals announced positive results with na ##lm ##efe ##ne in a pilot phase 2 clinical trial for smoking ce ##ssa ##tion [SEP]\n",
            "12/17/2025 03:34:20 - INFO - finbert.utils -   input_ids: 101 2044 1996 7316 2558 1010 16012 9515 2167 2137 13202 4256 2061 17848 2239 24797 2623 3893 3463 2007 6583 13728 27235 2638 1999 1037 4405 4403 1016 6612 3979 2005 9422 8292 11488 3508 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/17/2025 03:34:20 - INFO - finbert.utils -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/17/2025 03:34:20 - INFO - finbert.utils -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/17/2025 03:34:20 - INFO - finbert.utils -   label: positive (id = 0)\n",
            "12/17/2025 03:34:21 - INFO - finbert.distillFinBert -   ***** Loading data *****\n",
            "12/17/2025 03:34:21 - INFO - finbert.distillFinBert -     Num examples = 3488\n",
            "12/17/2025 03:34:21 - INFO - finbert.distillFinBert -     Batch size = 32\n",
            "12/17/2025 03:34:21 - INFO - finbert.distillFinBert -     Num steps = 72\n",
            "/home/tfs2123/hpml-project/pipelines/finBERT/notebooks/../finbert/distillFinBert.py:369: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=self.config.use_amp)\n",
            "Epoch:   0%|          | 0/6 [00:00<?, ?it/s]/home/tfs2123/hpml-project/pipelines/finBERT/notebooks/../finbert/distillFinBert.py:426: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=self.config.use_amp):\n",
            "/home/tfs2123/hpml-project/pipelines/finBERT/notebooks/../finbert/distillFinBert.py:430: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=self.config.use_amp):\n",
            "Iteration:  17%|█▋        | 19/109 [00:03<00:14,  6.14it/s]\n",
            "Epoch:   0%|          | 0/6 [00:03<?, ?it/s]\n",
            "Epoch:   0%|          | 0/6 [00:00<?, ?it/s]/home/tfs2123/hpml-project/pipelines/finBERT/notebooks/../finbert/distillFinBert.py:527: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=self.config.use_amp):\n",
            "Iteration: 100%|██████████| 109/109 [00:15<00:00,  7.09it/s]\n",
            "12/17/2025 03:35:00 - INFO - finbert.utils -   *** Example ***\n",
            "12/17/2025 03:35:00 - INFO - finbert.utils -   guid: validation-1\n",
            "12/17/2025 03:35:00 - INFO - finbert.utils -   tokens: [CLS] our in - depth expertise extends to the fields of energy , industry , urban & mobility and water & environment [SEP]\n",
            "12/17/2025 03:35:00 - INFO - finbert.utils -   input_ids: 101 2256 1999 1011 5995 11532 8908 2000 1996 4249 1997 2943 1010 3068 1010 3923 1004 12969 1998 2300 1004 4044 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/17/2025 03:35:00 - INFO - finbert.utils -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/17/2025 03:35:00 - INFO - finbert.utils -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/17/2025 03:35:00 - INFO - finbert.utils -   label: neutral (id = 2)\n",
            "12/17/2025 03:35:00 - INFO - finbert.distillFinBert -   ***** Loading data *****\n",
            "12/17/2025 03:35:00 - INFO - finbert.distillFinBert -     Num examples = 388\n",
            "12/17/2025 03:35:00 - INFO - finbert.distillFinBert -     Batch size = 32\n",
            "12/17/2025 03:35:00 - INFO - finbert.distillFinBert -     Num steps = 72\n",
            "Validating: 100%|██████████| 13/13 [00:01<00:00,  9.30it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation losses: [0.699349004488725]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Iteration: 100%|██████████| 109/109 [00:16<00:00,  6.72it/s]\n",
            "12/17/2025 03:35:18 - INFO - finbert.utils -   *** Example ***\n",
            "12/17/2025 03:35:18 - INFO - finbert.utils -   guid: validation-1\n",
            "12/17/2025 03:35:18 - INFO - finbert.utils -   tokens: [CLS] our in - depth expertise extends to the fields of energy , industry , urban & mobility and water & environment [SEP]\n",
            "12/17/2025 03:35:18 - INFO - finbert.utils -   input_ids: 101 2256 1999 1011 5995 11532 8908 2000 1996 4249 1997 2943 1010 3068 1010 3923 1004 12969 1998 2300 1004 4044 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/17/2025 03:35:18 - INFO - finbert.utils -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/17/2025 03:35:18 - INFO - finbert.utils -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/17/2025 03:35:18 - INFO - finbert.utils -   label: neutral (id = 2)\n",
            "12/17/2025 03:35:18 - INFO - finbert.distillFinBert -   ***** Loading data *****\n",
            "12/17/2025 03:35:18 - INFO - finbert.distillFinBert -     Num examples = 388\n",
            "12/17/2025 03:35:18 - INFO - finbert.distillFinBert -     Batch size = 32\n",
            "12/17/2025 03:35:18 - INFO - finbert.distillFinBert -     Num steps = 72\n",
            "Validating: 100%|██████████| 13/13 [00:01<00:00,  9.24it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation losses: [0.699349004488725, 0.6039920311707717]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Iteration: 100%|██████████| 109/109 [00:15<00:00,  7.08it/s]\n",
            "12/17/2025 03:35:35 - INFO - finbert.utils -   *** Example ***\n",
            "12/17/2025 03:35:35 - INFO - finbert.utils -   guid: validation-1\n",
            "12/17/2025 03:35:35 - INFO - finbert.utils -   tokens: [CLS] our in - depth expertise extends to the fields of energy , industry , urban & mobility and water & environment [SEP]\n",
            "12/17/2025 03:35:35 - INFO - finbert.utils -   input_ids: 101 2256 1999 1011 5995 11532 8908 2000 1996 4249 1997 2943 1010 3068 1010 3923 1004 12969 1998 2300 1004 4044 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/17/2025 03:35:35 - INFO - finbert.utils -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/17/2025 03:35:35 - INFO - finbert.utils -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/17/2025 03:35:35 - INFO - finbert.utils -   label: neutral (id = 2)\n",
            "12/17/2025 03:35:35 - INFO - finbert.distillFinBert -   ***** Loading data *****\n",
            "12/17/2025 03:35:35 - INFO - finbert.distillFinBert -     Num examples = 388\n",
            "12/17/2025 03:35:35 - INFO - finbert.distillFinBert -     Batch size = 32\n",
            "12/17/2025 03:35:35 - INFO - finbert.distillFinBert -     Num steps = 72\n",
            "Validating: 100%|██████████| 13/13 [00:01<00:00,  9.29it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation losses: [0.699349004488725, 0.6039920311707717, 0.6039920311707717]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Iteration: 100%|██████████| 109/109 [00:16<00:00,  6.71it/s]\n",
            "12/17/2025 03:35:53 - INFO - finbert.utils -   *** Example ***\n",
            "12/17/2025 03:35:53 - INFO - finbert.utils -   guid: validation-1\n",
            "12/17/2025 03:35:53 - INFO - finbert.utils -   tokens: [CLS] our in - depth expertise extends to the fields of energy , industry , urban & mobility and water & environment [SEP]\n",
            "12/17/2025 03:35:53 - INFO - finbert.utils -   input_ids: 101 2256 1999 1011 5995 11532 8908 2000 1996 4249 1997 2943 1010 3068 1010 3923 1004 12969 1998 2300 1004 4044 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/17/2025 03:35:53 - INFO - finbert.utils -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/17/2025 03:35:53 - INFO - finbert.utils -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/17/2025 03:35:53 - INFO - finbert.utils -   label: neutral (id = 2)\n",
            "12/17/2025 03:35:53 - INFO - finbert.distillFinBert -   ***** Loading data *****\n",
            "12/17/2025 03:35:53 - INFO - finbert.distillFinBert -     Num examples = 388\n",
            "12/17/2025 03:35:53 - INFO - finbert.distillFinBert -     Batch size = 32\n",
            "12/17/2025 03:35:53 - INFO - finbert.distillFinBert -     Num steps = 72\n",
            "Validating: 100%|██████████| 13/13 [00:01<00:00,  9.36it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation losses: [0.699349004488725, 0.6039920311707717, 0.6039920311707717, 0.6039920311707717]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Iteration: 100%|██████████| 109/109 [00:15<00:00,  7.08it/s]\n",
            "12/17/2025 03:36:11 - INFO - finbert.utils -   *** Example ***\n",
            "12/17/2025 03:36:11 - INFO - finbert.utils -   guid: validation-1\n",
            "12/17/2025 03:36:11 - INFO - finbert.utils -   tokens: [CLS] our in - depth expertise extends to the fields of energy , industry , urban & mobility and water & environment [SEP]\n",
            "12/17/2025 03:36:11 - INFO - finbert.utils -   input_ids: 101 2256 1999 1011 5995 11532 8908 2000 1996 4249 1997 2943 1010 3068 1010 3923 1004 12969 1998 2300 1004 4044 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/17/2025 03:36:11 - INFO - finbert.utils -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/17/2025 03:36:11 - INFO - finbert.utils -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/17/2025 03:36:11 - INFO - finbert.utils -   label: neutral (id = 2)\n",
            "12/17/2025 03:36:11 - INFO - finbert.distillFinBert -   ***** Loading data *****\n",
            "12/17/2025 03:36:11 - INFO - finbert.distillFinBert -     Num examples = 388\n",
            "12/17/2025 03:36:11 - INFO - finbert.distillFinBert -     Batch size = 32\n",
            "12/17/2025 03:36:11 - INFO - finbert.distillFinBert -     Num steps = 72\n",
            "Validating: 100%|██████████| 13/13 [00:01<00:00,  9.36it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation losses: [0.699349004488725, 0.6039920311707717, 0.6039920311707717, 0.6039920311707717, 0.6039920311707717]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Iteration: 100%|██████████| 109/109 [00:16<00:00,  6.80it/s]\n",
            "12/17/2025 03:36:29 - INFO - finbert.utils -   *** Example ***\n",
            "12/17/2025 03:36:29 - INFO - finbert.utils -   guid: validation-1\n",
            "12/17/2025 03:36:29 - INFO - finbert.utils -   tokens: [CLS] our in - depth expertise extends to the fields of energy , industry , urban & mobility and water & environment [SEP]\n",
            "12/17/2025 03:36:29 - INFO - finbert.utils -   input_ids: 101 2256 1999 1011 5995 11532 8908 2000 1996 4249 1997 2943 1010 3068 1010 3923 1004 12969 1998 2300 1004 4044 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/17/2025 03:36:29 - INFO - finbert.utils -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/17/2025 03:36:29 - INFO - finbert.utils -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/17/2025 03:36:29 - INFO - finbert.utils -   label: neutral (id = 2)\n",
            "12/17/2025 03:36:29 - INFO - finbert.distillFinBert -   ***** Loading data *****\n",
            "12/17/2025 03:36:29 - INFO - finbert.distillFinBert -     Num examples = 388\n",
            "12/17/2025 03:36:29 - INFO - finbert.distillFinBert -     Batch size = 32\n",
            "12/17/2025 03:36:29 - INFO - finbert.distillFinBert -     Num steps = 72\n",
            "Validating: 100%|██████████| 13/13 [00:00<00:00, 13.72it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation losses: [0.699349004488725, 0.6039920311707717, 0.6039920311707717, 0.6039920311707717, 0.6039920311707717, 0.6039920311707717]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch: 100%|██████████| 6/6 [01:45<00:00, 17.62s/it]\n"
          ]
        }
      ],
      "source": [
        "# Baseline model and training (no AMP)\n",
        "bertmodel_baseline = AutoModelForSequenceClassification.from_pretrained(\n",
        "    'bert-base-uncased', cache_dir=None, num_labels=3\n",
        ")\n",
        "\n",
        "config_baseline = Config(\n",
        "    data_dir=cl_data_path,\n",
        "    bert_model=bertmodel_baseline,\n",
        "    num_train_epochs=6,\n",
        "    model_dir=cl_path_baseline,  # Use baseline path\n",
        "    max_seq_length=64,\n",
        "    train_batch_size=32,\n",
        "    learning_rate=0.00001420326287435756,\n",
        "    output_mode='classification',\n",
        "    warm_up_proportion=0.14386028719686458,\n",
        "    local_rank=-1,\n",
        "    discriminate=True,\n",
        "    gradual_unfreeze=False,\n",
        "    use_amp=False,  # Baseline: AMP is off\n",
        ")\n",
        "config_baseline.profile_train_steps = 20\n",
        "\n",
        "finbert_baseline = ProfiledFinBert(config_baseline)\n",
        "finbert_baseline.base_model = 'bert-base-uncased'\n",
        "finbert_baseline.prepare_model(label_list=['positive', 'negative', 'neutral'])\n",
        "\n",
        "train_data_baseline = finbert_baseline.get_data('train')\n",
        "test_data_baseline = finbert_baseline.get_data('test')\n",
        "\n",
        "model_baseline = finbert_baseline.create_the_model()\n",
        "    \n",
        "# Train baseline\n",
        "start = time.perf_counter()\n",
        "trained_model_baseline = finbert_baseline.train(train_examples=train_data_baseline, model=model_baseline)\n",
        "baseline_train_wall_s = time.perf_counter() - start\n",
        "\n",
        "# AMP model and training\n",
        "bertmodel_amp = AutoModelForSequenceClassification.from_pretrained(\n",
        "    'distilbert-base-uncased', cache_dir=None, num_labels=3\n",
        ")\n",
        "\n",
        "config_amp = Config(\n",
        "    data_dir=cl_data_path,\n",
        "    bert_model=bertmodel_amp,\n",
        "    num_train_epochs=6,\n",
        "    model_dir=cl_path_amp,  # Changed from cl_path\n",
        "    max_seq_length=64,\n",
        "    train_batch_size=32,\n",
        "    learning_rate=0.00001420326287435756,\n",
        "    output_mode='classification',\n",
        "    warm_up_proportion=0.14386028719686458,\n",
        "    local_rank=-1,\n",
        "    discriminate=True,\n",
        "    gradual_unfreeze=False,\n",
        "    use_amp=True,  # Enable AMP\n",
        ")\n",
        "config_amp.profile_train_steps = 20\n",
        "\n",
        "# W&B\n",
        "wandb.init(\n",
        "    entity=\"si2449-columbia-university\",\n",
        "    project=\"finbert-experiments\",\n",
        "    name=\"master-profile\",\n",
        "    group=\"master-profile\",\n",
        "    config=vars(config_amp)\n",
        ")\n",
        "\n",
        "finbert_amp = DistillFinBert(config_amp)\n",
        "finbert_amp.base_model = 'distilbert-base-uncased'\n",
        "finbert_amp.prepare_model(label_list=['positive', 'negative', 'neutral'])\n",
        "\n",
        "train_data_amp = finbert_amp.get_data('train')\n",
        "test_data_amp = finbert_amp.get_data('test')\n",
        "\n",
        "# train_data and test_data already loaded above\n",
        "model_amp = finbert_amp.create_the_model()\n",
        "    \n",
        "# Train AMP\n",
        "start = time.perf_counter()\n",
        "trained_model_amp = finbert_amp.train(train_examples=train_data_amp, model=model_amp)\n",
        "amp_train_wall_s = time.perf_counter() - start\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "12/17/2025 03:36:31 - INFO - finbert.utils -   *** Example ***\n",
            "12/17/2025 03:36:31 - INFO - finbert.utils -   guid: test-1\n",
            "12/17/2025 03:36:31 - INFO - finbert.utils -   tokens: [CLS] the bristol port company has sealed a one million pound contract with cooper specialised handling to supply it with four 45 - ton ##ne , custom ##ised reach stack ##ers from ko ##ne ##cr ##ane ##s [SEP]\n",
            "12/17/2025 03:36:31 - INFO - finbert.utils -   input_ids: 101 1996 7067 3417 2194 2038 10203 1037 2028 2454 9044 3206 2007 6201 17009 8304 2000 4425 2009 2007 2176 3429 1011 10228 2638 1010 7661 5084 3362 9991 2545 2013 12849 2638 26775 7231 2015 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/17/2025 03:36:31 - INFO - finbert.utils -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/17/2025 03:36:31 - INFO - finbert.utils -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/17/2025 03:36:31 - INFO - finbert.utils -   label: positive (id = 0)\n",
            "12/17/2025 03:36:31 - INFO - finbert.finbert -   ***** Loading data *****\n",
            "12/17/2025 03:36:31 - INFO - finbert.finbert -     Num examples = 970\n",
            "12/17/2025 03:36:31 - INFO - finbert.finbert -     Batch size = 32\n",
            "12/17/2025 03:36:31 - INFO - finbert.finbert -     Num steps = 72\n",
            "12/17/2025 03:36:38 - INFO - finbert.utils -   *** Example ***\n",
            "12/17/2025 03:36:38 - INFO - finbert.utils -   guid: test-1\n",
            "12/17/2025 03:36:38 - INFO - finbert.utils -   tokens: [CLS] the bristol port company has sealed a one million pound contract with cooper specialised handling to supply it with four 45 - ton ##ne , custom ##ised reach stack ##ers from ko ##ne ##cr ##ane ##s [SEP]\n",
            "12/17/2025 03:36:38 - INFO - finbert.utils -   input_ids: 101 1996 7067 3417 2194 2038 10203 1037 2028 2454 9044 3206 2007 6201 17009 8304 2000 4425 2009 2007 2176 3429 1011 10228 2638 1010 7661 5084 3362 9991 2545 2013 12849 2638 26775 7231 2015 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/17/2025 03:36:38 - INFO - finbert.utils -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/17/2025 03:36:38 - INFO - finbert.utils -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/17/2025 03:36:38 - INFO - finbert.utils -   label: positive (id = 0)\n",
            "12/17/2025 03:36:38 - INFO - finbert.distillFinBert -   ***** Loading data *****\n",
            "12/17/2025 03:36:38 - INFO - finbert.distillFinBert -     Num examples = 970\n",
            "12/17/2025 03:36:38 - INFO - finbert.distillFinBert -     Batch size = 32\n",
            "12/17/2025 03:36:38 - INFO - finbert.distillFinBert -     Num steps = 72\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Baseline Model - Accuracy: 0.7237, F1: 0.6852\n",
            "Baseline Model - Throughput: 154.6 samples/sec\n",
            "Trained AMP Model - Accuracy: 0.7722, F1: 0.7555\n",
            "Trained AMP Model - Throughput: 830.8 samples/sec\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "def timed_eval(*, finbert, model, examples, use_amp=False):\n",
        "    \"\"\"Evaluation with timing.\"\"\"\n",
        "    loader = finbert.get_loader(examples, phase=\"eval\")\n",
        "    device = finbert.device\n",
        "    model.eval()\n",
        "\n",
        "    preds, labels = [], []\n",
        "    if device.type == \"cuda\":\n",
        "        torch.cuda.synchronize(device)\n",
        "\n",
        "    start = time.perf_counter()\n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            batch = tuple(t.to(device) for t in batch)\n",
        "            input_ids, attention_mask, token_type_ids, label_ids, _ = batch  # token_type_ids unused for DistilBERT\n",
        "\n",
        "            if use_amp and device.type == \"cuda\":\n",
        "                with torch.amp.autocast(device_type=\"cuda\"):\n",
        "                    out = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            else:\n",
        "                out = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "            logits = out.logits if hasattr(out, \"logits\") else out[0]\n",
        "            preds.extend(logits.detach().cpu().numpy())\n",
        "            labels.extend(label_ids.detach().cpu().numpy().tolist())\n",
        "\n",
        "    if device.type == \"cuda\":\n",
        "        torch.cuda.synchronize(device)\n",
        "\n",
        "    wall_s = time.perf_counter() - start\n",
        "    n = len(labels)\n",
        "\n",
        "    return pd.DataFrame({\"predictions\": preds, \"labels\": labels}), {\n",
        "        \"eval_wall_s\": wall_s,\n",
        "        \"eval_samples_per_s\": n / wall_s if wall_s > 0 else float(\"inf\"),\n",
        "    }\n",
        "\n",
        "def get_metrics(df):\n",
        "    \"\"\"Extract accuracy and F1 from eval results.\"\"\"\n",
        "    preds = np.array([np.argmax(p) for p in df['predictions']])\n",
        "    labels = np.array(df['labels'])\n",
        "    acc = (preds == labels).mean()\n",
        "    from sklearn.metrics import f1_score\n",
        "    f1 = f1_score(labels, preds, average='macro')\n",
        "    return {\"accuracy\": acc, \"f1_macro\": f1}\n",
        "\n",
        "\n",
        "# Baseline eval (no AMP)\n",
        "baseline_eval_df, baseline_timing = timed_eval(\n",
        "    finbert=finbert_baseline, model=trained_model_baseline, examples=test_data_baseline, use_amp=False\n",
        ")\n",
        "baseline_metrics = get_metrics(baseline_eval_df)\n",
        "print(f\"Baseline Model - Accuracy: {baseline_metrics['accuracy']:.4f}, F1: {baseline_metrics['f1_macro']:.4f}\")\n",
        "print(f\"Baseline Model - Throughput: {baseline_timing['eval_samples_per_s']:.1f} samples/sec\")\n",
        "\n",
        "# AMP-trained eval (AMP enabled)\n",
        "amp_trained_eval_df, amp_trained_timing = timed_eval(\n",
        "    finbert=finbert_amp, model=trained_model_amp, examples=test_data_amp, use_amp=True\n",
        ")\n",
        "amp_trained_metrics = get_metrics(amp_trained_eval_df)\n",
        "print(f\"Trained AMP Model - Accuracy: {amp_trained_metrics['accuracy']:.4f}, F1: {amp_trained_metrics['f1_macro']:.4f}\")\n",
        "print(f\"Trained AMP Model - Throughput: {amp_trained_timing['eval_samples_per_s']:.1f} samples/sec\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "12/17/2025 03:36:39 - INFO - finbert.utils -   *** Example ***\n",
            "12/17/2025 03:36:39 - INFO - finbert.utils -   guid: test-1\n",
            "12/17/2025 03:36:39 - INFO - finbert.utils -   tokens: [CLS] the bristol port company has sealed a one million pound contract with cooper specialised handling to supply it with four 45 - ton ##ne , custom ##ised reach stack ##ers from ko ##ne ##cr ##ane ##s [SEP]\n",
            "12/17/2025 03:36:39 - INFO - finbert.utils -   input_ids: 101 1996 7067 3417 2194 2038 10203 1037 2028 2454 9044 3206 2007 6201 17009 8304 2000 4425 2009 2007 2176 3429 1011 10228 2638 1010 7661 5084 3362 9991 2545 2013 12849 2638 26775 7231 2015 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/17/2025 03:36:39 - INFO - finbert.utils -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/17/2025 03:36:39 - INFO - finbert.utils -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/17/2025 03:36:39 - INFO - finbert.utils -   label: positive (id = 0)\n",
            "12/17/2025 03:36:39 - INFO - finbert.finbert -   ***** Loading data *****\n",
            "12/17/2025 03:36:39 - INFO - finbert.finbert -     Num examples = 970\n",
            "12/17/2025 03:36:39 - INFO - finbert.finbert -     Batch size = 32\n",
            "12/17/2025 03:36:39 - INFO - finbert.finbert -     Num steps = 72\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Baseline - Accuracy: 0.7237, F1: 0.6852\n",
            "Baseline - Throughput: 154.9 samples/sec\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "12/17/2025 03:36:46 - INFO - finbert.utils -   *** Example ***\n",
            "12/17/2025 03:36:46 - INFO - finbert.utils -   guid: test-1\n",
            "12/17/2025 03:36:46 - INFO - finbert.utils -   tokens: [CLS] the bristol port company has sealed a one million pound contract with cooper specialised handling to supply it with four 45 - ton ##ne , custom ##ised reach stack ##ers from ko ##ne ##cr ##ane ##s [SEP]\n",
            "12/17/2025 03:36:46 - INFO - finbert.utils -   input_ids: 101 1996 7067 3417 2194 2038 10203 1037 2028 2454 9044 3206 2007 6201 17009 8304 2000 4425 2009 2007 2176 3429 1011 10228 2638 1010 7661 5084 3362 9991 2545 2013 12849 2638 26775 7231 2015 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/17/2025 03:36:46 - INFO - finbert.utils -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/17/2025 03:36:46 - INFO - finbert.utils -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/17/2025 03:36:46 - INFO - finbert.utils -   label: positive (id = 0)\n",
            "12/17/2025 03:36:46 - INFO - finbert.distillFinBert -   ***** Loading data *****\n",
            "12/17/2025 03:36:46 - INFO - finbert.distillFinBert -     Num examples = 970\n",
            "12/17/2025 03:36:46 - INFO - finbert.distillFinBert -     Batch size = 32\n",
            "12/17/2025 03:36:46 - INFO - finbert.distillFinBert -     Num steps = 72\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AMP+FP16 - Accuracy: 0.7722, F1: 0.7555\n",
            "AMP+FP16 - Throughput: 872.4 samples/sec\n",
            "\n",
            "[STATS]\n",
            "Relative speedup (AMP+FP16 vs. baseline): 5.63x\n",
            "Model size compression (AMP+FP16 vs. baseline): 3.27x smaller (127.7MB vs 417.7MB)\n",
            "Accuracy delta (AMP+FP16 minus baseline): +0.048\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>variant</th>\n",
              "      <th>model_size_mb</th>\n",
              "      <th>eval_wall_s</th>\n",
              "      <th>eval_samples_per_s</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>f1_macro</th>\n",
              "      <th>speedup_x</th>\n",
              "      <th>compression_x</th>\n",
              "      <th>accuracy_delta</th>\n",
              "      <th>base_accuracy</th>\n",
              "      <th>amp_fp16_accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>baseline</td>\n",
              "      <td>417.658215</td>\n",
              "      <td>6.262848</td>\n",
              "      <td>154.881624</td>\n",
              "      <td>0.723711</td>\n",
              "      <td>0.685212</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>amp_fp16_weights</td>\n",
              "      <td>127.711920</td>\n",
              "      <td>1.111841</td>\n",
              "      <td>872.426830</td>\n",
              "      <td>0.772165</td>\n",
              "      <td>0.755461</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>amp_fp16_vs_baseline</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5.632862</td>\n",
              "      <td>3.270315</td>\n",
              "      <td>0.048454</td>\n",
              "      <td>0.723711</td>\n",
              "      <td>0.772165</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                variant  model_size_mb  eval_wall_s  eval_samples_per_s  \\\n",
              "0              baseline     417.658215     6.262848          154.881624   \n",
              "1      amp_fp16_weights     127.711920     1.111841          872.426830   \n",
              "2  amp_fp16_vs_baseline            NaN          NaN                 NaN   \n",
              "\n",
              "   accuracy  f1_macro  speedup_x  compression_x  accuracy_delta  \\\n",
              "0  0.723711  0.685212        NaN            NaN             NaN   \n",
              "1  0.772165  0.755461        NaN            NaN             NaN   \n",
              "2       NaN       NaN   5.632862       3.270315        0.048454   \n",
              "\n",
              "   base_accuracy  amp_fp16_accuracy  \n",
              "0            NaN                NaN  \n",
              "1            NaN                NaN  \n",
              "2       0.723711           0.772165  "
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = finbert_baseline.device\n",
        "\n",
        "# Evaluate baseline (no AMP/FP16)\n",
        "baseline_eval_df, baseline_timing = timed_eval(\n",
        "    finbert=finbert_baseline, model=model_baseline, examples=test_data_baseline, use_amp=False\n",
        ")\n",
        "baseline_metrics = get_metrics(baseline_eval_df)\n",
        "\n",
        "results_records = [{\n",
        "    \"variant\": \"baseline\",\n",
        "    \"model_size_mb\": get_model_size_mb(model_baseline),\n",
        "    **baseline_timing,\n",
        "    **baseline_metrics,\n",
        "}]\n",
        "print(f\"Baseline - Accuracy: {baseline_metrics['accuracy']:.4f}, F1: {baseline_metrics['f1_macro']:.4f}\")\n",
        "print(f\"Baseline - Throughput: {baseline_timing['eval_samples_per_s']:.1f} samples/sec\")\n",
        "\n",
        "device = finbert_amp.device\n",
        "if device.type == \"cuda\":\n",
        "    # Load model with FP16 weights\n",
        "    fp16_amp_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        cl_path_amp, num_labels=3, dtype=torch.float16\n",
        "    ).to(device)\n",
        "\n",
        "    # AMP autocast + FP16 weights\n",
        "    amp_fp16_eval_df, amp_fp16_timing = timed_eval(\n",
        "        finbert=finbert_amp, model=fp16_amp_model, examples=test_data_amp, use_amp=True\n",
        "    )\n",
        "    amp_fp16_metrics = get_metrics(amp_fp16_eval_df)\n",
        "\n",
        "    results_records.append({\n",
        "        \"variant\": \"amp_fp16_weights\",\n",
        "        \"model_size_mb\": get_model_size_mb(fp16_amp_model),\n",
        "        **amp_fp16_timing,\n",
        "        **amp_fp16_metrics,\n",
        "    })\n",
        "    print(f\"AMP+FP16 - Accuracy: {amp_fp16_metrics['accuracy']:.4f}, F1: {amp_fp16_metrics['f1_macro']:.4f}\")\n",
        "    print(f\"AMP+FP16 - Throughput: {amp_fp16_timing['eval_samples_per_s']:.1f} samples/sec\")\n",
        "else:\n",
        "    print(\"AMP+FP16 eval requires CUDA\")\n",
        "\n",
        "results_df = pd.DataFrame(results_records)\n",
        "\n",
        "# ---- Added statistics ----\n",
        "if len(results_df) > 1:\n",
        "    base = results_df.iloc[0]\n",
        "    amp = results_df.iloc[1]\n",
        "    # Relative speedup: how much faster is AMP+FP16 throughput\n",
        "    speedup = amp['eval_samples_per_s'] / base['eval_samples_per_s'] if base['eval_samples_per_s'] > 0 else float('nan')\n",
        "    # Compression: how much smaller is the model (MB)\n",
        "    compression = amp['model_size_mb'] / base['model_size_mb'] if base['model_size_mb'] > 0 else float('nan')\n",
        "    compression_str = f\"{(1/compression):.2f}x\" if compression > 0 else \"n/a\"\n",
        "    # Accuracy delta: amp - baseline\n",
        "    acc_delta = amp['accuracy'] - base['accuracy']\n",
        "    print(f\"\\n[STATS]\")\n",
        "    print(f\"Relative speedup (AMP+FP16 vs. baseline): {speedup:.2f}x\")\n",
        "    print(f\"Model size compression (AMP+FP16 vs. baseline): {compression_str} smaller \"\n",
        "          f\"({amp['model_size_mb']:.1f}MB vs {base['model_size_mb']:.1f}MB)\")\n",
        "    print(f\"Accuracy delta (AMP+FP16 minus baseline): {acc_delta:+.3f}\")\n",
        "\n",
        "    # Optionally also store in DataFrame\n",
        "    stats_row = {\n",
        "        \"variant\": \"amp_fp16_vs_baseline\",\n",
        "        \"speedup_x\": speedup,\n",
        "        \"compression_x\": (1/compression) if compression > 0 else float('nan'),\n",
        "        \"accuracy_delta\": acc_delta,\n",
        "        \"base_accuracy\": base['accuracy'],\n",
        "        \"amp_fp16_accuracy\": amp['accuracy']\n",
        "    }\n",
        "    # This row for stats, not normal metric row\n",
        "    results_df = pd.concat([results_df, pd.DataFrame([stats_row])], ignore_index=True)\n",
        "\n",
        "results_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>amp_fp16_trained_accuracy</td><td>▁</td></tr><tr><td>amp_fp16_trained_f1</td><td>▁</td></tr><tr><td>amp_fp16_trained_model_size</td><td>▁</td></tr><tr><td>amp_fp16_trained_throughput</td><td>▁</td></tr><tr><td>baseline_accuracy</td><td>▁</td></tr><tr><td>baseline_f1</td><td>▁</td></tr><tr><td>baseline_model_size</td><td>▁</td></tr><tr><td>baseline_throughput</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>amp_fp16_trained_accuracy</td><td>0.77216</td></tr><tr><td>amp_fp16_trained_f1</td><td>0.75546</td></tr><tr><td>amp_fp16_trained_model_size</td><td>127.71192</td></tr><tr><td>amp_fp16_trained_throughput</td><td>872.42683</td></tr><tr><td>baseline_accuracy</td><td>0.72371</td></tr><tr><td>baseline_f1</td><td>0.68521</td></tr><tr><td>baseline_model_size</td><td>417.65821</td></tr><tr><td>baseline_throughput</td><td>154.88162</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">master-profile</strong> at: <a href='https://wandb.ai/si2449-columbia-university/finbert-experiments/runs/x292gz3q' target=\"_blank\">https://wandb.ai/si2449-columbia-university/finbert-experiments/runs/x292gz3q</a><br> View project at: <a href='https://wandb.ai/si2449-columbia-university/finbert-experiments' target=\"_blank\">https://wandb.ai/si2449-columbia-university/finbert-experiments</a><br>Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20251217_033418-x292gz3q/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Log summary to W&B with both baseline and AMP/FP16 metrics (if available)\n",
        "summary = {}\n",
        "\n",
        "# Always add baseline summary stats\n",
        "summary.update({\n",
        "    \"baseline_accuracy\": baseline_metrics[\"accuracy\"],\n",
        "    \"baseline_f1\": baseline_metrics[\"f1_macro\"],\n",
        "    \"baseline_throughput\": baseline_timing[\"eval_samples_per_s\"],\n",
        "    \"baseline_model_size\": get_model_size_mb(model_baseline),\n",
        "})\n",
        "\n",
        "# Add AMP/FP16 statistics if on CUDA and results available\n",
        "if device.type == \"cuda\":\n",
        "    summary.update({\n",
        "        \"amp_fp16_trained_accuracy\": amp_fp16_metrics[\"accuracy\"],\n",
        "        \"amp_fp16_trained_f1\": amp_fp16_metrics[\"f1_macro\"],\n",
        "        \"amp_fp16_trained_throughput\": amp_fp16_timing[\"eval_samples_per_s\"],\n",
        "        \"amp_fp16_trained_model_size\": get_model_size_mb(fp16_amp_model),\n",
        "    })\n",
        "\n",
        "wandb.log(summary)\n",
        "\n",
        "# We'll upload the results_df (or plot_df) as a W&B Table for interactive dashboards\n",
        "\n",
        "# Option 1: upload the underlying DataFrame as a W&B Table\n",
        "# (works best if your DataFrame columns cover all metrics you'd like to compare)\n",
        "table = wandb.Table(dataframe=results_df)\n",
        "\n",
        "wandb.log({\"variant_summary_table\": table})\n",
        "\n",
        "wandb.finish()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
