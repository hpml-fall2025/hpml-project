{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Memory Optimized FinBERT Profiling: FP16 and AMP\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note: **AMP and FP16 are only enabled on CUDA by default** (to avoid MPS/CPU autocast edge cases).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imports\n",
        "We import the necessary libraries and modules, including the custom `finbert` modules we have defined that allow for profiling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "from pathlib import Path\n",
        "import shutil\n",
        "import time\n",
        "import sys\n",
        "sys.path.append('..')\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from sklearn.metrics import classification_report\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "from finbert.finbert import *\n",
        "from finbert.finbert_profile import *\n",
        "from finbert.profile_utils import get_model_size_mb, print_device_info, setup_nltk_data\n",
        "import finbert.utils as tools\n",
        "\n",
        "import wandb\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "project_dir = Path.cwd().parent\n",
        "pd.set_option('max_colwidth', None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Paths\n",
        "cl_path = project_dir / 'models' / 'mem_opt_comparison'\n",
        "cl_path_baseline = project_dir / 'models' / 'mem_opt_comparison' / 'baseline'\n",
        "cl_path_amp = project_dir / 'models' / 'mem_opt_comparison' / 'amp'\n",
        "cl_data_path = project_dir / 'data' / 'sentiment_data'\n",
        "\n",
        "# Clean up previous run\n",
        "try:\n",
        "    shutil.rmtree(cl_path)\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# # W&B\n",
        "wandb.init(\n",
        "    entity=\"si2449-columbia-university\",\n",
        "    project=\"finbert-experiments\",\n",
        "    name=\"mem-opt-comparison\",\n",
        "    group=\"mem_optimization\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Baseline (FP32) model and training\n",
        "bertmodel_fp32 = AutoModelForSequenceClassification.from_pretrained(\n",
        "    'bert-base-uncased', cache_dir=None, num_labels=3\n",
        ")\n",
        "\n",
        "config_baseline = Config(\n",
        "    data_dir=cl_data_path,\n",
        "    bert_model=bertmodel_fp32,\n",
        "    num_train_epochs=6,\n",
        "    model_dir=cl_path_baseline,  # Changed from cl_path\n",
        "    max_seq_length=64,\n",
        "    train_batch_size=32,\n",
        "    learning_rate=0.00001420326287435756,\n",
        "    output_mode='classification',\n",
        "    warm_up_proportion=0.14386028719686458,\n",
        "    local_rank=-1,\n",
        "    discriminate=False,\n",
        "    gradual_unfreeze=False,\n",
        "    use_amp=False,  # Baseline uses FP32\n",
        ")\n",
        "config_baseline.profile_train_steps = 20\n",
        "\n",
        "finbert_fp32 = ProfiledFinBert(config_baseline)\n",
        "finbert_fp32.base_model = 'bert-base-uncased'\n",
        "finbert_fp32.prepare_model(label_list=['positive', 'negative', 'neutral'])\n",
        "\n",
        "train_data = finbert_fp32.get_data('train')\n",
        "test_data = finbert_fp32.get_data('test')\n",
        "\n",
        "model_fp32 = finbert_fp32.create_the_model()\n",
        "\n",
        "# Train FP32\n",
        "start = time.perf_counter()\n",
        "trained_model_fp32 = finbert_fp32.train(train_examples=train_data, model=model_fp32)\n",
        "baseline_train_wall_s = time.perf_counter() - start\n",
        "\n",
        "# AMP model and training\n",
        "bertmodel_amp = AutoModelForSequenceClassification.from_pretrained(\n",
        "    'bert-base-uncased', cache_dir=None, num_labels=3\n",
        ")\n",
        "\n",
        "config_amp = Config(\n",
        "    data_dir=cl_data_path,\n",
        "    bert_model=bertmodel_amp,\n",
        "    num_train_epochs=6,\n",
        "    model_dir=cl_path_amp,  # Changed from cl_path\n",
        "    max_seq_length=64,\n",
        "    train_batch_size=32,\n",
        "    learning_rate=0.00001420326287435756,\n",
        "    output_mode='classification',\n",
        "    warm_up_proportion=0.14386028719686458,\n",
        "    local_rank=-1,\n",
        "    discriminate=False,\n",
        "    gradual_unfreeze=False,\n",
        "    use_amp=True,  # Enable AMP\n",
        ")\n",
        "config_amp.profile_train_steps = 20\n",
        "\n",
        "finbert_amp = ProfiledFinBert(config_amp)\n",
        "finbert_amp.base_model = 'bert-base-uncased'\n",
        "finbert_amp.prepare_model(label_list=['positive', 'negative', 'neutral'])\n",
        "\n",
        "train_data = finbert_amp.get_data('train')\n",
        "test_data = finbert_amp.get_data('test')\n",
        "\n",
        "# train_data and test_data already loaded above\n",
        "model_amp = finbert_amp.create_the_model()\n",
        "    \n",
        "# Train AMP\n",
        "start = time.perf_counter()\n",
        "trained_model_amp = finbert_amp.train(train_examples=train_data, model=model_amp)\n",
        "amp_train_wall_s = time.perf_counter() - start\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def timed_eval(*, finbert, model, examples, use_amp=False):\n",
        "    \"\"\"Evaluation with timing.\"\"\"\n",
        "    loader = finbert.get_loader(examples, phase=\"eval\")\n",
        "    device = finbert.device\n",
        "    model.eval()\n",
        "    \n",
        "    preds, labels = [], []\n",
        "    if device.type == \"cuda\":\n",
        "        torch.cuda.synchronize(device)\n",
        "    \n",
        "    start = time.perf_counter()\n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            batch = tuple(t.to(device) for t in batch)\n",
        "            input_ids, attention_mask, token_type_ids, label_ids, _ = batch\n",
        "            \n",
        "            if use_amp and device.type == \"cuda\":\n",
        "                with torch.amp.autocast('cuda'):\n",
        "                    logits = model(input_ids, attention_mask, token_type_ids)[0]\n",
        "            else:\n",
        "                logits = model(input_ids, attention_mask, token_type_ids)[0]\n",
        "            \n",
        "            preds.extend(logits.detach().cpu().numpy())\n",
        "            labels.extend(label_ids.detach().cpu().numpy().tolist())\n",
        "    \n",
        "    if device.type == \"cuda\":\n",
        "        torch.cuda.synchronize(device)\n",
        "    \n",
        "    wall_s = time.perf_counter() - start\n",
        "    n = len(labels)\n",
        "    \n",
        "    return pd.DataFrame({\"predictions\": preds, \"labels\": labels}), {\n",
        "        \"eval_wall_s\": wall_s,\n",
        "        \"eval_samples_per_s\": n / wall_s if wall_s > 0 else float(\"inf\"),\n",
        "    }\n",
        "\n",
        "def get_metrics(df):\n",
        "    \"\"\"Extract accuracy and F1 from eval results.\"\"\"\n",
        "    preds = np.array([np.argmax(p) for p in df['predictions']])\n",
        "    labels = np.array(df['labels'])\n",
        "    acc = (preds == labels).mean()\n",
        "    from sklearn.metrics import f1_score\n",
        "    f1 = f1_score(labels, preds, average='macro')\n",
        "    return {\"accuracy\": acc, \"f1_macro\": f1}\n",
        "\n",
        "# Evaluate baseline\n",
        "baseline_eval_df, baseline_timing = timed_eval(finbert=finbert_fp32, model=trained_model_fp32, examples=test_data, use_amp=False)\n",
        "baseline_metrics = get_metrics(baseline_eval_df)\n",
        "\n",
        "print(f\"Baseline - Accuracy: {baseline_metrics['accuracy']:.4f}, F1: {baseline_metrics['f1_macro']:.4f}\")\n",
        "print(f\"Baseline - Throughput: {baseline_timing['eval_samples_per_s']:.1f} samples/sec\")\n",
        "\n",
        "\n",
        "amp_trained_eval_df, amp_trained_timing = timed_eval(\n",
        "    finbert=finbert_amp, model=trained_model_amp, examples=test_data, use_amp=True\n",
        ")\n",
        "amp_trained_metrics = get_metrics(amp_trained_eval_df)\n",
        "print(f\"Trained AMP Model - Accuracy: {amp_trained_metrics['accuracy']:.4f}, F1: {amp_trained_metrics['f1_macro']:.4f}\")\n",
        "print(f\"Trained AMP Model - Throughput: {amp_trained_timing['eval_samples_per_s']:.1f} samples/sec\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = finbert_fp32.device\n",
        "all_results = []\n",
        "\n",
        "# ===== BASELINE =====\n",
        "all_results.append({\n",
        "    \"variant\": \"baseline\",\n",
        "    \"model_size_mb\": get_model_size_mb(trained_model_fp32),\n",
        "    **baseline_timing,\n",
        "    **baseline_metrics,\n",
        "})\n",
        "\n",
        "# ===== FP16 WEIGHTS =====\n",
        "if device.type == \"cuda\":\n",
        "    # Load model with FP16 weights\n",
        "    fp16_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        cl_path_baseline, num_labels=3, dtype=torch.float16  # Changed from cl_path\n",
        "    ).to(device)\n",
        "    \n",
        "    fp16_eval_df, fp16_timing = timed_eval(finbert=finbert_fp32, model=fp16_model, examples=test_data, use_amp=False)\n",
        "    fp16_metrics = get_metrics(fp16_eval_df)\n",
        "    \n",
        "    all_results.append({\n",
        "        \"variant\": \"fp16_weights\",\n",
        "        \"model_size_mb\": get_model_size_mb(fp16_model),\n",
        "        **fp16_timing,\n",
        "        **fp16_metrics,\n",
        "    })\n",
        "    print(f\"FP16 - Accuracy: {fp16_metrics['accuracy']:.4f}, F1: {fp16_metrics['f1_macro']:.4f}\")\n",
        "    print(f\"FP16 - Throughput: {fp16_timing['eval_samples_per_s']:.1f} samples/sec\")\n",
        "else:\n",
        "    print(\"Skipping FP16 (requires CUDA)\")\n",
        "\n",
        "# ===== AMP AUTOCAST =====\n",
        "if device.type == \"cuda\":\n",
        "    # Use baseline model with AMP autocast during inference\n",
        "    amp_eval_df, amp_timing = timed_eval(finbert=finbert_amp, model=trained_model_amp, examples=test_data, use_amp=True)\n",
        "    amp_metrics = get_metrics(amp_eval_df)\n",
        "    \n",
        "    all_results.append({\n",
        "        \"variant\": \"amp_autocast\",\n",
        "        \"model_size_mb\": get_model_size_mb(trained_model_amp),\n",
        "        **amp_timing,\n",
        "        **amp_metrics,\n",
        "    })\n",
        "    print(f\"AMP - Accuracy: {amp_metrics['accuracy']:.4f}, F1: {amp_metrics['f1_macro']:.4f}\")\n",
        "    print(f\"AMP - Throughput: {amp_timing['eval_samples_per_s']:.1f} samples/sec\")\n",
        "else:\n",
        "    print(\"Skipping AMP (requires CUDA)\")\n",
        "\n",
        "results_df = pd.DataFrame(all_results)\n",
        "results_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Stable ordering + nicer display names\n",
        "preferred_order = [\"baseline\", \"fp16_weights\", \"trained_amp\", \"amp_trained\", \"amp_autocast\"]\n",
        "present = results_df[\"variant\"].tolist()\n",
        "order = [v for v in preferred_order if v in present] + [v for v in present if v not in preferred_order]\n",
        "\n",
        "display_name = {\n",
        "    \"baseline\": \"Baseline (FP32)\",\n",
        "    \"fp16_weights\": \"FP16 weights\",\n",
        "    \"trained_amp\": \"Trained AMP (use_amp=True)\",\n",
        "    \"amp_trained\": \"Trained AMP (use_amp=True)\",\n",
        "    \"amp_autocast\": \"Trained AMP (use_amp=True)\",\n",
        "}\n",
        "\n",
        "variant_color = {\n",
        "    \"baseline\": \"#2ecc71\",\n",
        "    \"fp16_weights\": \"#3498db\",\n",
        "    \"trained_amp\": \"#e74c3c\",\n",
        "    \"amp_trained\": \"#e74c3c\",\n",
        "    \"amp_autocast\": \"#e74c3c\",\n",
        "}\n",
        "\n",
        "plot_df = results_df.set_index(\"variant\").loc[order]\n",
        "colors = [variant_color.get(v, \"#95a5a6\") for v in plot_df.index]\n",
        "xticklabels = [display_name.get(v, v) for v in plot_df.index]\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
        "\n",
        "# Throughput\n",
        "ax = axes[0]\n",
        "plot_df[\"eval_samples_per_s\"].plot(kind=\"bar\", ax=ax, color=colors)\n",
        "ax.set_title(\"Inference Throughput\")\n",
        "ax.set_ylabel(\"samples/sec\")\n",
        "ax.set_xticklabels(xticklabels, rotation=0)\n",
        "\n",
        "# Accuracy\n",
        "ax = axes[1]\n",
        "plot_df[\"accuracy\"].plot(kind=\"bar\", ax=ax, color=colors)\n",
        "ax.set_title(\"Accuracy\")\n",
        "ax.set_ylabel(\"accuracy\")\n",
        "ax.set_ylim(0.7, 1.0)\n",
        "ax.set_xticklabels(xticklabels, rotation=0)\n",
        "\n",
        "# Model Size\n",
        "ax = axes[2]\n",
        "plot_df[\"model_size_mb\"].plot(kind=\"bar\", ax=ax, color=colors)\n",
        "ax.set_title(\"Model Size\")\n",
        "ax.set_ylabel(\"MB\")\n",
        "ax.set_xticklabels(xticklabels, rotation=0)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Speedup summary\n",
        "if len(plot_df) > 1 and \"baseline\" in plot_df.index:\n",
        "    base = plot_df.loc[\"baseline\"]\n",
        "    print(\"\\n=== Speedup vs Baseline ===\")\n",
        "    for variant, row in plot_df.iterrows():\n",
        "        if variant == \"baseline\":\n",
        "            continue\n",
        "        speedup = row[\"eval_samples_per_s\"] / base[\"eval_samples_per_s\"]\n",
        "        delta_acc = row[\"accuracy\"] - base[\"accuracy\"]\n",
        "        print(f\"{display_name.get(variant, variant)}: {speedup:.2f}x speedup, Î”accuracy={delta_acc:+.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Log summary to W&B\n",
        "summary = {\n",
        "    \"baseline_train_wall_s\": baseline_train_wall_s,\n",
        "    \"baseline_accuracy\": baseline_metrics[\"accuracy\"],\n",
        "    \"baseline_f1\": baseline_metrics[\"f1_macro\"],\n",
        "    \"baseline_throughput\": baseline_timing[\"eval_samples_per_s\"],\n",
        "    \"baseline_model_size\": get_model_size_mb(trained_model_fp32),\n",
        "}\n",
        "\n",
        "if device.type == \"cuda\":\n",
        "    summary.update({\n",
        "        \"fp16_accuracy\": fp16_metrics[\"accuracy\"],\n",
        "        \"fp16_throughput\": fp16_timing[\"eval_samples_per_s\"],\n",
        "        \"fp16_model_size\": get_model_size_mb(fp16_model),\n",
        "        \"amp_train_wall_s\": amp_train_wall_s,\n",
        "        \"amp_accuracy\": amp_metrics[\"accuracy\"],\n",
        "        \"amp_f1\": amp_metrics[\"f1_macro\"],\n",
        "        \"amp_throughput\": amp_timing[\"eval_samples_per_s\"],\n",
        "        \"amp_model_size\": get_model_size_mb(trained_model_amp),\n",
        "    })\n",
        "\n",
        "wandb.log(summary)\n",
        "\n",
        "# We'll upload the results_df (or plot_df) as a W&B Table for interactive dashboards\n",
        "\n",
        "# Option 1: upload the underlying DataFrame as a W&B Table\n",
        "# (works best if your DataFrame columns cover all metrics you'd like to compare)\n",
        "table = wandb.Table(dataframe=results_df)\n",
        "\n",
        "wandb.log({\"variant_summary_table\": table})\n",
        "\n",
        "wandb.finish()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
