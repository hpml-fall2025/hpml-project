{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# FinBERT Profiling: Baseline vs FP16 vs AMP\n",
        "\n",
        "This notebook is intentionally **thin**: it reuses the profiling utilities in `pipelines/finBERT/finbert/` (especially `finbert/finbert_profile.py` and `finbert/profile_utils.py`) instead of copying large code blocks.\n",
        "\n",
        "### What this notebook compares\n",
        "- **Training**: Baseline (FP32) vs **AMP** (autocast + GradScaler) — wall time + profiler breakdown + accuracy\n",
        "- **Inference** (from the **baseline checkpoint**): FP32 vs **FP16-weights** vs **AMP-autocast** — throughput/latency + accuracy\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Quick start\n",
        "\n",
        "1. Run **Setup + Config + Helpers** (cells 2–4).\n",
        "2. Run **Training** (cell 5) to train/eval **baseline** and **AMP**.\n",
        "3. Run **Inference benchmarking** (cell 6) to benchmark **baseline / FP16-weights / AMP-autocast** off the baseline checkpoint.\n",
        "4. Run **Results** (cell 7) to see comparison tables and plots.\n",
        "\n",
        "### Notes\n",
        "- **AMP and FP16 are only enabled on CUDA by default** (to avoid MPS/CPU autocast edge cases).\n",
        "- If you already have a trained checkpoint you want to use, set `BASELINE_CKPT_DIR` in the config cell and set `RUN_TRAIN_BASELINE = False`.\n",
        "- This notebook follows the same “thin notebook, modules do the work” style as `finbert_sweep.ipynb`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "from pathlib import Path\n",
        "import sys\n",
        "import time\n",
        "import warnings\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "# Silence noisy deprecations (we still keep runtime-correct code)\n",
        "warnings.filterwarnings(\n",
        "    \"ignore\",\n",
        "    category=FutureWarning,\n",
        "    message=\"`torch\\\\.cuda\\\\.amp\\\\.autocast\\\\(args\\\\.\\\\.\\\\.\\\\)` is deprecated\",\n",
        ")\n",
        "\n",
        "\n",
        "def _find_finbert_root(start: Path) -> Path:\n",
        "    \"\"\"Return the directory that contains the `finbert/` package.\"\"\"\n",
        "    start = start.resolve()\n",
        "    for p in [start, *start.parents]:\n",
        "        if (p / \"finbert\").is_dir() and (p / \"finbert\" / \"finbert.py\").exists():\n",
        "            return p\n",
        "        if (p / \"pipelines\" / \"finBERT\" / \"finbert\").is_dir():\n",
        "            return p / \"pipelines\" / \"finBERT\"\n",
        "    raise FileNotFoundError(\n",
        "        \"Could not locate finBERT root. Expected a folder containing finbert/finbert.py\"\n",
        "    )\n",
        "\n",
        "\n",
        "FINBERT_ROOT = _find_finbert_root(Path.cwd())\n",
        "if str(FINBERT_ROOT) not in sys.path:\n",
        "    sys.path.insert(0, str(FINBERT_ROOT))\n",
        "\n",
        "PROJECT_ROOT = FINBERT_ROOT.parents[1] if FINBERT_ROOT.name == \"finBERT\" else FINBERT_ROOT.parent\n",
        "\n",
        "from finbert.finbert import Config, FinBert\n",
        "from finbert.finbert_profile import ProfiledFinBert, profile_inference\n",
        "from finbert.profile_utils import get_model_size_mb, print_device_info, setup_nltk_data\n",
        "from finbert.utils import get_device\n",
        "\n",
        "LABEL_LIST = [\"positive\", \"negative\", \"neutral\"]\n",
        "\n",
        "pd.set_option(\"display.max_colwidth\", 120)\n",
        "\n",
        "print(\"✓ FINBERT_ROOT:\", FINBERT_ROOT)\n",
        "print(\"✓ PROJECT_ROOT:\", PROJECT_ROOT)\n",
        "print(\"✓ torch:\", torch.__version__)\n",
        "print(\"✓ cuda available:\", torch.cuda.is_available())\n",
        "print(\"✓ mps available:\", hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =========================\n",
        "# Experiment configuration\n",
        "# =========================\n",
        "\n",
        "BASE_MODEL_NAME = \"bert-base-uncased\"\n",
        "\n",
        "DATA_DIR = FINBERT_ROOT / \"data\" / \"sentiment_data\"\n",
        "\n",
        "# Where to write new trained models (kept separate from the shipped `models/sentiment` checkpoint)\n",
        "RUNS_DIR = FINBERT_ROOT / \"models\" / \"profiling_runs\"\n",
        "RUNS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# If you want to skip training, point this at an existing checkpoint directory\n",
        "BASELINE_CKPT_DIR = FINBERT_ROOT / \"models\" / \"sentiment\"\n",
        "\n",
        "# Training params (using optimal hyperparameters from W&B sweep: solar-sweep-21)\n",
        "TRAINING = dict(\n",
        "    num_train_epochs=6,\n",
        "    train_batch_size=32,\n",
        "    eval_batch_size=32,\n",
        "    learning_rate=1.4e-5,\n",
        "    warm_up_proportion=0.144,\n",
        "    max_seq_length=64,\n",
        "    discriminate=False,\n",
        "    gradual_unfreeze=False,\n",
        ")\n",
        "\n",
        "# Profiling params\n",
        "PROFILE_TRAIN_STEPS = 20  # first N optimizer steps to profile during training\n",
        "\n",
        "INFER_TEXT_BATCH_SIZE = 5\n",
        "INFER_TEST_TEXT = \"\"\"Later that day Apple said it was revising down its earnings expectations in \\\n",
        "The fourth quarter of 2018, largely because of lower sales and signs of economic weakness in China. \\\n",
        "The news rapidly infected financial markets. Apple's share price fell by around 7% in after-hours \\\n",
        "Trading and the decline was extended to more than 10% when the market opened. The dollar fell \\\n",
        "By 3.7% against the yen in a matter of minutes after the announcement, before rapidly recovering \\\n",
        "Some ground. Asian stockmarkets closed down on January 3rd and European ones opened lower. \\\n",
        "Yields on government bonds fell as investors fled to the traditional haven in a market storm.\"\"\"\n",
        "\n",
        "# Device settings\n",
        "PREFER_GPU = True\n",
        "GPU_NAME = \"cuda:0\"  # only used when CUDA is present\n",
        "\n",
        "# Optional W&B logging (same idea as `finbert_sweep.ipynb`)\n",
        "USE_WANDB = False\n",
        "WANDB_ENTITY = \"si2449-columbia-university\"\n",
        "WANDB_PROJECT = \"finbert-profiling\"\n",
        "WANDB_GROUP = \"profiling\"\n",
        "\n",
        "# Which runs to execute\n",
        "RUN_TRAIN_BASELINE = True\n",
        "RUN_TRAIN_AMP = True\n",
        "RUN_INFER_BENCHMARKS = True\n",
        "\n",
        "# Safety\n",
        "OVERWRITE_EXISTING_RUN_DIRS = True\n",
        "\n",
        "# Variants (edit here to add/remove)\n",
        "INFERENCE_VARIANTS = {\n",
        "    \"baseline\": {\"torch_dtype\": None, \"use_amp\": False, \"requires_cuda\": False},\n",
        "    \"fp16_weights\": {\"torch_dtype\": torch.float16, \"use_amp\": False, \"requires_cuda\": True},\n",
        "    \"amp_autocast\": {\"torch_dtype\": None, \"use_amp\": True, \"requires_cuda\": True},\n",
        "}\n",
        "\n",
        "print(\"✓ DATA_DIR:\", DATA_DIR)\n",
        "print(\"✓ RUNS_DIR:\", RUNS_DIR)\n",
        "print(\"✓ BASELINE_CKPT_DIR:\", BASELINE_CKPT_DIR)\n",
        "print(\"✓ INFERENCE_VARIANTS:\", list(INFERENCE_VARIANTS.keys()))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "from contextlib import nullcontext\n",
        "from typing import Any\n",
        "\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "\n",
        "def resolve_device(prefer_gpu: bool = True, gpu_name: str = \"cuda:0\") -> torch.device:\n",
        "    if not prefer_gpu:\n",
        "        return torch.device(\"cpu\")\n",
        "\n",
        "    device = get_device(no_cuda=False)\n",
        "    if device.type == \"cuda\" and gpu_name.startswith(\"cuda:\"):\n",
        "        return torch.device(gpu_name)\n",
        "    return device\n",
        "\n",
        "\n",
        "def autocast_ctx(device: torch.device, enabled: bool):\n",
        "    if not enabled:\n",
        "        return nullcontext()\n",
        "    if device.type != \"cuda\":\n",
        "        # Keep this conservative: AMP is only enabled on CUDA in this notebook.\n",
        "        return nullcontext()\n",
        "    try:\n",
        "        return torch.amp.autocast(device_type=\"cuda\", enabled=True)\n",
        "    except Exception:\n",
        "        # Older torch fallback\n",
        "        return torch.cuda.amp.autocast(enabled=True)\n",
        "\n",
        "\n",
        "def make_run_dir(prefix: str) -> Path:\n",
        "    ts = time.strftime(\"%Y%m%d_%H%M%S\")\n",
        "    run_dir = RUNS_DIR / f\"{prefix}_{ts}\"\n",
        "\n",
        "    if run_dir.exists() and OVERWRITE_EXISTING_RUN_DIRS:\n",
        "        import shutil\n",
        "\n",
        "        shutil.rmtree(run_dir)\n",
        "\n",
        "    if run_dir.exists() and any(run_dir.iterdir()):\n",
        "        raise ValueError(f\"Run dir exists and is not empty: {run_dir}\")\n",
        "\n",
        "    return run_dir\n",
        "\n",
        "\n",
        "def make_finbert_config(*, model_dir: Path, use_amp: bool) -> Config:\n",
        "    # Guard AMP to CUDA only (FinBERT training uses torch.cuda.amp.*)\n",
        "    use_amp = bool(use_amp and torch.cuda.is_available())\n",
        "\n",
        "    bertmodel = AutoModelForSequenceClassification.from_pretrained(\n",
        "        BASE_MODEL_NAME, cache_dir=None, num_labels=3\n",
        "    )\n",
        "\n",
        "    cfg = Config(\n",
        "        data_dir=DATA_DIR,\n",
        "        bert_model=bertmodel,\n",
        "        model_dir=model_dir,\n",
        "        max_seq_length=TRAINING[\"max_seq_length\"],\n",
        "        train_batch_size=TRAINING[\"train_batch_size\"],\n",
        "        eval_batch_size=TRAINING[\"eval_batch_size\"],\n",
        "        learning_rate=TRAINING[\"learning_rate\"],\n",
        "        num_train_epochs=TRAINING[\"num_train_epochs\"],\n",
        "        warm_up_proportion=TRAINING[\"warm_up_proportion\"],\n",
        "        local_rank=-1,\n",
        "        output_mode=\"classification\",\n",
        "        discriminate=TRAINING[\"discriminate\"],\n",
        "        gradual_unfreeze=TRAINING[\"gradual_unfreeze\"],\n",
        "        fp16=False,\n",
        "        use_amp=use_amp,\n",
        "    )\n",
        "\n",
        "    # Read by `ProfiledFinBert.train()` via `getattr(self.config, 'profile_train_steps', 20)`\n",
        "    cfg.profile_train_steps = PROFILE_TRAIN_STEPS\n",
        "\n",
        "    return cfg\n",
        "\n",
        "\n",
        "def calculate_metrics(results_df: pd.DataFrame) -> dict[str, float]:\n",
        "    \"\"\"Sweep-style metrics: CE loss + accuracy + macro/per-class F1.\"\"\"\n",
        "    y_true = np.asarray(results_df[\"labels\"], dtype=np.int64)\n",
        "    logits = np.stack(results_df[\"predictions\"].to_numpy())\n",
        "    y_pred = logits.argmax(axis=1)\n",
        "\n",
        "    loss = F.cross_entropy(torch.tensor(logits, dtype=torch.float32), torch.tensor(y_true)).item()\n",
        "    acc = float((y_true == y_pred).mean())\n",
        "\n",
        "    f1_per = f1_score(y_true, y_pred, average=None, labels=[0, 1, 2])\n",
        "    f1_macro = float(f1_score(y_true, y_pred, average=\"macro\"))\n",
        "\n",
        "    return {\n",
        "        \"loss\": float(loss),\n",
        "        \"accuracy\": acc,\n",
        "        \"f1_positive\": float(f1_per[0]),\n",
        "        \"f1_negative\": float(f1_per[1]),\n",
        "        \"f1_neutral\": float(f1_per[2]),\n",
        "        \"f1_macro\": f1_macro,\n",
        "    }\n",
        "\n",
        "\n",
        "def timed_eval(\n",
        "    *, finbert: FinBert, model: torch.nn.Module, examples, use_amp: bool\n",
        ") -> tuple[pd.DataFrame, dict[str, Any]]:\n",
        "    \"\"\"Evaluation loop with optional CUDA autocast + timing (kept small for notebook use).\"\"\"\n",
        "    loader = finbert.get_loader(examples, phase=\"eval\")\n",
        "    device = finbert.device\n",
        "\n",
        "    model.eval()\n",
        "    preds: list[np.ndarray] = []\n",
        "    labels: list[int] = []\n",
        "\n",
        "    if device.type == \"cuda\":\n",
        "        torch.cuda.synchronize(device)\n",
        "\n",
        "    start = time.perf_counter()\n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            batch = tuple(t.to(device) for t in batch)\n",
        "            input_ids, attention_mask, token_type_ids, label_ids, _agree_ids = batch\n",
        "\n",
        "            with autocast_ctx(device, enabled=bool(use_amp)):\n",
        "                logits = model(input_ids, attention_mask, token_type_ids)[0]\n",
        "\n",
        "            preds.extend(logits.detach().cpu().numpy())\n",
        "            labels.extend(label_ids.detach().cpu().numpy().tolist())\n",
        "\n",
        "    if device.type == \"cuda\":\n",
        "        torch.cuda.synchronize(device)\n",
        "\n",
        "    wall_s = time.perf_counter() - start\n",
        "    n = len(labels)\n",
        "\n",
        "    results_df = pd.DataFrame({\"predictions\": preds, \"labels\": labels})\n",
        "\n",
        "    timing = {\n",
        "        \"eval_wall_s\": float(wall_s),\n",
        "        \"eval_num_samples\": int(n),\n",
        "        \"eval_samples_per_s\": float(n / wall_s) if wall_s > 0 else float(\"inf\"),\n",
        "    }\n",
        "    return results_df, timing\n",
        "\n",
        "\n",
        "def maybe_wandb_init(run_name: str, config: dict[str, Any]):\n",
        "    if not USE_WANDB:\n",
        "        return None\n",
        "    try:\n",
        "        import wandb\n",
        "\n",
        "        return wandb.init(\n",
        "            entity=WANDB_ENTITY,\n",
        "            project=WANDB_PROJECT,\n",
        "            group=WANDB_GROUP,\n",
        "            name=run_name,\n",
        "            config=config,\n",
        "        )\n",
        "    except ImportError:\n",
        "        print(\"⚠ wandb is not installed; set USE_WANDB=False or install wandb\")\n",
        "        return None\n",
        "\n",
        "\n",
        "print(\"✓ Helper functions loaded\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =========================\n",
        "# Training: baseline vs AMP\n",
        "# =========================\n",
        "\n",
        "if not DATA_DIR.exists():\n",
        "    raise FileNotFoundError(\n",
        "        f\"DATA_DIR not found: {DATA_DIR}\\n\"\n",
        "        \"Create train/validation/test TSVs under data/sentiment_data (see pipelines/finBERT/README.md).\"\n",
        "    )\n",
        "\n",
        "device = resolve_device(PREFER_GPU, GPU_NAME)\n",
        "print_device_info(device)\n",
        "\n",
        "training_summaries: list[dict[str, Any]] = []\n",
        "trained_ckpts: dict[str, Path] = {}\n",
        "\n",
        "\n",
        "def run_train(tag: str, *, use_amp: bool) -> tuple[dict[str, Any], Path]:\n",
        "    run_dir = make_run_dir(tag)\n",
        "\n",
        "    cfg = make_finbert_config(model_dir=run_dir, use_amp=use_amp)\n",
        "\n",
        "    finbert = ProfiledFinBert(cfg)\n",
        "    finbert.base_model = BASE_MODEL_NAME\n",
        "    finbert.prepare_model(label_list=LABEL_LIST)\n",
        "\n",
        "    train_data = finbert.get_data(\"train\")\n",
        "    test_data = finbert.get_data(\"test\")\n",
        "\n",
        "    model = finbert.create_the_model()\n",
        "\n",
        "    wb = maybe_wandb_init(\n",
        "        run_name=f\"train-{tag}\",\n",
        "        config={\"tag\": tag, \"use_amp\": bool(use_amp), **TRAINING},\n",
        "    )\n",
        "\n",
        "    start = time.perf_counter()\n",
        "    trained_model = finbert.train(train_examples=train_data, model=model)\n",
        "    train_wall_s = time.perf_counter() - start\n",
        "\n",
        "    eval_df, eval_timing = timed_eval(\n",
        "        finbert=finbert, model=trained_model, examples=test_data, use_amp=bool(use_amp)\n",
        "    )\n",
        "    metrics = calculate_metrics(eval_df)\n",
        "\n",
        "    summary = {\n",
        "        \"run\": tag,\n",
        "        \"use_amp\": bool(use_amp and torch.cuda.is_available()),\n",
        "        \"device\": str(finbert.device),\n",
        "        \"model_dir\": str(run_dir),\n",
        "        \"train_wall_s\": float(train_wall_s),\n",
        "        \"train_examples\": int(len(train_data)),\n",
        "        \"train_examples_per_s\": float((len(train_data) * TRAINING[\"num_train_epochs\"]) / train_wall_s)\n",
        "        if train_wall_s > 0\n",
        "        else float(\"inf\"),\n",
        "        \"model_size_mb\": float(get_model_size_mb(trained_model)),\n",
        "        \"profile_train_steps\": int(getattr(cfg, \"profile_train_steps\", PROFILE_TRAIN_STEPS)),\n",
        "        **(finbert.profile_results.get(\"training_summary\", {}) or {}),\n",
        "        **eval_timing,\n",
        "        **metrics,\n",
        "    }\n",
        "\n",
        "    if wb is not None:\n",
        "        import wandb\n",
        "\n",
        "        wandb.log(summary)\n",
        "        wb.finish()\n",
        "\n",
        "    return summary, run_dir\n",
        "\n",
        "\n",
        "if RUN_TRAIN_BASELINE:\n",
        "    baseline_summary, baseline_dir = run_train(\"baseline\", use_amp=False)\n",
        "    training_summaries.append(baseline_summary)\n",
        "    trained_ckpts[\"baseline\"] = baseline_dir\n",
        "\n",
        "    # Use the freshly trained baseline checkpoint for inference benchmarking\n",
        "    BASELINE_CKPT_DIR = baseline_dir\n",
        "    print(\"✓ BASELINE_CKPT_DIR set to:\", BASELINE_CKPT_DIR)\n",
        "else:\n",
        "    print(\"Skipping baseline training. Using BASELINE_CKPT_DIR =\", BASELINE_CKPT_DIR)\n",
        "\n",
        "if RUN_TRAIN_AMP:\n",
        "    if not torch.cuda.is_available():\n",
        "        print(\"⚠ RUN_TRAIN_AMP=True but CUDA is not available — skipping AMP training.\")\n",
        "    else:\n",
        "        amp_summary, amp_dir = run_train(\"amp\", use_amp=True)\n",
        "        training_summaries.append(amp_summary)\n",
        "        trained_ckpts[\"amp\"] = amp_dir\n",
        "else:\n",
        "    print(\"Skipping AMP training.\")\n",
        "\n",
        "train_summary_df = pd.DataFrame(training_summaries)\n",
        "train_summary_df\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ================================================\n",
        "# Inference benchmarking: baseline ckpt variations\n",
        "# ================================================\n",
        "\n",
        "if RUN_INFER_BENCHMARKS:\n",
        "    device = resolve_device(PREFER_GPU, GPU_NAME)\n",
        "    use_gpu = device.type != \"cpu\"\n",
        "\n",
        "    print_device_info(device)\n",
        "    print(\"✓ Using baseline checkpoint:\", BASELINE_CKPT_DIR)\n",
        "\n",
        "    # Ensure sentence tokenizer is available\n",
        "    setup_nltk_data()\n",
        "\n",
        "    # FinBERT instance for data loading + batching (model_dir must be empty)\n",
        "    eval_dir = make_run_dir(\"_eval\")\n",
        "    eval_cfg = Config(\n",
        "        data_dir=DATA_DIR,\n",
        "        bert_model=None,\n",
        "        model_dir=eval_dir,\n",
        "        max_seq_length=TRAINING[\"max_seq_length\"],\n",
        "        train_batch_size=TRAINING[\"eval_batch_size\"],\n",
        "        eval_batch_size=TRAINING[\"eval_batch_size\"],\n",
        "        learning_rate=TRAINING[\"learning_rate\"],\n",
        "        num_train_epochs=1,\n",
        "        warm_up_proportion=TRAINING[\"warm_up_proportion\"],\n",
        "        local_rank=-1,\n",
        "        output_mode=\"classification\",\n",
        "        discriminate=False,\n",
        "        gradual_unfreeze=False,\n",
        "        fp16=False,\n",
        "        use_amp=False,\n",
        "    )\n",
        "\n",
        "    finbert_eval = FinBert(eval_cfg)\n",
        "    finbert_eval.base_model = BASE_MODEL_NAME\n",
        "    finbert_eval.prepare_model(label_list=LABEL_LIST)\n",
        "    test_data = finbert_eval.get_data(\"test\")\n",
        "\n",
        "    def load_model_from_ckpt(*, ckpt_dir: Path, torch_dtype=None) -> torch.nn.Module:\n",
        "        if torch_dtype is None:\n",
        "            model = AutoModelForSequenceClassification.from_pretrained(\n",
        "                ckpt_dir, cache_dir=None, num_labels=3\n",
        "            )\n",
        "        else:\n",
        "            model = AutoModelForSequenceClassification.from_pretrained(\n",
        "                ckpt_dir, cache_dir=None, num_labels=3, torch_dtype=torch_dtype\n",
        "            )\n",
        "        return model.to(device)\n",
        "\n",
        "    variants = list(INFERENCE_VARIANTS.items())\n",
        "\n",
        "    inference_rows: list[dict[str, Any]] = []\n",
        "\n",
        "    # Optional: log all inference variants to a single W&B run\n",
        "    wb = maybe_wandb_init(\n",
        "        run_name=\"inference-benchmark\",\n",
        "        config={\"baseline_ckpt\": str(BASELINE_CKPT_DIR), **TRAINING},\n",
        "    )\n",
        "\n",
        "    for variant_name, spec in variants:\n",
        "        if spec[\"requires_cuda\"] and device.type != \"cuda\":\n",
        "            print(f\"Skipping {variant_name} (requires CUDA; current device={device})\")\n",
        "            continue\n",
        "\n",
        "        use_amp = bool(spec[\"use_amp\"])\n",
        "        model = load_model_from_ckpt(ckpt_dir=BASELINE_CKPT_DIR, torch_dtype=spec[\"torch_dtype\"])\n",
        "\n",
        "        # Dataset eval: throughput + accuracy\n",
        "        eval_df, eval_timing = timed_eval(\n",
        "            finbert=finbert_eval, model=model, examples=test_data, use_amp=use_amp\n",
        "        )\n",
        "        metrics = calculate_metrics(eval_df)\n",
        "\n",
        "        # Text inference profiling (torch.profiler + forward timing)\n",
        "        _pred_df, prof_metrics = profile_inference(\n",
        "            INFER_TEST_TEXT,\n",
        "            model,\n",
        "            variant_name=variant_name,\n",
        "            use_gpu=use_gpu,\n",
        "            gpu_name=GPU_NAME,\n",
        "            batch_size=INFER_TEXT_BATCH_SIZE,\n",
        "            use_amp=use_amp,\n",
        "        )\n",
        "\n",
        "        # Prefix the text-inference metrics to avoid collisions\n",
        "        text_metrics = {f\"text_{k}\": v for k, v in prof_metrics.items() if k != \"variant\"}\n",
        "\n",
        "        row = {\n",
        "            \"variant\": variant_name,\n",
        "            \"device\": str(device),\n",
        "            \"use_amp\": use_amp,\n",
        "            \"model_size_mb\": float(get_model_size_mb(model)),\n",
        "            **eval_timing,\n",
        "            **metrics,\n",
        "            **text_metrics,\n",
        "        }\n",
        "        inference_rows.append(row)\n",
        "\n",
        "        if wb is not None:\n",
        "            import wandb\n",
        "\n",
        "            wandb.log({f\"{variant_name}/{k}\": v for k, v in row.items() if k != \"variant\"})\n",
        "\n",
        "    if wb is not None:\n",
        "        wb.finish()\n",
        "\n",
        "    inference_summary_df = pd.DataFrame(inference_rows)\n",
        "    inference_summary_df\n",
        "else:\n",
        "    inference_summary_df = pd.DataFrame()\n",
        "    print(\"Skipping inference benchmarking\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =========================\n",
        "# Results: tables + plots\n",
        "# =========================\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "pd.set_option(\"display.max_columns\", 200)\n",
        "\n",
        "print(\"\\n=== Training summary ===\")\n",
        "if \"train_summary_df\" in globals() and not train_summary_df.empty:\n",
        "    cols = [\n",
        "        \"run\",\n",
        "        \"use_amp\",\n",
        "        \"device\",\n",
        "        \"train_wall_s\",\n",
        "        \"train_examples_per_s\",\n",
        "        \"accuracy\",\n",
        "        \"f1_macro\",\n",
        "        \"train_forward_pass_ms\",\n",
        "        \"train_backward_pass_ms\",\n",
        "        \"train_optimizer_step_ms\",\n",
        "    ]\n",
        "    cols = [c for c in cols if c in train_summary_df.columns]\n",
        "    display(train_summary_df[cols].sort_values(\"run\"))\n",
        "\n",
        "    if {\"baseline\", \"amp\"}.issubset(set(train_summary_df[\"run\"])):\n",
        "        base = train_summary_df[train_summary_df[\"run\"] == \"baseline\"].iloc[0]\n",
        "        amp = train_summary_df[train_summary_df[\"run\"] == \"amp\"].iloc[0]\n",
        "        print(\n",
        "            f\"Training speedup (baseline/amp): {base['train_wall_s'] / amp['train_wall_s']:.2f}x | \"\n",
        "            f\"Δaccuracy (amp-baseline): {amp['accuracy'] - base['accuracy']:+.4f} | \"\n",
        "            f\"Δf1_macro (amp-baseline): {amp['f1_macro'] - base['f1_macro']:+.4f}\"\n",
        "        )\n",
        "\n",
        "    ax = train_summary_df.set_index(\"run\")[\"train_wall_s\"].plot(\n",
        "        kind=\"bar\", title=\"Training wall time (s)\", rot=0\n",
        "    )\n",
        "    ax.set_ylabel(\"seconds\")\n",
        "    plt.show()\n",
        "\n",
        "    ax = train_summary_df.set_index(\"run\")[\"train_examples_per_s\"].plot(\n",
        "        kind=\"bar\", title=\"Training throughput (examples/sec)\", rot=0\n",
        "    )\n",
        "    ax.set_ylabel(\"examples/sec\")\n",
        "    plt.show()\n",
        "\n",
        "    ax = train_summary_df.set_index(\"run\")[[\"accuracy\", \"f1_macro\"]].plot(\n",
        "        kind=\"bar\", title=\"Test metrics\", rot=0\n",
        "    )\n",
        "    ax.set_ylabel(\"score\")\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"(no training runs executed)\")\n",
        "\n",
        "\n",
        "print(\"\\n=== Inference summary ===\")\n",
        "if \"inference_summary_df\" in globals() and not inference_summary_df.empty:\n",
        "    df = inference_summary_df.copy()\n",
        "\n",
        "    if \"baseline\" in set(df[\"variant\"]):\n",
        "        base = df[df[\"variant\"] == \"baseline\"].iloc[0]\n",
        "        df[\"eval_speedup_vs_baseline\"] = df[\"eval_samples_per_s\"] / base[\"eval_samples_per_s\"]\n",
        "        df[\"compression_vs_baseline\"] = base[\"model_size_mb\"] / df[\"model_size_mb\"]\n",
        "        if \"text_time_per_sentence_ms\" in df.columns and base.get(\"text_time_per_sentence_ms\"):\n",
        "            df[\"text_latency_speedup_vs_baseline\"] = (\n",
        "                base[\"text_time_per_sentence_ms\"] / df[\"text_time_per_sentence_ms\"]\n",
        "            )\n",
        "\n",
        "    display(\n",
        "        df[[\n",
        "            \"variant\",\n",
        "            \"device\",\n",
        "            \"model_size_mb\",\n",
        "            \"eval_samples_per_s\",\n",
        "            \"accuracy\",\n",
        "            \"f1_macro\",\n",
        "            \"text_time_per_sentence_ms\",\n",
        "            \"eval_speedup_vs_baseline\",\n",
        "            \"compression_vs_baseline\",\n",
        "            \"text_latency_speedup_vs_baseline\",\n",
        "        ]]\n",
        "        .sort_values(\"variant\")\n",
        "        .reset_index(drop=True)\n",
        "    )\n",
        "\n",
        "    if \"baseline\" in set(df[\"variant\"]):\n",
        "        base = df[df[\"variant\"] == \"baseline\"].iloc[0]\n",
        "        for _, row in df.iterrows():\n",
        "            if row[\"variant\"] == \"baseline\":\n",
        "                continue\n",
        "            print(\n",
        "                f\"{row['variant']}: Δaccuracy={row['accuracy'] - base['accuracy']:+.4f}, \"\n",
        "                f\"Δf1_macro={row['f1_macro'] - base['f1_macro']:+.4f}, \"\n",
        "                f\"eval_speedup={row.get('eval_speedup_vs_baseline', float('nan')):.2f}x, \"\n",
        "                f\"compression={row.get('compression_vs_baseline', float('nan')):.2f}x\"\n",
        "            )\n",
        "\n",
        "    ax = df.set_index(\"variant\")[\"eval_samples_per_s\"].plot(\n",
        "        kind=\"bar\", title=\"Test-set throughput (samples/sec)\", rot=0\n",
        "    )\n",
        "    ax.set_ylabel(\"samples/sec\")\n",
        "    plt.show()\n",
        "\n",
        "    if \"text_time_per_sentence_ms\" in df.columns:\n",
        "        ax = df.set_index(\"variant\")[\"text_time_per_sentence_ms\"].plot(\n",
        "            kind=\"bar\", title=\"Text inference latency (ms/sentence)\", rot=0\n",
        "        )\n",
        "        ax.set_ylabel(\"ms/sentence\")\n",
        "        plt.show()\n",
        "\n",
        "else:\n",
        "    print(\"(no inference benchmarks executed)\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
