{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# FinBERT Modular Profiling Notebook\n",
        "\n",
        "This notebook provides a modular framework for profiling the baseline FinBERT model.\n",
        "\n",
        "**Device Support:**\n",
        "- **CUDA (NVIDIA GPUs)**: Full profiling with CPU and CUDA time tracking\n",
        "- **MPS (Apple Silicon)**: CPU time profiling only (GPU execution not separately tracked)\n",
        "- **CPU**: Standard CPU profiling\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Quick Start Guide\n",
        "\n",
        "### How to Use This Modular Notebook\n",
        "\n",
        "1. **Run Setup Cells (1-8)**: Execute all cells from the beginning through \"Setup Paths\" to load utilities and configuration.\n",
        "\n",
        "2. **Configure Your Experiment (Cell 7)**: \n",
        "   - Set `SELECTED_VARIANT` to currently available model variants: `'baseline'` or `'fp16'`\n",
        "   - Set `TRAIN_NEW_MODEL = True` if you want to train from scratch, or `False` to use existing model\n",
        "   - Adjust `USE_GPU` and other settings as needed\n",
        "\n",
        "3. **Optional: Train Model (Cell 9)**:\n",
        "   - Only runs if `TRAIN_NEW_MODEL = True`\n",
        "   - Trains with profiling on first epoch, then continues full training\n",
        "\n",
        "4. **Load Model Variant (Cell 10)**:\n",
        "   - Automatically loads the selected model variant (baseline or fp16)\n",
        "\n",
        "5. **Profile Inference (Cell 11)**:\n",
        "   - Runs profiled inference on the test text\n",
        "   - Displays performance metrics and predictions\n",
        "\n",
        "### Available Model Variants\n",
        "\n",
        "- **baseline**: Standard FinBERT model (FP32, ~438MB)\n",
        "- **fp16_int8**: INT8 post-training FP16 model (3-4x smaller, GPU-accelerated)\n",
        "  - Requires: GPU with CUDA support\n",
        "  - Benefits: Reduced memory footprint, faster inference\n",
        "  - Trade-off: Slight accuracy reduction (~1-3%)\n",
        "\n",
        "### Example Workflow\n",
        "\n",
        "```python\n",
        "# 1. Change configuration\n",
        "SELECTED_VARIANT = 'fp16'  # or 'baseline'\n",
        "TRAIN_NEW_MODEL = False\n",
        "\n",
        "# 2. Run cells 8-11 to profile the model\n",
        "```\n",
        "\n",
        "### How to add a new model variant\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup: Imports and Constants\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Imports loaded successfully\n",
            "✓ Project directory: /home/tfs2123/finBERT\n",
            "✓ PyTorch version: 2.9.1+cu128\n",
            "✓ CUDA available: True\n"
          ]
        }
      ],
      "source": [
        "# Core imports\n",
        "from pathlib import Path\n",
        "import shutil\n",
        "import os\n",
        "import logging\n",
        "import sys\n",
        "import time\n",
        "from collections import defaultdict\n",
        "sys.path.append('..')\n",
        "\n",
        "# NLP & ML\n",
        "from textblob import TextBlob\n",
        "from pprint import pprint\n",
        "from sklearn.metrics import classification_report\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "import nltk\n",
        "\n",
        "# FinBERT\n",
        "from finbert.finbert import *\n",
        "import finbert.utils as tools\n",
        "\n",
        "# PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.profiler import profile, record_function, ProfilerActivity\n",
        "import torch.ao.quantization\n",
        "\n",
        "# Data processing\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Notebook utilities\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "# Global configuration\n",
        "project_dir = Path.cwd().parent\n",
        "pd.set_option('max_colwidth', None)\n",
        "\n",
        "# Constants\n",
        "LABEL_LIST = ['positive', 'negative', 'neutral']\n",
        "LABEL_DICT = {0: 'positive', 1: 'negative', 2: 'neutral'}\n",
        "BASE_TOKENIZER = 'bert-base-uncased'\n",
        "\n",
        "print(\"✓ Imports loaded successfully\")\n",
        "print(f\"✓ Project directory: {project_dir}\")\n",
        "print(f\"✓ PyTorch version: {torch.__version__}\")\n",
        "print(f\"✓ CUDA available: {torch.cuda.is_available()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
        "                    datefmt = '%m/%d/%Y %H:%M:%S',\n",
        "                    level = logging.ERROR)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Helper Utilities\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5.5 Enhanced Metrics Collection\n",
        "\n",
        "Functions for comprehensive model comparison with statistical rigor.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Enhanced metrics collection functions loaded\n"
          ]
        }
      ],
      "source": [
        "def profile_inference_enhanced(model, text, variant_name=\"unknown\", use_gpu=False, gpu_name='cuda:0', \n",
        "                               batch_size=5, num_runs=10, warmup_runs=2):\n",
        "    \"\"\"\n",
        "    Enhanced inference profiling with detailed metrics for research comparison.\n",
        "    \n",
        "    Args:\n",
        "        model: Model to profile\n",
        "        text: Text to analyze\n",
        "        variant_name: Name of the model variant\n",
        "        use_gpu: Whether to use GPU\n",
        "        gpu_name: GPU device name\n",
        "        batch_size: Batch size for inference\n",
        "        num_runs: Number of timing runs for statistical analysis\n",
        "        warmup_runs: Number of warmup runs to discard\n",
        "    \n",
        "    Returns:\n",
        "        results_df: DataFrame with predictions\n",
        "        metrics: Dictionary with comprehensive performance metrics\n",
        "    \"\"\"\n",
        "    from nltk.tokenize import sent_tokenize\n",
        "    from finbert.utils import InputExample, convert_examples_to_features, softmax, chunks, get_device\n",
        "    import numpy as np\n",
        "    \n",
        "    setup_nltk_data()\n",
        "    model.eval()\n",
        "    tokenizer = AutoTokenizer.from_pretrained(BASE_TOKENIZER)\n",
        "    \n",
        "    # Device selection\n",
        "    if use_gpu:\n",
        "        device = get_device(no_cuda=False)\n",
        "        if device.type == \"cuda\" and gpu_name.startswith(\"cuda:\"):\n",
        "            device = torch.device(gpu_name)\n",
        "    else:\n",
        "        device = torch.device(\"cpu\")\n",
        "    \n",
        "    print_device_info(device)\n",
        "    \n",
        "    # Check if model is fp16\n",
        "    is_fp16 = hasattr(model, 'is_loaded_in_8bit') and model.is_loaded_in_8bit\n",
        "    is_fp16 = is_fp16 or (hasattr(model, 'is_loaded_in_4bit') and model.is_loaded_in_4bit)\n",
        "    \n",
        "    # Move model if not fp16\n",
        "    if not is_fp16:\n",
        "        model = model.to(device)\n",
        "    else:\n",
        "        print(f\"✓ Model already fp16 and placed on device\")\n",
        "    \n",
        "    # Tokenize sentences\n",
        "    sentences = sent_tokenize(text)\n",
        "    examples = [InputExample(str(i), sentence) for i, sentence in enumerate(sentences)]\n",
        "    features = convert_examples_to_features(examples, LABEL_LIST, 64, tokenizer)\n",
        "    \n",
        "    # Prepare tensors\n",
        "    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long).to(device)\n",
        "    all_attention_mask = torch.tensor([f.attention_mask for f in features], dtype=torch.long).to(device)\n",
        "    all_token_type_ids = torch.tensor([f.token_type_ids for f in features], dtype=torch.long).to(device)\n",
        "    \n",
        "    # Count actual tokens (non-padding)\n",
        "    total_tokens = all_attention_mask.sum().item()\n",
        "    \n",
        "    # Reset peak memory stats\n",
        "    if device.type == \"cuda\":\n",
        "        torch.cuda.reset_peak_memory_stats(device)\n",
        "    \n",
        "    # Warmup runs\n",
        "    print(f\"\\nRunning {warmup_runs} warmup iterations...\")\n",
        "    with torch.no_grad():\n",
        "        for _ in range(warmup_runs):\n",
        "            _ = model(input_ids=all_input_ids, attention_mask=all_attention_mask, \n",
        "                     token_type_ids=all_token_type_ids)[0]\n",
        "    \n",
        "    # Timed runs\n",
        "    print(f\"Running {num_runs} timed iterations...\")\n",
        "    inference_times = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for run in range(num_runs):\n",
        "            if device.type == \"cuda\":\n",
        "                torch.cuda.synchronize(device)\n",
        "            \n",
        "            start_time = time.time()\n",
        "            logits = model(input_ids=all_input_ids, attention_mask=all_attention_mask, \n",
        "                          token_type_ids=all_token_type_ids)[0]\n",
        "            \n",
        "            if device.type == \"cuda\":\n",
        "                torch.cuda.synchronize(device)\n",
        "            \n",
        "            elapsed = time.time() - start_time\n",
        "            inference_times.append(elapsed)\n",
        "    \n",
        "    # Get final logits for predictions\n",
        "    with torch.no_grad():\n",
        "        logits = model(input_ids=all_input_ids, attention_mask=all_attention_mask, \n",
        "                      token_type_ids=all_token_type_ids)[0]\n",
        "        logits_np = softmax(np.array(logits.cpu()))\n",
        "    \n",
        "    # Memory metrics\n",
        "    peak_memory_mb = 0\n",
        "    if device.type == \"cuda\":\n",
        "        peak_memory_mb = torch.cuda.max_memory_allocated(device) / 1024**2\n",
        "    \n",
        "    # Calculate statistics\n",
        "    inference_times_ms = np.array(inference_times) * 1000\n",
        "    \n",
        "    metrics = {\n",
        "        'variant': variant_name,\n",
        "        'device': str(device),\n",
        "        'is_fp16': is_fp16,\n",
        "        'total_sentences': len(sentences),\n",
        "        'total_tokens': total_tokens,\n",
        "        'avg_tokens_per_sentence': total_tokens / len(sentences),\n",
        "        \n",
        "        # Latency metrics (milliseconds)\n",
        "        'latency_mean_ms': float(np.mean(inference_times_ms)),\n",
        "        'latency_std_ms': float(np.std(inference_times_ms)),\n",
        "        'latency_min_ms': float(np.min(inference_times_ms)),\n",
        "        'latency_max_ms': float(np.max(inference_times_ms)),\n",
        "        'latency_p50_ms': float(np.percentile(inference_times_ms, 50)),\n",
        "        'latency_p95_ms': float(np.percentile(inference_times_ms, 95)),\n",
        "        'latency_p99_ms': float(np.percentile(inference_times_ms, 99)),\n",
        "        \n",
        "        # Throughput metrics\n",
        "        'throughput_tokens_per_sec': total_tokens / np.mean(inference_times),\n",
        "        'throughput_samples_per_sec': len(sentences) / np.mean(inference_times),\n",
        "        'time_per_sentence_ms': (np.mean(inference_times) * 1000) / len(sentences),\n",
        "        'time_per_token_ms': (np.mean(inference_times) * 1000) / total_tokens,\n",
        "        \n",
        "        # Memory metrics\n",
        "        'peak_memory_mb': peak_memory_mb,\n",
        "    }\n",
        "    \n",
        "    # Create results dataframe\n",
        "    sentiment_score = pd.Series(logits_np[:, 0] - logits_np[:, 1])\n",
        "    predictions = np.squeeze(np.argmax(logits_np, axis=1))\n",
        "    \n",
        "    results_df = pd.DataFrame({\n",
        "        'sentence': sentences,\n",
        "        'logit': list(logits_np),\n",
        "        'prediction': [LABEL_DICT[p] for p in predictions],\n",
        "        'sentiment_score': sentiment_score\n",
        "    })\n",
        "    \n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"Enhanced Profiling Results - {variant_name}\")\n",
        "    print(f\"{'='*80}\")\n",
        "    print(f\"Latency (mean ± std): {metrics['latency_mean_ms']:.2f} ± {metrics['latency_std_ms']:.2f} ms\")\n",
        "    print(f\"Latency (P50/P95/P99): {metrics['latency_p50_ms']:.2f} / {metrics['latency_p95_ms']:.2f} / {metrics['latency_p99_ms']:.2f} ms\")\n",
        "    print(f\"Throughput: {metrics['throughput_tokens_per_sec']:.2f} tokens/sec, {metrics['throughput_samples_per_sec']:.2f} samples/sec\")\n",
        "    print(f\"Peak GPU Memory: {metrics['peak_memory_mb']:.2f} MB\")\n",
        "    print(f\"{'='*80}\\n\")\n",
        "    \n",
        "    return results_df, metrics\n",
        "\n",
        "\n",
        "def evaluate_model_accuracy(model, finbert, test_data, variant_name=\"unknown\"):\n",
        "    \"\"\"\n",
        "    Evaluate model accuracy on test set.\n",
        "    \n",
        "    Args:\n",
        "        model: Model to evaluate\n",
        "        finbert: FinBert instance for evaluation utilities\n",
        "        test_data: Test dataset\n",
        "        variant_name: Name of the model variant\n",
        "    \n",
        "    Returns:\n",
        "        Dictionary with accuracy metrics\n",
        "    \"\"\"\n",
        "    from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
        "    \n",
        "    print(f\"\\nEvaluating {variant_name} on test set...\")\n",
        "    \n",
        "    # Run evaluation\n",
        "    results = finbert.evaluate(examples=test_data, model=model)\n",
        "    results['prediction'] = results.predictions.apply(lambda x: np.argmax(x, axis=0))\n",
        "    \n",
        "    # Calculate metrics\n",
        "    y_true = results['labels']\n",
        "    y_pred = results['prediction']\n",
        "    \n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    precision, recall, f1, support = precision_recall_fscore_support(\n",
        "        y_true, y_pred, average=None, labels=[0, 1, 2]\n",
        "    )\n",
        "    precision_weighted, recall_weighted, f1_weighted, _ = precision_recall_fscore_support(\n",
        "        y_true, y_pred, average='weighted'\n",
        "    )\n",
        "    \n",
        "    # Calculate loss - handle GPU tensors and convert to CPU\n",
        "    from torch.nn import CrossEntropyLoss\n",
        "    try:\n",
        "        # Convert predictions to numpy array first, then to tensor on CPU\n",
        "        predictions_array = np.array([p for p in results['predictions']])\n",
        "        labels_array = np.array(list(results['labels']))\n",
        "        \n",
        "        # Create tensors on CPU\n",
        "        predictions_tensor = torch.tensor(predictions_array, dtype=torch.float32)\n",
        "        labels_tensor = torch.tensor(labels_array, dtype=torch.long)\n",
        "        \n",
        "        # Ensure class weights are on CPU\n",
        "        class_weights_cpu = finbert.class_weights.cpu() if finbert.class_weights.is_cuda else finbert.class_weights\n",
        "        \n",
        "        cs = CrossEntropyLoss(weight=class_weights_cpu)\n",
        "        loss = cs(predictions_tensor, labels_tensor)\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: Could not compute loss ({e}). Using fallback calculation.\")\n",
        "        # Fallback: compute loss manually\n",
        "        predictions_array = np.array([p for p in results['predictions']])\n",
        "        labels_array = np.array(list(results['labels']))\n",
        "        \n",
        "        # Simple cross-entropy: -log(p_correct)\n",
        "        correct_probs = predictions_array[np.arange(len(labels_array)), labels_array]\n",
        "        loss = -np.mean(np.log(correct_probs + 1e-10))\n",
        "        loss = torch.tensor(loss)\n",
        "    \n",
        "    metrics = {\n",
        "        'variant': variant_name,\n",
        "        'accuracy': float(accuracy),\n",
        "        'loss': float(loss.item()),\n",
        "        'precision_weighted': float(precision_weighted),\n",
        "        'recall_weighted': float(recall_weighted),\n",
        "        'f1_weighted': float(f1_weighted),\n",
        "        \n",
        "        # Per-class metrics\n",
        "        'precision_positive': float(precision[0]),\n",
        "        'precision_negative': float(precision[1]),\n",
        "        'precision_neutral': float(precision[2]),\n",
        "        \n",
        "        'recall_positive': float(recall[0]),\n",
        "        'recall_negative': float(recall[1]),\n",
        "        'recall_neutral': float(recall[2]),\n",
        "        \n",
        "        'f1_positive': float(f1[0]),\n",
        "        'f1_negative': float(f1[1]),\n",
        "        'f1_neutral': float(f1[2]),\n",
        "        \n",
        "        'support_positive': int(support[0]),\n",
        "        'support_negative': int(support[1]),\n",
        "        'support_neutral': int(support[2]),\n",
        "    }\n",
        "    \n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"Accuracy Metrics - {variant_name}\")\n",
        "    print(f\"{'='*80}\")\n",
        "    print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n",
        "    print(f\"Loss: {metrics['loss']:.4f}\")\n",
        "    print(f\"Weighted F1: {metrics['f1_weighted']:.4f}\")\n",
        "    print(f\"\\nPer-class F1 scores:\")\n",
        "    print(f\"  Positive: {metrics['f1_positive']:.4f}\")\n",
        "    print(f\"  Negative: {metrics['f1_negative']:.4f}\")\n",
        "    print(f\"  Neutral: {metrics['f1_neutral']:.4f}\")\n",
        "    print(f\"{'='*80}\\n\")\n",
        "    \n",
        "    return metrics\n",
        "\n",
        "\n",
        "print(\"✓ Enhanced metrics collection functions loaded\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5.6 Comprehensive Comparison Experiment\n",
        "\n",
        "Run full comparison between baseline and FP16 models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Comprehensive comparison function loaded\n"
          ]
        }
      ],
      "source": [
        "def run_full_comparison(model_path, test_text, use_gpu=True, gpu_name='cuda:0', num_runs=10):\n",
        "    \"\"\"\n",
        "    Run comprehensive comparison between baseline and FP16 models.\n",
        "    \n",
        "    Args:\n",
        "        model_path: Path to trained model\n",
        "        test_text: Text for inference profiling\n",
        "        use_gpu: Whether to use GPU\n",
        "        gpu_name: GPU device name\n",
        "        num_runs: Number of timing iterations\n",
        "    \n",
        "    Returns:\n",
        "        Dictionary with all comparison results\n",
        "    \"\"\"\n",
        "    \n",
        "    results = {\n",
        "        'baseline': {},\n",
        "        'fp16': {},\n",
        "        'comparison': {}\n",
        "    }\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"COMPREHENSIVE MODEL COMPARISON EXPERIMENT\")\n",
        "    print(\"=\"*80 + \"\\n\")\n",
        "    \n",
        "    # =========================================================================\n",
        "    # BASELINE MODEL\n",
        "    # =========================================================================\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"PHASE 1: BASELINE MODEL (FP32)\")\n",
        "    print(\"=\"*80 + \"\\n\")\n",
        "    \n",
        "    # Load baseline model\n",
        "    print(\"Loading baseline model...\")\n",
        "    baseline_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        model_path, cache_dir=None, num_labels=3\n",
        "    )\n",
        "    baseline_variant = MODEL_VARIANTS['baseline']\n",
        "    baseline_variant.model = baseline_model\n",
        "    baseline_variant.size_mb = get_model_size_mb(baseline_model)\n",
        "    \n",
        "    print(f\"✓ Baseline model loaded\")\n",
        "    print(f\"✓ Model size: {baseline_variant.size_mb:.2f} MB\")\n",
        "    \n",
        "    # Run inference profiling\n",
        "    print(\"\\n--- Baseline Inference Profiling ---\")\n",
        "    baseline_results_df, baseline_perf = profile_inference_enhanced(\n",
        "        baseline_model, test_text, variant_name='baseline',\n",
        "        use_gpu=use_gpu, gpu_name=gpu_name, num_runs=num_runs\n",
        "    )\n",
        "    \n",
        "    # Add model size to metrics\n",
        "    baseline_perf['model_size_mb'] = baseline_variant.size_mb\n",
        "    results['baseline']['performance'] = baseline_perf\n",
        "    results['baseline']['predictions'] = baseline_results_df\n",
        "    \n",
        "    # Setup FinBERT for evaluation\n",
        "    print(\"\\n--- Baseline Accuracy Evaluation ---\")\n",
        "    config = create_finbert_config(model_path.parent, model_path.parent.parent / 'data' / 'sentiment_data', \n",
        "                                   baseline_model, num_train_epochs=4)\n",
        "    finbert = FinBert(config)\n",
        "    finbert.base_model = 'bert-base-uncased'\n",
        "    finbert.prepare_model(label_list=LABEL_LIST)\n",
        "    \n",
        "    # Load test data\n",
        "    test_data = finbert.get_data('test')\n",
        "    \n",
        "    # Run accuracy evaluation\n",
        "    baseline_accuracy = evaluate_model_accuracy(baseline_model, finbert, test_data, variant_name='baseline')\n",
        "    results['baseline']['accuracy'] = baseline_accuracy\n",
        "    \n",
        "    # Clean up\n",
        "    del baseline_model\n",
        "    if use_gpu and torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "    \n",
        "    # =========================================================================\n",
        "    # QUANTIZED MODEL\n",
        "    # =========================================================================\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"PHASE 2: QUANTIZED MODEL (INT8)\")\n",
        "    print(\"=\"*80 + \"\\n\")\n",
        "    \n",
        "    # Load FP16 model\n",
        "    print(\"Loading FP16 model...\")\n",
        "    fp16_model, fp16_variant = load_model_variant('fp16', model_path)\n",
        "    \n",
        "    # Run inference profiling\n",
        "    print(\"\\n--- Quantized Inference Profiling ---\")\n",
        "    fp16_results_df, fp16_perf = profile_inference_enhanced(\n",
        "        fp16_model, test_text, variant_name='fp16',\n",
        "        use_gpu=use_gpu, gpu_name=gpu_name, num_runs=num_runs\n",
        "    )\n",
        "    \n",
        "    # Add model size to metrics\n",
        "    fp16_perf['model_size_mb'] = fp16_variant.size_mb\n",
        "    results['fp16']['performance'] = fp16_perf\n",
        "    results['fp16']['predictions'] = fp16_results_df\n",
        "    \n",
        "    # Run accuracy evaluation\n",
        "    print(\"\\n--- Quantized Accuracy Evaluation ---\")\n",
        "    fp16_accuracy = evaluate_model_accuracy(fp16_model, finbert, test_data, variant_name='fp16')\n",
        "    results['fp16']['accuracy'] = fp16_accuracy\n",
        "    \n",
        "    # =========================================================================\n",
        "    # COMPARISON ANALYSIS\n",
        "    # =========================================================================\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"PHASE 3: COMPARISON ANALYSIS\")\n",
        "    print(\"=\"*80 + \"\\n\")\n",
        "    \n",
        "    # Calculate speedup and compression ratios\n",
        "    speedup_latency = baseline_perf['latency_mean_ms'] / fp16_perf['latency_mean_ms']\n",
        "    speedup_throughput = fp16_perf['throughput_tokens_per_sec'] / baseline_perf['throughput_tokens_per_sec']\n",
        "    compression_ratio = baseline_perf['model_size_mb'] / fp16_perf['model_size_mb']\n",
        "    accuracy_delta = fp16_accuracy['accuracy'] - baseline_accuracy['accuracy']\n",
        "    f1_delta = fp16_accuracy['f1_weighted'] - baseline_accuracy['f1_weighted']\n",
        "    \n",
        "    results['comparison'] = {\n",
        "        'speedup_latency': speedup_latency,\n",
        "        'speedup_throughput': speedup_throughput,\n",
        "        'compression_ratio': compression_ratio,\n",
        "        'accuracy_delta': accuracy_delta,\n",
        "        'accuracy_delta_pct': accuracy_delta * 100,\n",
        "        'f1_delta': f1_delta,\n",
        "        'f1_delta_pct': f1_delta * 100,\n",
        "        'memory_reduction_mb': baseline_perf['peak_memory_mb'] - fp16_perf['peak_memory_mb'],\n",
        "        'memory_reduction_pct': ((baseline_perf['peak_memory_mb'] - fp16_perf['peak_memory_mb']) / \n",
        "                                 baseline_perf['peak_memory_mb'] * 100) if baseline_perf['peak_memory_mb'] > 0 else 0,\n",
        "    }\n",
        "    \n",
        "    print(f\"Speedup (latency): {speedup_latency:.2f}x\")\n",
        "    print(f\"Speedup (throughput): {speedup_throughput:.2f}x\")\n",
        "    print(f\"Compression ratio: {compression_ratio:.2f}x\")\n",
        "    print(f\"Accuracy delta: {accuracy_delta:+.4f} ({accuracy_delta*100:+.2f}%)\")\n",
        "    print(f\"F1 delta: {f1_delta:+.4f} ({f1_delta*100:+.2f}%)\")\n",
        "    print(f\"Peak memory reduction: {results['comparison']['memory_reduction_mb']:.2f} MB ({results['comparison']['memory_reduction_pct']:.2f}%)\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"COMPARISON EXPERIMENT COMPLETE\")\n",
        "    print(\"=\"*80 + \"\\n\")\n",
        "    \n",
        "    return results\n",
        "\n",
        "\n",
        "print(\"✓ Comprehensive comparison function loaded\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# COMPREHENSIVE COMPARISON EXPERIMENT\n",
        "\n",
        "Execute full baseline vs fp16 comparison for research analysis.\n",
        "\n",
        "---\n",
        "\n",
        "## Experiment Execution\n",
        "\n",
        "Run comprehensive comparison with the trained model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ready to run comprehensive comparison after workflow setup...\n"
          ]
        }
      ],
      "source": [
        "# Run the comprehensive comparison experiment\n",
        "# This cell will execute after paths are set up in the workflow section below\n",
        "\n",
        "print(\"Ready to run comprehensive comparison after workflow setup...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Helper utilities loaded\n"
          ]
        }
      ],
      "source": [
        "def get_model_size_mb(model):\n",
        "    \"\"\"Calculate model size in MB\"\"\"\n",
        "    param_size = 0\n",
        "    for param in model.parameters():\n",
        "        param_size += param.nelement() * param.element_size()\n",
        "    buffer_size = 0\n",
        "    for buffer in model.buffers():\n",
        "        buffer_size += buffer.nelement() * buffer.element_size()\n",
        "    \n",
        "    size_mb = (param_size + buffer_size) / 1024**2\n",
        "    return size_mb\n",
        "\n",
        "\n",
        "def get_profiler_activities(device):\n",
        "    \"\"\"Get appropriate profiler activities based on device\"\"\"\n",
        "    activities = [ProfilerActivity.CPU]\n",
        "    if device.type == \"cuda\":\n",
        "        activities.append(ProfilerActivity.CUDA)\n",
        "    return activities\n",
        "\n",
        "\n",
        "def print_profiler_results(prof, device, title=\"Profiling Results\"):\n",
        "    \"\"\"Pretty print profiler results\"\"\"\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"{title}\")\n",
        "    print(f\"{'='*80}\\n\")\n",
        "    \n",
        "    print(\"By CPU Time:\")\n",
        "    print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=20))\n",
        "    \n",
        "    if device.type == \"cuda\":\n",
        "        print(\"\\nBy CUDA Time:\")\n",
        "        print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=20))\n",
        "    \n",
        "    print(f\"\\n{'='*80}\\n\")\n",
        "\n",
        "\n",
        "def setup_nltk_data():\n",
        "    \"\"\"Download necessary NLTK data\"\"\"\n",
        "    try:\n",
        "        nltk.download('punkt', quiet=True)\n",
        "        nltk.download('punkt_tab', quiet=True)\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "\n",
        "def print_device_info(device):\n",
        "    \"\"\"Print device information\"\"\"\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"Device: {device}\")\n",
        "    if device.type == \"cuda\":\n",
        "        print(f\"GPU Name: {torch.cuda.get_device_name(device)}\")\n",
        "        print(f\"GPU Memory: {torch.cuda.get_device_properties(device).total_memory / 1024**3:.1f} GB\")\n",
        "    elif device.type == \"mps\":\n",
        "        print(\"Note: MPS profiling shows CPU time only. Actual GPU execution time not separately tracked.\")\n",
        "    print(f\"{'='*80}\\n\")\n",
        "\n",
        "\n",
        "def quantize_int8_model(model, calibration_loader=None, device='cuda'):\n",
        "    \"\"\"\n",
        "    Apply INT8 post-training quantization using TorchAO for GPU inference.\n",
        "    \n",
        "    Args:\n",
        "        model: The model to quantize\n",
        "        calibration_loader: Optional data loader for calibration (not used for dynamic quantization)\n",
        "        device: Device to run quantization on\n",
        "    \n",
        "    Returns:\n",
        "        FP16 model ready for GPU inference\n",
        "    \"\"\"\n",
        "    try:\n",
        "        from torchao.quantization import quantize_, int8_dynamic_activation_int4_weight\n",
        "        \n",
        "        # Move model to GPU for quantization\n",
        "        model = model.to(device)\n",
        "        model.eval()\n",
        "        \n",
        "        # Apply dynamic INT8 activation with INT4 weight quantization\n",
        "        # This provides good compression with minimal accuracy loss\n",
        "        quantize_(model, int8_dynamic_activation_int4_weight())\n",
        "        \n",
        "        print(\"✓ Applied TorchAO INT8 dynamic quantization\")\n",
        "        return model\n",
        "        \n",
        "    except ImportError:\n",
        "        print(\"⚠ TorchAO not available, falling back to torch.ao.quantization\")\n",
        "        # Fallback to standard PyTorch dynamic quantization (CPU-optimized)\n",
        "        fp16_model = torch.ao.quantization.quantize_dynamic(\n",
        "            model.cpu(),\n",
        "            {torch.nn.Linear},\n",
        "            dtype=torch.qint8\n",
        "        )\n",
        "        print(\"✓ Applied torch.ao dynamic quantization (CPU-optimized)\")\n",
        "        return fp16_model.to(device)\n",
        "\n",
        "\n",
        "print(\"✓ Helper utilities loaded\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Data and Training Utilities\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Data and training utilities loaded\n"
          ]
        }
      ],
      "source": [
        "def setup_paths(project_dir):\n",
        "    \"\"\"Setup and return paths for model and data\"\"\"\n",
        "    cl_path = project_dir / 'models' / 'sentiment'\n",
        "    cl_data_path = project_dir / 'data' / 'sentiment_data'\n",
        "    return cl_path, cl_data_path\n",
        "\n",
        "\n",
        "def create_finbert_config(cl_path, cl_data_path, bert_model, **kwargs):\n",
        "    \"\"\"Create FinBERT configuration with defaults\"\"\"\n",
        "    defaults = {\n",
        "        'data_dir': cl_data_path,\n",
        "        'bert_model': bert_model,\n",
        "        'num_train_epochs': 6,\n",
        "        'model_dir': cl_path,\n",
        "        'max_seq_length': 48,\n",
        "        'train_batch_size': 32,\n",
        "        'learning_rate': 2e-5,\n",
        "        'output_mode': 'classification',\n",
        "        'warm_up_proportion': 0.2,\n",
        "        'local_rank': -1,\n",
        "        'discriminate': True,\n",
        "        'gradual_unfreeze': True\n",
        "    }\n",
        "    defaults.update(kwargs)\n",
        "    return Config(**defaults)\n",
        "\n",
        "\n",
        "def initialize_finbert(config, base_model='bert-base-uncased', use_profiling=True):\n",
        "    \"\"\"Initialize FinBERT instance with configuration\"\"\"\n",
        "    if use_profiling:\n",
        "        finbert = ProfiledFinBert(config)\n",
        "    else:\n",
        "        finbert = FinBert(config)\n",
        "    \n",
        "    finbert.base_model = base_model\n",
        "    finbert.config.discriminate = True\n",
        "    finbert.config.gradual_unfreeze = True\n",
        "    finbert.prepare_model(label_list=LABEL_LIST)\n",
        "    \n",
        "    return finbert\n",
        "\n",
        "\n",
        "def train_model(finbert, train_data, model):\n",
        "    \"\"\"Train the model and return the trained model\"\"\"\n",
        "    trained_model = finbert.train(train_examples=train_data, model=model)\n",
        "    return trained_model\n",
        "\n",
        "\n",
        "def evaluate_model(finbert, test_data, model):\n",
        "    \"\"\"Evaluate model on test data\"\"\"\n",
        "    results = finbert.evaluate(examples=test_data, model=model)\n",
        "    results['prediction'] = results.predictions.apply(lambda x: np.argmax(x, axis=0))\n",
        "    return results\n",
        "\n",
        "\n",
        "def generate_classification_report(results, finbert, cols=['labels', 'prediction', 'predictions']):\n",
        "    \"\"\"Generate and print classification report\"\"\"\n",
        "    from torch.nn import CrossEntropyLoss\n",
        "    \n",
        "    cs = CrossEntropyLoss(weight=finbert.class_weights)\n",
        "    loss = cs(torch.tensor(list(results[cols[2]])), torch.tensor(list(results[cols[0]])))\n",
        "    \n",
        "    accuracy = (results[cols[0]] == results[cols[1]]).sum() / results.shape[0]\n",
        "    \n",
        "    print(f\"Loss: {loss:.2f}\")\n",
        "    print(f\"Accuracy: {accuracy:.2f}\")\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(results[cols[0]], results[cols[1]]))\n",
        "    \n",
        "    return {'loss': loss.item(), 'accuracy': accuracy}\n",
        "\n",
        "\n",
        "print(\"✓ Data and training utilities loaded\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Model Variant Registry\n",
        "\n",
        "This section defines model variants for profiling, including baseline and optimized models.\n",
        "\n",
        "### Half-Precision (FP16)\n",
        "\n",
        "The notebook supports FP16 half-precision loading for GPU acceleration:\n",
        "\n",
        "- **Method**: Native PyTorch `torch.float16` loading\n",
        "- **Benefits**: ~2x memory reduction, ~2-3x faster inference on Tensor Cores (T4/V100/A100)\n",
        "- **Trade-offs**: Minimal accuracy impact, requires GPU support\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Model variant registry loaded\n",
            "Available Model Variants:\n",
            "================================================================================\n",
            "\n",
            "baseline:\n",
            "  Description: Standard FinBERT model (FP32)\n",
            "\n",
            "fp16:\n",
            "  Description: FinBERT in Half-Precision (FP16) for GPU acceleration\n",
            "\n",
            "amp:\n",
            "  Description: FinBERT with Automatic Mixed Precision (AMP) - Optimal for Training & Inference\n",
            "\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "class ModelVariant:\n",
        "    \"\"\"Base class for model variants\"\"\"\n",
        "    def __init__(self, name, description):\n",
        "        self.name = name\n",
        "        self.description = description\n",
        "        self.model = None\n",
        "        self.size_mb = None\n",
        "        self.use_amp = False # Default to False\n",
        "    \n",
        "    def load_model(self, model_path):\n",
        "        \"\"\"Load model - to be implemented by subclasses\"\"\"\n",
        "        raise NotImplementedError\n",
        "    \n",
        "    def get_info(self):\n",
        "        \"\"\"Get variant information\"\"\"\n",
        "        return {\n",
        "            'name': self.name,\n",
        "            'description': self.description,\n",
        "            'size_mb': self.size_mb\n",
        "        }\n",
        "\n",
        "\n",
        "class BaselineVariant(ModelVariant):\n",
        "    \"\"\"Standard FinBERT model without any optimization\"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__(\n",
        "            name=\"baseline\",\n",
        "            description=\"Standard FinBERT model (FP32)\"\n",
        "        )\n",
        "    \n",
        "    def load_model(self, model_path):\n",
        "        \"\"\"Load baseline model\"\"\"\n",
        "        self.model = AutoModelForSequenceClassification.from_pretrained(\n",
        "            model_path, cache_dir=None, num_labels=3\n",
        "        )\n",
        "        self.size_mb = get_model_size_mb(self.model)\n",
        "        return self.model\n",
        "\n",
        "\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "class FP16Variant(ModelVariant):\n",
        "    \"\"\"Half-Precision (FP16) FinBERT model\"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__(\n",
        "            name=\"fp16\",\n",
        "            description=\"FinBERT in Half-Precision (FP16) for GPU acceleration\"\n",
        "        )\n",
        "    \n",
        "    def load_model(self, model_path):\n",
        "        \"\"\"Load model in FP16\"\"\"\n",
        "        self.model = AutoModelForSequenceClassification.from_pretrained(\n",
        "            model_path,\n",
        "            cache_dir=None,\n",
        "            num_labels=3,\n",
        "            torch_dtype=torch.float16\n",
        "        )\n",
        "        self.size_mb = get_model_size_mb(self.model)\n",
        "        return self.model\n",
        "\n",
        "\n",
        "class AMPVariant(ModelVariant):\n",
        "    \"\"\"FinBERT with Automatic Mixed Precision (AMP)\"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__(\n",
        "            name=\"amp\",\n",
        "            description=\"FinBERT with Automatic Mixed Precision (AMP) - Optimal for Training & Inference\"\n",
        "        )\n",
        "        self.use_amp = True\n",
        "    \n",
        "    def load_model(self, model_path):\n",
        "        \"\"\"Load baseline model (FP32) but configured for AMP\"\"\"\n",
        "        self.model = AutoModelForSequenceClassification.from_pretrained(\n",
        "            model_path,\n",
        "            cache_dir=None,\n",
        "            num_labels=3\n",
        "        )\n",
        "        self.size_mb = get_model_size_mb(self.model)\n",
        "        return self.model\n",
        "\n",
        "\n",
        "\n",
        "# Registry of available variants\n",
        "MODEL_VARIANTS = {\n",
        "    'baseline': BaselineVariant(),\n",
        "    'fp16': FP16Variant(),\n",
        "    'amp': AMPVariant()\n",
        "}\n",
        "\n",
        "\n",
        "def list_available_variants():\n",
        "    \"\"\"List all available model variants\"\"\"\n",
        "    print(\"Available Model Variants:\")\n",
        "    print(\"=\" * 80)\n",
        "    for name, variant in MODEL_VARIANTS.items():\n",
        "        print(f\"\\n{name}:\")\n",
        "        print(f\"  Description: {variant.description}\")\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "\n",
        "\n",
        "def load_model_variant(variant_name, model_path):\n",
        "    \"\"\"Load a specific model variant\"\"\"\n",
        "    if variant_name not in MODEL_VARIANTS:\n",
        "        raise ValueError(f\"Unknown variant: {variant_name}. Available: {list(MODEL_VARIANTS.keys())}\")\n",
        "    \n",
        "    variant = MODEL_VARIANTS[variant_name]\n",
        "    print(f\"\\nLoading model variant: {variant.name}\")\n",
        "    print(f\"Description: {variant.description}\")\n",
        "    \n",
        "    model = variant.load_model(model_path)\n",
        "    \n",
        "    print(f\"✓ Model loaded successfully\")\n",
        "    print(f\"✓ Model size: {variant.size_mb:.2f} MB\")\n",
        "    \n",
        "    return model, variant\n",
        "\n",
        "\n",
        "print(\"✓ Model variant registry loaded\")\n",
        "list_available_variants()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Profiling API\n",
        "\n",
        "Generic profiling functions that work with any model variant.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def profile_inference(model, text, variant_name=\"unknown\", use_gpu=False, gpu_name='cuda:0', batch_size=5, use_amp=False):\n",
        "    \"\"\"\n",
        "    Profile model inference performance.\n",
        "    \n",
        "    Args:\n",
        "        model: Model to profile\n",
        "        text: Text to analyze\n",
        "        variant_name: Name of the model variant\n",
        "        use_gpu: Whether to use GPU\n",
        "        gpu_name: GPU device name\n",
        "        batch_size: Batch size for inference\n",
        "        use_amp: Whether to use Automatic Mixed Precision (AMP)\n",
        "    \n",
        "    Returns:\n",
        "        results_df: DataFrame with predictions\n",
        "        metrics: Dictionary with performance metrics\n",
        "    \"\"\"\n",
        "    from nltk.tokenize import sent_tokenize\n",
        "    from finbert.utils import InputExample, convert_examples_to_features, softmax, chunks, get_device\n",
        "    import torch.cuda.amp\n",
        "    \n",
        "    # Setup NLTK\n",
        "    setup_nltk_data()\n",
        "    \n",
        "    model.eval()\n",
        "    tokenizer = AutoTokenizer.from_pretrained(BASE_TOKENIZER)\n",
        "    \n",
        "    # Device selection\n",
        "    if use_gpu:\n",
        "        device = get_device(no_cuda=False)\n",
        "        if device.type == \"cuda\" and gpu_name.startswith(\"cuda:\"):\n",
        "            device = torch.device(gpu_name)\n",
        "    else:\n",
        "        device = torch.device(\"cpu\")\n",
        "    \n",
        "    print_device_info(device)\n",
        "    \n",
        "    # Check if model is already on device (e.g., BitsAndBytes FP16 models)\n",
        "    is_fp16 = hasattr(model, 'is_loaded_in_8bit') and model.is_loaded_in_8bit\n",
        "    is_fp16 = is_fp16 or (hasattr(model, 'is_loaded_in_4bit') and model.is_loaded_in_4bit)\n",
        "    \n",
        "    # Only move model if it's not already fp16 and placed\n",
        "    if not is_fp16:\n",
        "        model = model.to(device)\n",
        "    else:\n",
        "        print(f\"✓ Model already fp16 and placed on device (skipping .to() call)\")\n",
        "        # For FP16 models, get the actual device from model\n",
        "        if hasattr(model, 'device'):\n",
        "            device = model.device\n",
        "        elif hasattr(model, 'hf_device_map'):\n",
        "            # BitsAndBytes models have device_map\n",
        "            device = torch.device('cuda:0')  # Usually on cuda:0\n",
        "    \n",
        "    result = pd.DataFrame(columns=['sentence', 'logit', 'prediction', 'sentiment_score'])\n",
        "    \n",
        "    # Setup profiler\n",
        "    activities = get_profiler_activities(device)\n",
        "    \n",
        "    total_inference_time = 0\n",
        "    \n",
        "    with profile(\n",
        "        activities=activities,\n",
        "        record_shapes=True,\n",
        "        profile_memory=True,\n",
        "        with_stack=False\n",
        "    ) as prof:\n",
        "        \n",
        "        with record_function(\"sentence_tokenization\"):\n",
        "            sentences = sent_tokenize(text)\n",
        "        \n",
        "        for batch in chunks(sentences, batch_size):\n",
        "            with record_function(\"create_examples\"):\n",
        "                examples = [InputExample(str(i), sentence) for i, sentence in enumerate(batch)]\n",
        "            \n",
        "            with record_function(\"convert_to_features\"):\n",
        "                features = convert_examples_to_features(examples, LABEL_LIST, 64, tokenizer)\n",
        "            \n",
        "            with record_function(\"prepare_tensors\"):\n",
        "                all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long).to(device)\n",
        "                all_attention_mask = torch.tensor([f.attention_mask for f in features], dtype=torch.long).to(device)\n",
        "                all_token_type_ids = torch.tensor([f.token_type_ids for f in features], dtype=torch.long).to(device)\n",
        "            \n",
        "            with torch.no_grad():\n",
        "                # Remove the model_to_device profiling section for FP16 models\n",
        "                if not is_fp16:\n",
        "                    with record_function(\"model_to_device\"):\n",
        "                        model = model.to(device)\n",
        "                \n",
        "                with record_function(\"inference_forward\"):\n",
        "                    start_time = time.time()\n",
        "                    with torch.cuda.amp.autocast(enabled=use_amp):\n",
        "                         logits = model(input_ids=all_input_ids, attention_mask=all_attention_mask, token_type_ids=all_token_type_ids)[0]\n",
        "                    total_inference_time += time.time() - start_time\n",
        "                \n",
        "                with record_function(\"postprocess_results\"):\n",
        "                    logits = softmax(np.array(logits.cpu()))\n",
        "                    sentiment_score = pd.Series(logits[:, 0] - logits[:, 1])\n",
        "                    predictions = np.squeeze(np.argmax(logits, axis=1))\n",
        "                    \n",
        "                    batch_result = {\n",
        "                        'sentence': batch,\n",
        "                        'logit': list(logits),\n",
        "                        'prediction': predictions,\n",
        "                        'sentiment_score': sentiment_score\n",
        "                    }\n",
        "                    \n",
        "                    batch_result = pd.DataFrame(batch_result)\n",
        "                    result = pd.concat([result, batch_result], ignore_index=True)\n",
        "    \n",
        "    # Print profiler results\n",
        "    print_profiler_results(prof, device, title=f\"Inference Profiling - {variant_name}\")\n",
        "    \n",
        "    result['prediction'] = result.prediction.apply(lambda x: LABEL_DICT[x])\n",
        "    \n",
        "    metrics = {\n",
        "        'variant': variant_name,\n",
        "        'total_sentences': len(sentences),\n",
        "        'inference_time_ms': total_inference_time * 1000,\n",
        "        'time_per_sentence_ms': (total_inference_time * 1000) / len(sentences),\n",
        "        'device': str(device),\n",
        "        'is_fp16': is_fp16,\n",
        "        'use_amp': use_amp\n",
        "    }\n",
        "    \n",
        "    print(f\"\\nInference Summary:\")\n",
        "    print(f\"  Total sentences: {metrics['total_sentences']}\")\n",
        "    print(f\"  Total inference time: {metrics['inference_time_ms']:.2f} ms\")\n",
        "    print(f\"  Time per sentence: {metrics['time_per_sentence_ms']:.2f} ms\")\n",
        "    if is_fp16:\n",
        "        print(f\"  ✓ FP16 model profiled successfully\")\n",
        "    if use_amp:\n",
        "        print(f\"  ✓ AMP enabled\")\n",
        "    \n",
        "    return result, metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. ProfiledFinBert Class (for Training)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Extended FinBert class with profiling instrumentation for training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Profiling loaded\n"
          ]
        }
      ],
      "source": [
        "class ProfiledFinBert(FinBert):\n",
        "    \"\"\"Extended FinBert class with profiling instrumentation.\n",
        "    \n",
        "    Note: GPU-specific profiling (ProfilerActivity.CUDA) only works with NVIDIA CUDA devices.\n",
        "    For MPS (Apple Silicon), only CPU profiling is available, though actual computation runs on GPU.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.profile_results = {}\n",
        "    \n",
        "    def train(self, train_examples, model):\n",
        "        \"\"\"\n",
        "        Trains the model with profiling instrumentation.\n",
        "        \"\"\"\n",
        "        validation_examples = self.get_data('validation')\n",
        "        global_step = 0\n",
        "        self.validation_losses = []\n",
        "        \n",
        "        # Training\n",
        "        train_dataloader = self.get_loader(train_examples, 'train')\n",
        "        model.train()\n",
        "        step_number = len(train_dataloader)\n",
        "        \n",
        "        # Setup profiler - CUDA profiling only works with NVIDIA GPUs, not MPS\n",
        "        activities = [ProfilerActivity.CPU]\n",
        "        if self.device.type == \"cuda\":\n",
        "            activities.append(ProfilerActivity.CUDA)\n",
        "        \n",
        "        print(\"\\\\n\" + \"=\"*80)\n",
        "        print(\"Starting Profiled Training\")\n",
        "        print(f\"Device: {self.device}\")\n",
        "        print(f\"Profiling activities: {activities}\")\n",
        "        if self.device.type == \"mps\":\n",
        "            print(\"Note: MPS profiling shows CPU time only. Actual GPU execution time not separately tracked.\")\n",
        "        print(\"=\"*80 + \"\\\\n\")\n",
        "        \n",
        "        i = 0\n",
        "        \n",
        "        with profile(\n",
        "            activities=activities,\n",
        "            record_shapes=True,\n",
        "            profile_memory=True,\n",
        "            with_stack=False\n",
        "        ) as prof:\n",
        "            \n",
        "            for epoch in trange(int(self.config.num_train_epochs), desc=\"Epoch\"):\n",
        "                model.train()\n",
        "                tr_loss = 0\n",
        "                nb_tr_examples, nb_tr_steps = 0, 0\n",
        "                \n",
        "                for step, batch in enumerate(tqdm(train_dataloader, desc='Iteration')):\n",
        "                    \n",
        "                    # Gradual unfreezing logic\n",
        "                    if (self.config.gradual_unfreeze and i == 0):\n",
        "                        for param in model.bert.parameters():\n",
        "                            param.requires_grad = False\n",
        "                    \n",
        "                    if (step % (step_number // 3)) == 0:\n",
        "                        i += 1\n",
        "                    \n",
        "                    if (self.config.gradual_unfreeze and i > 1 and i < self.config.encoder_no):\n",
        "                        for k in range(i - 1):\n",
        "                            try:\n",
        "                                for param in model.bert.encoder.layer[self.config.encoder_no - 1 - k].parameters():\n",
        "                                    param.requires_grad = True\n",
        "                            except:\n",
        "                                pass\n",
        "                    \n",
        "                    if (self.config.gradual_unfreeze and i > self.config.encoder_no + 1):\n",
        "                        for param in model.bert.embeddings.parameters():\n",
        "                            param.requires_grad = True\n",
        "                    \n",
        "                    # Data loading profiling\n",
        "                    with record_function(\"data_transfer\"):\n",
        "                        batch = tuple(t.to(self.device) for t in batch)\n",
        "                        input_ids, attention_mask, token_type_ids, label_ids, agree_ids = batch\n",
        "                    \n",
        "                    # Forward pass profiling\n",
        "                    with record_function(\"forward_pass\"):\n",
        "                        logits = model(input_ids, attention_mask, token_type_ids)[0]\n",
        "                    \n",
        "                    # Loss calculation profiling\n",
        "                    with record_function(\"loss_calculation\"):\n",
        "                        weights = self.class_weights.to(self.device)\n",
        "                        if self.config.output_mode == \"classification\":\n",
        "                            loss_fct = CrossEntropyLoss(weight=weights)\n",
        "                            loss = loss_fct(logits.view(-1, self.num_labels), label_ids.view(-1))\n",
        "                        elif self.config.output_mode == \"regression\":\n",
        "                            loss_fct = MSELoss()\n",
        "                            loss = loss_fct(logits.view(-1), label_ids.view(-1))\n",
        "                        \n",
        "                        if self.config.gradient_accumulation_steps > 1:\n",
        "                            loss = loss / self.config.gradient_accumulation_steps\n",
        "                    \n",
        "                    # Backward pass profiling\n",
        "                    with record_function(\"backward_pass\"):\n",
        "                        loss.backward()\n",
        "                    \n",
        "                    tr_loss += loss.item()\n",
        "                    nb_tr_examples += input_ids.size(0)\n",
        "                    nb_tr_steps += 1\n",
        "                    \n",
        "                    # Optimizer step profiling\n",
        "                    if (step + 1) % self.config.gradient_accumulation_steps == 0:\n",
        "                        with record_function(\"optimizer_step\"):\n",
        "                            if self.config.fp16:\n",
        "                                lr_this_step = self.config.learning_rate * warmup_linear(\n",
        "                                    global_step / self.num_train_optimization_steps, self.config.warm_up_proportion)\n",
        "                                for param_group in self.optimizer.param_groups:\n",
        "                                    param_group['lr'] = lr_this_step\n",
        "                            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "                            self.optimizer.step()\n",
        "                            self.scheduler.step()\n",
        "                            self.optimizer.zero_grad()\n",
        "                            global_step += 1\n",
        "                    \n",
        "                    # Only profile first epoch to save time\n",
        "                    if epoch == 0 and step >= 20:\n",
        "                        break\n",
        "                \n",
        "                # Break after first epoch for profiling\n",
        "                if epoch == 0:\n",
        "                    print(\"\\\\n\" + \"=\"*80)\n",
        "                    print(\"Profiling complete for first epoch (20 steps)\")\n",
        "                    print(\"Continuing full training without profiling...\")\n",
        "                    print(\"=\"*80 + \"\\\\n\")\n",
        "                    break\n",
        "        \n",
        "        # Print profiler results\n",
        "        print(\"\\\\n\" + \"=\"*80)\n",
        "        print(\"PROFILING RESULTS - Training\")\n",
        "        print(\"=\"*80 + \"\\\\n\")\n",
        "        \n",
        "        print(\"\\\\nBy CPU Time:\")\n",
        "        print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=20))\n",
        "        \n",
        "        if self.device.type == \"cuda\":\n",
        "            print(\"\\\\nBy CUDA Time:\")\n",
        "            print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=20))\n",
        "        \n",
        "        print(\"\\\\n\" + \"=\"*80 + \"\\\\n\")\n",
        "        \n",
        "        # Store results\n",
        "        self.profile_results['training'] = prof.key_averages()\n",
        "        \n",
        "        # Continue with full training without profiling\n",
        "        for epoch in trange(int(self.config.num_train_epochs), desc=\"Epoch\"):\n",
        "            model.train()\n",
        "            tr_loss = 0\n",
        "            nb_tr_examples, nb_tr_steps = 0, 0\n",
        "            \n",
        "            for step, batch in enumerate(tqdm(train_dataloader, desc='Iteration')):\n",
        "                \n",
        "                if (self.config.gradual_unfreeze and i == 0):\n",
        "                    for param in model.bert.parameters():\n",
        "                        param.requires_grad = False\n",
        "                \n",
        "                if (step % (step_number // 3)) == 0:\n",
        "                    i += 1\n",
        "                \n",
        "                if (self.config.gradual_unfreeze and i > 1 and i < self.config.encoder_no):\n",
        "                    for k in range(i - 1):\n",
        "                        try:\n",
        "                            for param in model.bert.encoder.layer[self.config.encoder_no - 1 - k].parameters():\n",
        "                                param.requires_grad = True\n",
        "                        except:\n",
        "                            pass\n",
        "                \n",
        "                if (self.config.gradual_unfreeze and i > self.config.encoder_no + 1):\n",
        "                    for param in model.bert.embeddings.parameters():\n",
        "                        param.requires_grad = True\n",
        "                \n",
        "                batch = tuple(t.to(self.device) for t in batch)\n",
        "                input_ids, attention_mask, token_type_ids, label_ids, agree_ids = batch\n",
        "                \n",
        "                logits = model(input_ids, attention_mask, token_type_ids)[0]\n",
        "                weights = self.class_weights.to(self.device)\n",
        "                \n",
        "                if self.config.output_mode == \"classification\":\n",
        "                    loss_fct = CrossEntropyLoss(weight=weights)\n",
        "                    loss = loss_fct(logits.view(-1, self.num_labels), label_ids.view(-1))\n",
        "                elif self.config.output_mode == \"regression\":\n",
        "                    loss_fct = MSELoss()\n",
        "                    loss = loss_fct(logits.view(-1), label_ids.view(-1))\n",
        "                \n",
        "                if self.config.gradient_accumulation_steps > 1:\n",
        "                    loss = loss / self.config.gradient_accumulation_steps\n",
        "                else:\n",
        "                    loss.backward()\n",
        "                \n",
        "                tr_loss += loss.item()\n",
        "                nb_tr_examples += input_ids.size(0)\n",
        "                nb_tr_steps += 1\n",
        "                \n",
        "                if (step + 1) % self.config.gradient_accumulation_steps == 0:\n",
        "                    if self.config.fp16:\n",
        "                        lr_this_step = self.config.learning_rate * warmup_linear(\n",
        "                            global_step / self.num_train_optimization_steps, self.config.warm_up_proportion)\n",
        "                        for param_group in self.optimizer.param_groups:\n",
        "                            param_group['lr'] = lr_this_step\n",
        "                    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "                    self.optimizer.step()\n",
        "                    self.scheduler.step()\n",
        "                    self.optimizer.zero_grad()\n",
        "                    global_step += 1\n",
        "            \n",
        "            # Validation\n",
        "            validation_loader = self.get_loader(validation_examples, phase='eval')\n",
        "            model.eval()\n",
        "            \n",
        "            valid_loss, valid_accuracy = 0, 0\n",
        "            nb_valid_steps, nb_valid_examples = 0, 0\n",
        "            \n",
        "            for input_ids, attention_mask, token_type_ids, label_ids, agree_ids in tqdm(validation_loader, desc=\"Validating\"):\n",
        "                input_ids = input_ids.to(self.device)\n",
        "                attention_mask = attention_mask.to(self.device)\n",
        "                token_type_ids = token_type_ids.to(self.device)\n",
        "                label_ids = label_ids.to(self.device)\n",
        "                agree_ids = agree_ids.to(self.device)\n",
        "                \n",
        "                with torch.no_grad():\n",
        "                    logits = model(input_ids, attention_mask, token_type_ids)[0]\n",
        "                    \n",
        "                    if self.config.output_mode == \"classification\":\n",
        "                        loss_fct = CrossEntropyLoss(weight=weights)\n",
        "                        tmp_valid_loss = loss_fct(logits.view(-1, self.num_labels), label_ids.view(-1))\n",
        "                    elif self.config.output_mode == \"regression\":\n",
        "                        loss_fct = MSELoss()\n",
        "                        tmp_valid_loss = loss_fct(logits.view(-1), label_ids.view(-1))\n",
        "                    \n",
        "                    valid_loss += tmp_valid_loss.mean().item()\n",
        "                    nb_valid_steps += 1\n",
        "            \n",
        "            valid_loss = valid_loss / nb_valid_steps\n",
        "            self.validation_losses.append(valid_loss)\n",
        "            print(\"Validation losses: {}\".format(self.validation_losses))\n",
        "            \n",
        "            if valid_loss == min(self.validation_losses):\n",
        "                try:\n",
        "                    os.remove(self.config.model_dir / ('temporary' + str(best_model)))\n",
        "                except:\n",
        "                    print('No best model found')\n",
        "                torch.save({'epoch': str(epoch), 'state_dict': model.state_dict()},\n",
        "                           self.config.model_dir / ('temporary' + str(epoch)))\n",
        "                best_model = epoch\n",
        "        \n",
        "        # Save the trained model\n",
        "        checkpoint = torch.load(self.config.model_dir / ('temporary' + str(best_model)))\n",
        "        model.load_state_dict(checkpoint['state_dict'])\n",
        "        model_to_save = model.module if hasattr(model, 'module') else model\n",
        "        output_model_file = os.path.join(self.config.model_dir, WEIGHTS_NAME)\n",
        "        torch.save(model_to_save.state_dict(), output_model_file)\n",
        "        output_config_file = os.path.join(self.config.model_dir, CONFIG_NAME)\n",
        "        with open(output_config_file, 'w') as f:\n",
        "            f.write(model_to_save.config.to_json_string())\n",
        "        os.remove(self.config.model_dir / ('temporary' + str(best_model)))\n",
        "        \n",
        "        return model\n",
        "\n",
        "print(\"Profiling loaded\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# WORKFLOW SECTION\n",
        "\n",
        "Below are the workflow cells that demonstrate how to use the modular system.\n",
        "\n",
        "---\n",
        "\n",
        "## 7. Configuration: Select Model Variant and Settings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Configuration loaded\n",
            "  Selected variant: amp\n",
            "  Training mode: Train new model\n",
            "  GPU enabled: True\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# CONFIGURATION CELL - Modify these settings to change workflow behavior\n",
        "# =============================================================================\n",
        "\n",
        "# Choose which model variant to use\n",
        "# Current Options: 'baseline', 'fp16'\n",
        "SELECTED_VARIANT = 'amp'\n",
        "\n",
        "# Whether to train a new model (True) or load existing model (False)\n",
        "TRAIN_NEW_MODEL = True  # Set to False if you already have a trained model\n",
        "\n",
        "# Number of training epochs (if training)\n",
        "NUM_EPOCHS = 4\n",
        "\n",
        "# GPU settings\n",
        "USE_GPU = True\n",
        "GPU_NAME = 'cuda:0'\n",
        "\n",
        "# Test text for inference profiling\n",
        "TEST_TEXT = \"\"\"Later that day Apple said it was revising down its earnings expectations in \\\n",
        "the fourth quarter of 2018, largely because of lower sales and signs of economic weakness in China. \\\n",
        "The news rapidly infected financial markets. Apple's share price fell by around 7% in after-hours \\\n",
        "trading and the decline was extended to more than 10% when the market opened. The dollar fell \\\n",
        "by 3.7% against the yen in a matter of minutes after the announcement, before rapidly recovering \\\n",
        "some ground. Asian stockmarkets closed down on January 3rd and European ones opened lower. \\\n",
        "Yields on government bonds fell as investors fled to the traditional haven in a market storm.\"\"\"\n",
        "\n",
        "print(f\"✓ Configuration loaded\")\n",
        "print(f\"  Selected variant: {SELECTED_VARIANT}\")\n",
        "print(f\"  Training mode: {'Train new model' if TRAIN_NEW_MODEL else 'Load existing model'}\")\n",
        "print(f\"  GPU enabled: {USE_GPU}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Setup Paths\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Paths configured:\n",
            "  Model path: /home/tfs2123/finBERT/models/sentiment\n",
            "  Data path: /home/tfs2123/finBERT/data/sentiment_data\n"
          ]
        }
      ],
      "source": [
        "# Setup paths using the utility function\n",
        "cl_path, cl_data_path = setup_paths(project_dir)\n",
        "\n",
        "print(f\"✓ Paths configured:\")\n",
        "print(f\"  Model path: {cl_path}\")\n",
        "print(f\"  Data path: {cl_data_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Training Workflow (Optional - Only if TRAIN_NEW_MODEL = True)\n",
        "\n",
        "Skip this section if you already have a trained model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "TRAINING NEW MODEL\n",
            "================================================================================\n",
            "✓ Cleaned previous model directory\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "12/11/2025 02:23:32 - WARNING - torchao -   Skipping import of cpp extensions due to incompatible torch version 2.9.1+cu128 for torchao version 0.14.1             Please see https://github.com/pytorch/ao/issues/2919 for more info\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "12/11/2025 02:23:34 - INFO - finbert.finbert -   device: cuda n_gpu: 1, distributed training: False, 16-bits training: False\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "✓ Loading training data...\n",
            "✓ Creating model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "12/11/2025 02:23:35 - INFO - finbert.utils -   *** Example ***\n",
            "12/11/2025 02:23:35 - INFO - finbert.utils -   guid: train-1\n",
            "12/11/2025 02:23:35 - INFO - finbert.utils -   tokens: [CLS] after the reporting period , bio ##tie north american licensing partner so ##max ##on pharmaceuticals announced positive results with na ##lm ##efe ##ne in a pilot phase 2 clinical trial for smoking ce ##ssa ##tion [SEP]\n",
            "12/11/2025 02:23:35 - INFO - finbert.utils -   input_ids: 101 2044 1996 7316 2558 1010 16012 9515 2167 2137 13202 4256 2061 17848 2239 24797 2623 3893 3463 2007 6583 13728 27235 2638 1999 1037 4405 4403 1016 6612 3979 2005 9422 8292 11488 3508 102 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/11/2025 02:23:35 - INFO - finbert.utils -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/11/2025 02:23:35 - INFO - finbert.utils -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/11/2025 02:23:35 - INFO - finbert.utils -   label: positive (id = 0)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "✓ Starting training with profiling...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "12/11/2025 02:23:36 - INFO - finbert.finbert -   ***** Loading data *****\n",
            "12/11/2025 02:23:36 - INFO - finbert.finbert -     Num examples = 3488\n",
            "12/11/2025 02:23:36 - INFO - finbert.finbert -     Batch size = 32\n",
            "12/11/2025 02:23:36 - INFO - finbert.finbert -     Num steps = 48\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\\n================================================================================\n",
            "Starting Profiled Training\n",
            "Device: cuda\n",
            "Profiling activities: [<ProfilerActivity.CPU: 0>, <ProfilerActivity.CUDA: 2>]\n",
            "================================================================================\\n\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Iteration:  18%|█▊        | 20/109 [00:02<00:08,  9.91it/s]\n",
            "Epoch:   0%|          | 0/4 [00:02<?, ?it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\\n================================================================================\n",
            "Profiling complete for first epoch (20 steps)\n",
            "Continuing full training without profiling...\n",
            "================================================================================\\n\n",
            "\\n================================================================================\n",
            "PROFILING RESULTS - Training\n",
            "================================================================================\\n\n",
            "\\nBy CPU Time:\n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                       loss_calculation         0.32%       6.439ms        56.20%        1.132s      53.905ms       0.000us         0.00%     178.042us       8.478us           0 B           0 B      22.00 KB     -20.00 KB            21  \n",
            "                                               aten::to         0.05%     915.631us        55.29%        1.114s       3.295ms       0.000us         0.00%     533.967us       1.580us           0 B           0 B       8.15 MB           0 B           338  \n",
            "                                         aten::_to_copy         0.07%       1.408ms        55.24%        1.113s       6.624ms       0.000us         0.00%     533.967us       3.178us           0 B           0 B       8.15 MB           0 B           168  \n",
            "                                            aten::copy_         0.09%       1.877ms        55.13%        1.111s       5.876ms     567.087us         0.04%     605.039us       3.201us           0 B           0 B           0 B           0 B           189  \n",
            "                                  cudaStreamSynchronize        54.45%        1.097s        54.45%        1.097s       6.529ms       0.000us         0.00%       0.000us       0.000us           0 B           0 B           0 B           0 B           168  \n",
            "                                           forward_pass         5.76%     116.090ms        29.49%     594.093ms      28.290ms       0.000us         0.00%        1.531s      72.889ms           0 B         -84 B      11.09 MB     -22.64 GB            21  \n",
            "                                       cudaLaunchKernel         2.77%      55.885ms        19.54%     393.634ms      85.128us       0.000us         0.00%       2.654ms       0.574us           0 B           0 B           0 B           0 B          4624  \n",
            "                       Runtime Triggered Module Loading        17.54%     353.268ms        17.54%     353.268ms       5.888ms       4.570ms         0.30%       4.570ms      76.164us           0 B           0 B           0 B           0 B            60  \n",
            "                                           aten::linear         0.85%      17.218ms        12.01%     241.880ms     155.650us       0.000us         0.00%        1.338s     861.223us           0 B           0 B       9.98 GB           0 B          1554  \n",
            "                                            aten::addmm         3.14%      63.304ms         9.89%     199.191ms     128.179us        1.332s        87.34%        1.338s     861.223us           0 B           0 B       9.98 GB       9.98 GB          1554  \n",
            "                                         optimizer_step         1.28%      25.809ms         8.68%     174.857ms       8.327ms       0.000us         0.00%       2.260ms     107.621us           8 B           0 B    -180.50 KB     -84.00 KB            21  \n",
            "                              Optimizer.step#AdamW.step         0.93%      18.635ms         3.62%      72.945ms       3.474ms       0.000us         0.00%       1.442ms      68.658us           8 B        -168 B      19.00 KB    -199.50 KB            21  \n",
            "enumerate(DataLoader)#_SingleProcessDataLoaderIter._...         0.80%      16.201ms         2.08%      41.869ms       1.994ms       0.000us         0.00%       0.000us       0.000us     766.50 KB     -27.26 KB           0 B           0 B            21  \n",
            "                                      aten::masked_fill         0.01%     213.557us         2.05%      41.355ms       1.969ms       0.000us         0.00%     149.465us       7.117us           0 B           0 B       5.91 MB           0 B            21  \n",
            "                                          backward_pass         1.10%      22.239ms         2.02%      40.785ms       1.942ms       0.000us         0.00%      50.112us       2.386us           0 B           0 B       6.33 MB       6.32 MB            21  \n",
            "                                     aten::masked_fill_         0.02%     310.297us         2.00%      40.382ms       1.923ms      65.724us         0.00%      78.393us       3.733us           0 B           0 B           0 B           0 B            21  \n",
            "                                          aten::dropout         0.16%       3.124ms         1.77%      35.649ms      65.291us       0.000us         0.00%      25.462ms      46.634us           0 B           0 B       2.31 GB      -1.03 GB           546  \n",
            "                                       aten::layer_norm         0.15%       3.090ms         1.69%      33.978ms      64.721us       0.000us         0.00%      26.619ms      50.703us           0 B           0 B       2.31 GB      -5.82 MB           525  \n",
            "                                   aten::native_dropout         0.47%       9.548ms         1.61%      32.525ms      59.569us      25.310ms         1.66%      25.462ms      46.634us           0 B           0 B       3.33 GB           0 B           546  \n",
            "                                aten::native_layer_norm         0.56%      11.232ms         1.53%      30.888ms      58.834us      26.406ms         1.73%      26.619ms      50.703us           0 B           0 B       2.31 GB           0 B           525  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 2.014s\n",
            "Self CUDA time total: 1.525s\n",
            "\n",
            "\\nBy CUDA Time:\n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                           forward_pass         0.00%       0.000us         0.00%       0.000us       0.000us        1.682s       110.30%        1.682s      80.089ms           0 B           0 B           0 B           0 B            21  \n",
            "                                           forward_pass         5.76%     116.090ms        29.49%     594.093ms      28.290ms       0.000us         0.00%        1.531s      72.889ms           0 B         -84 B      11.09 MB     -22.64 GB            21  \n",
            "                                           aten::linear         0.85%      17.218ms        12.01%     241.880ms     155.650us       0.000us         0.00%        1.338s     861.223us           0 B           0 B       9.98 GB           0 B          1554  \n",
            "                                            aten::addmm         3.14%      63.304ms         9.89%     199.191ms     128.179us        1.332s        87.34%        1.338s     861.223us           0 B           0 B       9.98 GB       9.98 GB          1554  \n",
            "                                  volta_sgemm_128x64_tn         0.00%       0.000us         0.00%       0.000us       0.000us     873.182ms        57.27%     873.182ms       1.733ms           0 B           0 B           0 B           0 B           504  \n",
            "                                 volta_sgemm_128x128_tn         0.00%       0.000us         0.00%       0.000us       0.000us     456.866ms        29.96%     456.866ms     453.240us           0 B           0 B           0 B           0 B          1008  \n",
            "                                         optimizer_step         0.00%       0.000us         0.00%       0.000us       0.000us      78.187ms         5.13%      78.187ms       3.723ms           0 B           0 B           0 B           0 B            21  \n",
            "                     aten::scaled_dot_product_attention         0.19%       3.774ms         1.46%      29.327ms     116.379us       0.000us         0.00%      64.951ms     257.744us           0 B      -3.94 KB       1.11 GB           0 B           252  \n",
            "          aten::_scaled_dot_product_efficient_attention         0.17%       3.418ms         1.17%      23.598ms      93.642us       0.000us         0.00%      64.951ms     257.744us       3.94 KB           0 B       1.11 GB           0 B           252  \n",
            "                     aten::_efficient_attention_forward         0.31%       6.155ms         0.79%      15.948ms      63.285us      63.932ms         4.19%      64.951ms     257.744us       3.94 KB           0 B       1.11 GB           0 B           252  \n",
            "fmha_cutlassF_f32_aligned_64x64_rf_sm75(PyTorchMemEf...         0.00%       0.000us         0.00%       0.000us       0.000us      63.932ms         4.19%      63.932ms     253.700us           0 B           0 B           0 B           0 B           252  \n",
            "                              Optimizer.step#AdamW.step         0.00%       0.000us         0.00%       0.000us       0.000us      59.103ms         3.88%      59.103ms       2.814ms           0 B           0 B           0 B           0 B            21  \n",
            "                                             aten::gelu         0.21%       4.290ms         0.69%      13.871ms      55.042us      40.105ms         2.63%      40.584ms     161.046us           0 B           0 B       4.43 GB       4.43 GB           252  \n",
            "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      40.105ms         2.63%      40.105ms     159.147us           0 B           0 B           0 B           0 B           252  \n",
            "                                       loss_calculation         0.00%       0.000us         0.00%       0.000us       0.000us      32.976ms         2.16%      32.976ms       1.570ms           0 B           0 B           0 B           0 B            21  \n",
            "                                              aten::add         0.41%       8.288ms         1.18%      23.828ms      43.642us      31.582ms         2.07%      31.770ms      58.186us           0 B           0 B       2.31 GB       2.31 GB           546  \n",
            "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      31.535ms         2.07%      31.535ms      60.067us           0 B           0 B           0 B           0 B           525  \n",
            "                                       aten::layer_norm         0.15%       3.090ms         1.69%      33.978ms      64.721us       0.000us         0.00%      26.619ms      50.703us           0 B           0 B       2.31 GB      -5.82 MB           525  \n",
            "                                aten::native_layer_norm         0.56%      11.232ms         1.53%      30.888ms      58.834us      26.406ms         1.73%      26.619ms      50.703us           0 B           0 B       2.31 GB           0 B           525  \n",
            "void at::native::(anonymous namespace)::vectorized_l...         0.00%       0.000us         0.00%       0.000us       0.000us      26.406ms         1.73%      26.406ms      50.296us           0 B           0 B           0 B           0 B           525  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 2.014s\n",
            "Self CUDA time total: 1.525s\n",
            "\n",
            "\\n================================================================================\\n\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Iteration: 100%|██████████| 109/109 [00:11<00:00,  9.76it/s]\n",
            "12/11/2025 02:23:59 - INFO - finbert.utils -   *** Example ***\n",
            "12/11/2025 02:23:59 - INFO - finbert.utils -   guid: validation-1\n",
            "12/11/2025 02:23:59 - INFO - finbert.utils -   tokens: [CLS] our in - depth expertise extends to the fields of energy , industry , urban & mobility and water & environment [SEP]\n",
            "12/11/2025 02:23:59 - INFO - finbert.utils -   input_ids: 101 2256 1999 1011 5995 11532 8908 2000 1996 4249 1997 2943 1010 3068 1010 3923 1004 12969 1998 2300 1004 4044 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/11/2025 02:23:59 - INFO - finbert.utils -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/11/2025 02:23:59 - INFO - finbert.utils -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/11/2025 02:23:59 - INFO - finbert.utils -   label: neutral (id = 2)\n",
            "12/11/2025 02:23:59 - INFO - finbert.finbert -   ***** Loading data *****\n",
            "12/11/2025 02:23:59 - INFO - finbert.finbert -     Num examples = 388\n",
            "12/11/2025 02:23:59 - INFO - finbert.finbert -     Batch size = 32\n",
            "12/11/2025 02:23:59 - INFO - finbert.finbert -     Num steps = 48\n",
            "Validating: 100%|██████████| 13/13 [00:00<00:00, 14.45it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation losses: [0.8864706066938547]\n",
            "No best model found\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Iteration: 100%|██████████| 109/109 [00:17<00:00,  6.37it/s]\n",
            "12/11/2025 02:24:17 - INFO - finbert.utils -   *** Example ***\n",
            "12/11/2025 02:24:17 - INFO - finbert.utils -   guid: validation-1\n",
            "12/11/2025 02:24:17 - INFO - finbert.utils -   tokens: [CLS] our in - depth expertise extends to the fields of energy , industry , urban & mobility and water & environment [SEP]\n",
            "12/11/2025 02:24:17 - INFO - finbert.utils -   input_ids: 101 2256 1999 1011 5995 11532 8908 2000 1996 4249 1997 2943 1010 3068 1010 3923 1004 12969 1998 2300 1004 4044 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/11/2025 02:24:17 - INFO - finbert.utils -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/11/2025 02:24:17 - INFO - finbert.utils -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/11/2025 02:24:17 - INFO - finbert.utils -   label: neutral (id = 2)\n",
            "12/11/2025 02:24:17 - INFO - finbert.finbert -   ***** Loading data *****\n",
            "12/11/2025 02:24:17 - INFO - finbert.finbert -     Num examples = 388\n",
            "12/11/2025 02:24:17 - INFO - finbert.finbert -     Batch size = 32\n",
            "12/11/2025 02:24:17 - INFO - finbert.finbert -     Num steps = 48\n",
            "Validating: 100%|██████████| 13/13 [00:00<00:00, 14.08it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation losses: [0.8864706066938547, 0.5778215527534485]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Iteration: 100%|██████████| 109/109 [00:23<00:00,  4.73it/s]\n",
            "12/11/2025 02:24:42 - INFO - finbert.utils -   *** Example ***\n",
            "12/11/2025 02:24:42 - INFO - finbert.utils -   guid: validation-1\n",
            "12/11/2025 02:24:42 - INFO - finbert.utils -   tokens: [CLS] our in - depth expertise extends to the fields of energy , industry , urban & mobility and water & environment [SEP]\n",
            "12/11/2025 02:24:42 - INFO - finbert.utils -   input_ids: 101 2256 1999 1011 5995 11532 8908 2000 1996 4249 1997 2943 1010 3068 1010 3923 1004 12969 1998 2300 1004 4044 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/11/2025 02:24:42 - INFO - finbert.utils -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/11/2025 02:24:42 - INFO - finbert.utils -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/11/2025 02:24:42 - INFO - finbert.utils -   label: neutral (id = 2)\n",
            "12/11/2025 02:24:42 - INFO - finbert.finbert -   ***** Loading data *****\n",
            "12/11/2025 02:24:42 - INFO - finbert.finbert -     Num examples = 388\n",
            "12/11/2025 02:24:42 - INFO - finbert.finbert -     Batch size = 32\n",
            "12/11/2025 02:24:42 - INFO - finbert.finbert -     Num steps = 48\n",
            "Validating: 100%|██████████| 13/13 [00:00<00:00, 13.69it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation losses: [0.8864706066938547, 0.5778215527534485, 0.4897470955665295]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Iteration: 100%|██████████| 109/109 [00:26<00:00,  4.08it/s]\n",
            "12/11/2025 02:25:11 - INFO - finbert.utils -   *** Example ***\n",
            "12/11/2025 02:25:11 - INFO - finbert.utils -   guid: validation-1\n",
            "12/11/2025 02:25:11 - INFO - finbert.utils -   tokens: [CLS] our in - depth expertise extends to the fields of energy , industry , urban & mobility and water & environment [SEP]\n",
            "12/11/2025 02:25:11 - INFO - finbert.utils -   input_ids: 101 2256 1999 1011 5995 11532 8908 2000 1996 4249 1997 2943 1010 3068 1010 3923 1004 12969 1998 2300 1004 4044 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/11/2025 02:25:11 - INFO - finbert.utils -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/11/2025 02:25:11 - INFO - finbert.utils -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/11/2025 02:25:11 - INFO - finbert.utils -   label: neutral (id = 2)\n",
            "12/11/2025 02:25:11 - INFO - finbert.finbert -   ***** Loading data *****\n",
            "12/11/2025 02:25:11 - INFO - finbert.finbert -     Num examples = 388\n",
            "12/11/2025 02:25:11 - INFO - finbert.finbert -     Batch size = 32\n",
            "12/11/2025 02:25:11 - INFO - finbert.finbert -     Num steps = 48\n",
            "Validating: 100%|██████████| 13/13 [00:00<00:00, 13.03it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation losses: [0.8864706066938547, 0.5778215527534485, 0.4897470955665295, 0.47151784942700314]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch: 100%|██████████| 4/4 [01:24<00:00, 21.24s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "TRAINING COMPLETE\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "if TRAIN_NEW_MODEL:\n",
        "    print(\"=\"*80)\n",
        "    print(\"TRAINING NEW MODEL\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    # Clean the model path\n",
        "    try:\n",
        "        shutil.rmtree(cl_path)\n",
        "        print(\"✓ Cleaned previous model directory\")\n",
        "    except:\n",
        "        print(\"✓ No previous model directory to clean\")\n",
        "    \n",
        "    # Create base BERT model\n",
        "    bertmodel = AutoModelForSequenceClassification.from_pretrained(\n",
        "        'bert-base-uncased', cache_dir=None, num_labels=3\n",
        "    )\n",
        "    \n",
        "    # Create configuration\n",
        "    config = create_finbert_config(\n",
        "        cl_path, cl_data_path, bertmodel, \n",
        "        num_train_epochs=NUM_EPOCHS\n",
        "    )\n",
        "    \n",
        "    # Initialize FinBERT\n",
        "    finbert = initialize_finbert(config, use_profiling=True)\n",
        "    \n",
        "    # Get training data\n",
        "    print(\"\\n✓ Loading training data...\")\n",
        "    train_data = finbert.get_data('train')\n",
        "    \n",
        "    # Create model\n",
        "    print(\"✓ Creating model...\")\n",
        "    model = finbert.create_the_model()\n",
        "    \n",
        "    # Train with profiling\n",
        "    print(\"\\n✓ Starting training with profiling...\")\n",
        "    trained_model = train_model(finbert, train_data, model)\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"TRAINING COMPLETE\")\n",
        "    print(\"=\"*80)\n",
        "else:\n",
        "    print(\"Skipping training - will load existing model\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Load Model Variant\n",
        "\n",
        "This cell loads the selected model variant using the modular registry.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Loading model variant: amp\n",
            "Description: FinBERT with Automatic Mixed Precision (AMP) - Optimal for Training & Inference\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Model loaded successfully\n",
            "✓ Model size: 417.66 MB\n",
            "\n",
            "✓ Ready for inference profiling with amp variant\n"
          ]
        }
      ],
      "source": [
        "# Load the selected model variant\n",
        "model, variant = load_model_variant(SELECTED_VARIANT, cl_path)\n",
        "\n",
        "print(f\"\\n✓ Ready for inference profiling with {SELECTED_VARIANT} variant\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Profile Inference\n",
        "\n",
        "Run inference profiling on the selected model variant.\n",
        "\n",
        "### Profiling with Selected Variant\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "12/11/2025 02:25:15 - INFO - finbert.utils -   *** Example ***\n",
            "12/11/2025 02:25:15 - INFO - finbert.utils -   guid: 0\n",
            "12/11/2025 02:25:15 - INFO - finbert.utils -   tokens: [CLS] later that day apple said it was rev ##ising down its earnings expectations in the fourth quarter of 2018 , largely because of lower sales and signs of economic weakness in china . [SEP]\n",
            "12/11/2025 02:25:15 - INFO - finbert.utils -   input_ids: 101 2101 2008 2154 6207 2056 2009 2001 7065 9355 2091 2049 16565 10908 1999 1996 2959 4284 1997 2760 1010 4321 2138 1997 2896 4341 1998 5751 1997 3171 11251 1999 2859 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/11/2025 02:25:15 - INFO - finbert.utils -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/11/2025 02:25:15 - INFO - finbert.utils -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/11/2025 02:25:15 - INFO - finbert.utils -   label: None (id = 9090)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "Device: cuda:0\n",
            "GPU Name: Tesla T4\n",
            "GPU Memory: 14.6 GB\n",
            "================================================================================\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/tmp/ipykernel_222855/3935091711.py:91: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=use_amp):\n",
            "12/11/2025 02:25:15 - INFO - finbert.utils -   *** Example ***\n",
            "12/11/2025 02:25:15 - INFO - finbert.utils -   guid: 0\n",
            "12/11/2025 02:25:15 - INFO - finbert.utils -   tokens: [CLS] yields on government bonds fell as investors fled to the traditional haven in a market storm . [SEP]\n",
            "12/11/2025 02:25:15 - INFO - finbert.utils -   input_ids: 101 16189 2006 2231 9547 3062 2004 9387 6783 2000 1996 3151 4033 1999 1037 3006 4040 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/11/2025 02:25:15 - INFO - finbert.utils -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/11/2025 02:25:15 - INFO - finbert.utils -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/11/2025 02:25:15 - INFO - finbert.utils -   label: None (id = 9090)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "Inference Profiling - amp\n",
            "================================================================================\n",
            "\n",
            "By CPU Time:\n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                      inference_forward        10.54%      11.166ms        43.75%      46.352ms      23.176ms       0.000us         0.00%      34.363ms      17.181ms           0 B          -8 B       1.00 KB    -248.22 MB             2  \n",
            "                                           aten::linear         1.39%       1.470ms        24.24%      25.678ms     173.503us       0.000us         0.00%      31.437ms     212.409us           0 B           0 B     121.52 MB           0 B           148  \n",
            "                                  sentence_tokenization        21.47%      22.743ms        21.47%      22.743ms      22.743ms       0.000us         0.00%       0.000us       0.000us           0 B           0 B           0 B           0 B             1  \n",
            "                                            aten::addmm         5.02%       5.321ms        20.75%      21.984ms     148.542us      31.326ms        91.44%      31.437ms     212.409us           0 B           0 B     121.52 MB     121.52 MB           148  \n",
            "cudaOccupancyMaxActiveBlocksPerMultiprocessorWithFla...         0.04%      46.290us        13.36%      14.151ms       3.538ms       0.000us         0.00%     101.530us      25.382us           0 B           0 B           0 B           0 B             4  \n",
            "                       Runtime Triggered Module Loading        13.16%      13.940ms        13.16%      13.940ms       3.485ms      46.844us         0.14%      46.844us      11.711us           0 B           0 B           0 B           0 B             4  \n",
            "                                               aten::to         0.30%     313.167us        12.52%      13.260ms      30.982us       0.000us         0.00%      42.014us       0.098us          72 B           0 B     129.00 KB           0 B           428  \n",
            "                                    postprocess_results         2.68%       2.834ms        12.47%      13.210ms       6.605ms       0.000us         0.00%       4.192us       2.096us           0 B         -72 B      -1.00 KB      -1.00 KB             2  \n",
            "                                         aten::_to_copy         0.10%     102.665us        12.22%      12.947ms       1.079ms       0.000us         0.00%      42.014us       3.501us          72 B           0 B     129.00 KB           0 B            12  \n",
            "                                            aten::copy_         0.17%     182.916us        12.01%      12.726ms     909.007us      47.678us         0.14%      47.678us       3.406us           0 B           0 B           0 B           0 B            14  \n",
            "                                    convert_to_features        10.99%      11.638ms        10.99%      11.638ms       5.819ms       0.000us         0.00%       0.000us       0.000us           0 B           0 B           0 B           0 B             2  \n",
            "                                        cudaMemcpyAsync         9.84%      10.420ms         9.84%      10.420ms     868.343us       0.000us         0.00%       0.000us       0.000us           0 B           0 B           0 B           0 B            12  \n",
            "                                        model_to_device         6.18%       6.542ms         6.87%       7.273ms       3.636ms       0.000us         0.00%       0.000us       0.000us           0 B           0 B           0 B           0 B             2  \n",
            "                                        prepare_tensors         1.97%       2.085ms         4.34%       4.594ms       2.297ms       0.000us         0.00%       2.208us       1.104us           0 B      -9.00 KB       1.50 KB      -7.50 KB             2  \n",
            "                                       cudaLaunchKernel         3.02%       3.195ms         3.03%       3.209ms       8.745us       0.000us         0.00%       8.639us       0.024us           0 B           0 B           0 B           0 B           367  \n",
            "                     aten::scaled_dot_product_attention         0.34%     365.021us         2.51%       2.654ms     110.597us       0.000us         0.00%       1.336ms      55.687us           0 B        -384 B      13.50 MB           0 B            24  \n",
            "                                       aten::layer_norm         0.27%     284.736us         2.43%       2.574ms      51.470us       0.000us         0.00%     739.056us      14.781us           0 B           0 B      28.13 MB     -92.00 KB            50  \n",
            "                                aten::native_layer_norm         0.94%     992.043us         2.16%       2.289ms      45.776us     739.056us         2.16%     739.056us      14.781us           0 B           0 B      28.22 MB           0 B            50  \n",
            "          aten::_scaled_dot_product_efficient_attention         0.28%     295.096us         2.01%       2.128ms      88.676us       0.000us         0.00%       1.336ms      55.687us         384 B           0 B      13.50 MB           0 B            24  \n",
            "                                Activity Buffer Request         1.94%       2.054ms         1.94%       2.054ms       2.054ms       0.000us         0.00%       0.000us       0.000us           0 B           0 B           0 B           0 B             1  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 105.938ms\n",
            "Self CUDA time total: 34.259ms\n",
            "\n",
            "\n",
            "By CUDA Time:\n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                      inference_forward         0.00%       0.000us         0.00%       0.000us       0.000us      55.105ms       160.85%      55.105ms      27.552ms           0 B           0 B           0 B           0 B             2  \n",
            "                                      inference_forward        10.54%      11.166ms        43.75%      46.352ms      23.176ms       0.000us         0.00%      34.363ms      17.181ms           0 B          -8 B       1.00 KB    -248.22 MB             2  \n",
            "                                           aten::linear         1.39%       1.470ms        24.24%      25.678ms     173.503us       0.000us         0.00%      31.437ms     212.409us           0 B           0 B     121.52 MB           0 B           148  \n",
            "                                            aten::addmm         5.02%       5.321ms        20.75%      21.984ms     148.542us      31.326ms        91.44%      31.437ms     212.409us           0 B           0 B     121.52 MB     121.52 MB           148  \n",
            "                                  volta_sgemm_128x64_tn         0.00%       0.000us         0.00%       0.000us       0.000us      30.767ms        89.81%      30.767ms     213.658us           0 B           0 B           0 B           0 B           144  \n",
            "                                        prepare_tensors         0.00%       0.000us         0.00%       0.000us       0.000us       1.343ms         3.92%       1.343ms       1.343ms           0 B           0 B           0 B           0 B             1  \n",
            "                     aten::scaled_dot_product_attention         0.34%     365.021us         2.51%       2.654ms     110.597us       0.000us         0.00%       1.336ms      55.687us           0 B        -384 B      13.50 MB           0 B            24  \n",
            "          aten::_scaled_dot_product_efficient_attention         0.28%     295.096us         2.01%       2.128ms      88.676us       0.000us         0.00%       1.336ms      55.687us         384 B           0 B      13.50 MB           0 B            24  \n",
            "                     aten::_efficient_attention_forward         0.40%     423.856us         1.37%       1.452ms      60.513us       1.336ms         3.90%       1.336ms      55.687us         384 B           0 B      13.50 MB           0 B            24  \n",
            "fmha_cutlassF_f32_aligned_64x64_rf_sm75(PyTorchMemEf...         0.00%       0.000us         0.00%       0.000us       0.000us       1.336ms         3.90%       1.336ms      55.687us           0 B           0 B           0 B           0 B            24  \n",
            "                                       aten::layer_norm         0.27%     284.736us         2.43%       2.574ms      51.470us       0.000us         0.00%     739.056us      14.781us           0 B           0 B      28.13 MB     -92.00 KB            50  \n",
            "                                aten::native_layer_norm         0.94%     992.043us         2.16%       2.289ms      45.776us     739.056us         2.16%     739.056us      14.781us           0 B           0 B      28.22 MB           0 B            50  \n",
            "void at::native::(anonymous namespace)::vectorized_l...         0.00%       0.000us         0.00%       0.000us       0.000us     739.056us         2.16%     739.056us      15.083us           0 B           0 B           0 B           0 B            49  \n",
            "                                             aten::gelu         0.37%     387.800us         0.58%     617.578us      25.732us     427.318us         1.25%     427.318us      17.805us           0 B           0 B      54.00 MB      54.00 MB            24  \n",
            "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     427.318us         1.25%     427.318us      17.805us           0 B           0 B           0 B           0 B            24  \n",
            "void cublasLt::splitKreduce_kernel<32, 16, int, floa...         0.00%       0.000us         0.00%       0.000us       0.000us     392.668us         1.15%     392.668us       8.181us           0 B           0 B           0 B           0 B            48  \n",
            "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     315.768us         0.92%     315.768us       6.315us           0 B           0 B           0 B           0 B            50  \n",
            "                                              aten::add         0.81%     854.404us         1.21%       1.284ms      25.687us     311.704us         0.91%     311.704us       6.234us           0 B           0 B      28.12 MB      28.12 MB            50  \n",
            "cudaOccupancyMaxActiveBlocksPerMultiprocessorWithFla...         0.04%      46.290us        13.36%      14.151ms       3.538ms       0.000us         0.00%     101.530us      25.382us           0 B           0 B           0 B           0 B             4  \n",
            "                                        Memset (Device)         0.00%       0.000us         0.00%       0.000us       0.000us      96.826us         0.28%      96.826us       1.153us           0 B           0 B           0 B           0 B            84  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 105.938ms\n",
            "Self CUDA time total: 34.259ms\n",
            "\n",
            "\n",
            "================================================================================\n",
            "\n",
            "\n",
            "Inference Summary:\n",
            "  Total sentences: 6\n",
            "  Total inference time: 46.22 ms\n",
            "  Time per sentence: 7.70 ms\n",
            "\n",
            "================================================================================\n",
            "PREDICTION RESULTS\n",
            "================================================================================\n",
            "                                                                                                                                                                       sentence prediction  sentiment_score\n",
            "Later that day Apple said it was revising down its earnings expectations in the fourth quarter of 2018, largely because of lower sales and signs of economic weakness in China.   negative        -0.826606\n",
            "                                                                                                                                   The news rapidly infected financial markets.   negative        -0.670445\n",
            "                                             Apple's share price fell by around 7% in after-hours trading and the decline was extended to more than 10% when the market opened.   negative        -0.832117\n",
            "                                                  The dollar fell by 3.7% against the yen in a matter of minutes after the announcement, before rapidly recovering some ground.   negative        -0.811834\n",
            "                                                                                                  Asian stockmarkets closed down on January 3rd and European ones opened lower.   negative        -0.810283\n",
            "                                                                                  Yields on government bonds fell as investors fled to the traditional haven in a market storm.   negative        -0.818802\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# Run profiled inference with the selected variant\n",
        "results_df, metrics = profile_inference(\n",
        "    model=model,\n",
        "    text=TEST_TEXT,\n",
        "    variant_name=SELECTED_VARIANT,\n",
        "    use_gpu=USE_GPU,\n",
        "    gpu_name=GPU_NAME\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"PREDICTION RESULTS\")\n",
        "print(\"=\"*80)\n",
        "print(results_df[['sentence', 'prediction', 'sentiment_score']].to_string(index=False))\n",
        "print(\"=\"*80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
