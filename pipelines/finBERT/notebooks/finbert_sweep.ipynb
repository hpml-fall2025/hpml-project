{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79b281b6",
   "metadata": {},
   "source": [
    "# FinBERT Hyperparameter Sweep \n",
    "\n",
    "This notebook extends the standard FinBERT training with:\n",
    "- Hyperparameter optimization using W&B sweeps\n",
    "- Automated experiment tracking\n",
    "- Comparison of different configurations\n",
    "\n",
    "## Prerequisites\n",
    "```bash\n",
    "pip install wandb\n",
    "wandb login\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1238b10b",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94331ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import shutil\n",
    "import os\n",
    "import logging\n",
    "import sys\n",
    "import numpy as np\n",
    "sys.path.append('..')\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "import torch\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "from finbert.finbert import *\n",
    "import finbert.utils as tools\n",
    "\n",
    "# Weights & Biases\n",
    "import wandb\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "project_dir = Path.cwd().parent\n",
    "pd.set_option('max_colwidth', None)\n",
    "\n",
    "logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "                    datefmt = '%m/%d/%Y %H:%M:%S',\n",
    "                    level = logging.ERROR)\n",
    "\n",
    "print(\"Imports loaded successfully\")\n",
    "print(f\"Project directory: {project_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c40ff6b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## 2. Configuration\n",
    "\n",
    "Set up paths and W&B project name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a21fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "cl_path = project_dir/'models'/'sentiment'\n",
    "cl_data_path = project_dir/'data'/'sentiment_data'\n",
    "\n",
    "# W&B Configuration\n",
    "WANDB_PROJECT = \"finbert-hyperparameter-sweep\"\n",
    "WANDB_ENTITY = None  \n",
    "\n",
    "print(f\"Model path: {cl_path}\")\n",
    "print(f\"Data path: {cl_data_path}\")\n",
    "print(f\"W&B Project: {WANDB_PROJECT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c10240",
   "metadata": {},
   "source": [
    "## 3. Define Sweep Configuration\n",
    "\n",
    "This defines the hyperparameter search space. W&B will automatically try different combinations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8e6be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    'method': 'bayes',  # 'grid', 'random', or 'bayes'\n",
    "    'metric': {\n",
    "        'name': 'val_loss',\n",
    "        'goal': 'minimize'\n",
    "    },\n",
    "    'parameters': {\n",
    "        'learning_rate': {\n",
    "            'distribution': 'log_uniform_values',\n",
    "            'min': 1e-5,\n",
    "            'max': 5e-5\n",
    "        },\n",
    "        'num_train_epochs': {\n",
    "            'values': [3, 4, 5, 6]\n",
    "        },\n",
    "        'train_batch_size': {\n",
    "            'values': [16, 32, 64]\n",
    "        },\n",
    "        'warm_up_proportion': {\n",
    "            'distribution': 'uniform',\n",
    "            'min': 0.1,\n",
    "            'max': 0.3\n",
    "        },\n",
    "        'max_seq_length': {\n",
    "            'values': [48, 64, 96]\n",
    "        },\n",
    "        # Advanced parameters\n",
    "        'discriminate': {\n",
    "            'values': [True, False]\n",
    "        },\n",
    "        'gradual_unfreeze': {\n",
    "            'values': [True, False]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Sweep configuration created\")\n",
    "print(f\"  Method: {sweep_config['method']}\")\n",
    "print(f\"  Optimization metric: {sweep_config['metric']['name']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa40dc9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_config(config=None):\n",
    "    with wandb.init(config=config):\n",
    "        config = wandb.config\n",
    "\n",
    "        # Print basic config\n",
    "        print(\"Starting run:\")\n",
    "        print(f\"LR={config.learning_rate}, Epochs={config.num_train_epochs}\")\n",
    "\n",
    "        # Create BERT model\n",
    "        bertmodel = AutoModelForSequenceClassification.from_pretrained(\n",
    "            'bert-base-uncased',\n",
    "            num_labels=3\n",
    "        )\n",
    "\n",
    "        # Create minimal FinBERT config\n",
    "        finbert_config = Config(\n",
    "            data_dir=cl_data_path,\n",
    "            bert_model=bertmodel,\n",
    "            num_train_epochs=config.num_train_epochs,\n",
    "            model_dir=project_dir / 'models' / 'sentiment',\n",
    "            max_seq_length=config.max_seq_length,\n",
    "            train_batch_size=config.train_batch_size,\n",
    "            learning_rate=config.learning_rate,\n",
    "            output_mode='classification',\n",
    "            warm_up_proportion=config.warm_up_proportion,\n",
    "            local_rank=-1\n",
    "        )\n",
    "\n",
    "        # Initialize FinBERT\n",
    "        finbert = FinBert(finbert_config)\n",
    "        finbert.prepare_model(label_list=['positive', 'negative', 'neutral'])\n",
    "\n",
    "        # Load data\n",
    "        train_data = finbert.get_data('train')\n",
    "        test_data = finbert.get_data('test')\n",
    "\n",
    "        # Build model\n",
    "        model = finbert.create_the_model()\n",
    "\n",
    "        # Train\n",
    "        trained_model = basic_train_loop(finbert, train_data, model)\n",
    "\n",
    "        # Evaluate\n",
    "        results = finbert.evaluate(examples=test_data, model=trained_model)\n",
    "        results['prediction'] = results.predictions.apply(lambda x: np.argmax(x, axis=0))\n",
    "\n",
    "        # Compute metrics\n",
    "        metrics = calculate_metrics(results, finbert)\n",
    "\n",
    "        # Log\n",
    "        wandb.log(metrics)\n",
    "\n",
    "        return metrics\n",
    "\n",
    "\n",
    "def train_loop(finbert, train_data, model):\n",
    "    train_loader = finbert.get_loader(train_data, 'train')\n",
    "    optimizer = finbert.optimizer\n",
    "    scheduler = finbert.scheduler\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(finbert.config.num_train_epochs):\n",
    "        total_loss = 0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            batch = tuple(t.to(finbert.device) for t in batch)\n",
    "            input_ids, attention_mask, token_type_ids, label_ids, agree_ids = batch\n",
    "\n",
    "            logits = model(input_ids, attention_mask, token_type_ids)[0]\n",
    "\n",
    "            loss_fct = CrossEntropyLoss(weight=finbert.class_weights.to(finbert.device))\n",
    "            loss = loss_fct(logits.view(-1, finbert.num_labels), label_ids.view(-1))\n",
    "\n",
    "            loss.backward()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        wandb.log({\"epoch_loss\": total_loss})\n",
    "        print(f\"Epoch {epoch}: Loss={total_loss:.4f}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def calculate_metrics(results, finbert):\n",
    "    \"\"\"\n",
    "    Basic metric calculation.\n",
    "    \"\"\"\n",
    "    from sklearn.metrics import f1_score\n",
    "\n",
    "    loss_fn = CrossEntropyLoss(weight=finbert.class_weights)\n",
    "    loss = loss_fn(\n",
    "        torch.tensor(list(results['predictions'])),\n",
    "        torch.tensor(list(results['labels']))\n",
    "    ).item()\n",
    "\n",
    "    accuracy = (results['labels'] == results['prediction']).mean()\n",
    "\n",
    "    f1_scores = f1_score(results['labels'], results['prediction'], average=None)\n",
    "    f1_macro = f1_score(results['labels'], results['prediction'], average='macro')\n",
    "\n",
    "    return {\n",
    "        \"loss\": loss,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"f1_positive\": f1_scores[0],\n",
    "        \"f1_negative\": f1_scores[1],\n",
    "        \"f1_neutral\": f1_scores[2],\n",
    "        \"f1_macro\": f1_macro,\n",
    "    }"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
