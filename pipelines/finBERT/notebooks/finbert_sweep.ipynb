{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79b281b6",
   "metadata": {},
   "source": [
    "# FinBERT Hyperparameter Sweep \n",
    "\n",
    "This notebook extends the standard FinBERT training with:\n",
    "- Hyperparameter optimization using W&B sweeps\n",
    "- Automated experiment tracking\n",
    "- Comparison of different configurations\n",
    "\n",
    "## Prerequisites\n",
    "```bash\n",
    "pip install wandb\n",
    "wandb login\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1238b10b",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94331ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import shutil\n",
    "import os\n",
    "import logging\n",
    "import sys\n",
    "import numpy as np\n",
    "sys.path.append('..')\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "import torch\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "from finbert.finbert import *\n",
    "import finbert.utils as tools\n",
    "\n",
    "# Weights & Biases\n",
    "import wandb\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "project_dir = Path.cwd().parent\n",
    "pd.set_option('max_colwidth', None)\n",
    "\n",
    "logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "                    datefmt = '%m/%d/%Y %H:%M:%S',\n",
    "                    level = logging.ERROR)\n",
    "\n",
    "print(\"Imports loaded successfully\")\n",
    "print(f\"Project directory: {project_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c40ff6b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## 2. Configuration\n",
    "\n",
    "Set up paths and W&B project name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a21fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "cl_path = project_dir/'models'/'sentiment'\n",
    "cl_data_path = project_dir/'data'/'sentiment_data'\n",
    "\n",
    "# W&B Configuration\n",
    "WANDB_PROJECT = \"finbert-hyperparameter-sweep\"\n",
    "WANDB_ENTITY = None  \n",
    "\n",
    "print(f\"Model path: {cl_path}\")\n",
    "print(f\"Data path: {cl_data_path}\")\n",
    "print(f\"W&B Project: {WANDB_PROJECT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c10240",
   "metadata": {},
   "source": [
    "## 3. Define Sweep Configuration\n",
    "\n",
    "This defines the hyperparameter search space. W&B will automatically try different combinations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8e6be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    'method': 'bayes',  # 'grid', 'random', or 'bayes'\n",
    "    'metric': {\n",
    "        'name': 'val_loss',\n",
    "        'goal': 'minimize'\n",
    "    },\n",
    "    'parameters': {\n",
    "        'learning_rate': {\n",
    "            'distribution': 'log_uniform_values',\n",
    "            'min': 1e-5,\n",
    "            'max': 5e-5\n",
    "        },\n",
    "        'num_train_epochs': {\n",
    "            'values': [3, 4, 5, 6]\n",
    "        },\n",
    "        'train_batch_size': {\n",
    "            'values': [16, 32, 64]\n",
    "        },\n",
    "        'warm_up_proportion': {\n",
    "            'distribution': 'uniform',\n",
    "            'min': 0.1,\n",
    "            'max': 0.3\n",
    "        },\n",
    "        'max_seq_length': {\n",
    "            'values': [48, 64, 96]\n",
    "        },\n",
    "        # Advanced parameters\n",
    "        'discriminate': {\n",
    "            'values': [True, False]\n",
    "        },\n",
    "        'gradual_unfreeze': {\n",
    "            'values': [True, False]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Sweep configuration created\")\n",
    "print(f\"  Method: {sweep_config['method']}\")\n",
    "print(f\"  Optimization metric: {sweep_config['metric']['name']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e413a37",
   "metadata": {},
   "source": [
    "## 4. Training Function with W&B Integration\n",
    "\n",
    "This wraps existing training code with W&B logging.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa40dc9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_config(config=None):\n",
    "    \"\"\"\n",
    "    Training function that W&B will call with different hyperparameters.\n",
    "    \"\"\"\n",
    "    # Initialize W&B run\n",
    "    with wandb.init(config=config):\n",
    "        # Get hyperparameters from W&B\n",
    "        config = wandb.config\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Starting training run with config:\")\n",
    "        print(f\"  Learning rate: {config.learning_rate}\")\n",
    "        print(f\"  Epochs: {config.num_train_epochs}\")\n",
    "        print(f\"  Batch size: {config.train_batch_size}\")\n",
    "        print(f\"  Warmup: {config.warm_up_proportion}\")\n",
    "        print(f\"  Max seq length: {config.max_seq_length}\")\n",
    "        print(f\"{'='*80}\\n\")\n",
    "        \n",
    "        # Clean previous model directory\n",
    "        model_path = project_dir / 'models' / 'sentiment' / f'sweep_{wandb.run.id}'\n",
    "        try:\n",
    "            shutil.rmtree(model_path)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Create BERT model\n",
    "        bertmodel = AutoModelForSequenceClassification.from_pretrained(\n",
    "            'bert-base-uncased', cache_dir=None, num_labels=3\n",
    "        )\n",
    "        \n",
    "        # Create FinBERT config with hyperparameters from sweep\n",
    "        finbert_config = Config(\n",
    "            data_dir=cl_data_path,\n",
    "            bert_model=bertmodel,\n",
    "            num_train_epochs=config.num_train_epochs,\n",
    "            model_dir=model_path,\n",
    "            max_seq_length=config.max_seq_length,\n",
    "            train_batch_size=config.train_batch_size,\n",
    "            learning_rate=config.learning_rate,\n",
    "            output_mode='classification',\n",
    "            warm_up_proportion=config.warm_up_proportion,\n",
    "            local_rank=-1,\n",
    "            discriminate=config.discriminate,\n",
    "            gradual_unfreeze=config.gradual_unfreeze\n",
    "        )\n",
    "        \n",
    "        # Initialize FinBERT\n",
    "        finbert = FinBert(finbert_config)\n",
    "        finbert.base_model = 'bert-base-uncased'\n",
    "        finbert.config.discriminate = config.discriminate\n",
    "        finbert.config.gradual_unfreeze = config.gradual_unfreeze\n",
    "        finbert.prepare_model(label_list=['positive', 'negative', 'neutral'])\n",
    "        \n",
    "        # Load data\n",
    "        train_data = finbert.get_data('train')\n",
    "        test_data = finbert.get_data('test')\n",
    "        \n",
    "        # Create model\n",
    "        model = finbert.create_the_model()\n",
    "        \n",
    "        # Train with W&B logging\n",
    "        trained_model = train_with_wandb_logging(\n",
    "            finbert, train_data, model, test_data\n",
    "        )\n",
    "        \n",
    "        # Final evaluation\n",
    "        results = finbert.evaluate(examples=test_data, model=trained_model)\n",
    "        results['prediction'] = results.predictions.apply(lambda x: np.argmax(x, axis=0))\n",
    "        \n",
    "        # Calculate metrics\n",
    "        metrics = calculate_metrics(results, finbert)\n",
    "        \n",
    "        # Log final metrics to W&B\n",
    "        wandb.log({\n",
    "            'final_test_loss': metrics['loss'],\n",
    "            'final_test_accuracy': metrics['accuracy'],\n",
    "            'final_f1_positive': metrics['f1_positive'],\n",
    "            'final_f1_negative': metrics['f1_negative'],\n",
    "            'final_f1_neutral': metrics['f1_neutral'],\n",
    "            'final_f1_macro': metrics['f1_macro']\n",
    "        })\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Final Results:\")\n",
    "        print(f\"  Test Loss: {metrics['loss']:.4f}\")\n",
    "        print(f\"  Test Accuracy: {metrics['accuracy']:.4f}\")\n",
    "        print(f\"  Macro F1: {metrics['f1_macro']:.4f}\")\n",
    "        print(f\"{'='*80}\\n\")\n",
    "        \n",
    "        return metrics\n",
    "\n",
    "\n",
    "def train_with_wandb_logging(finbert, train_data, model, test_data):\n",
    "    \"\"\"\n",
    "    Modified training loop with W&B logging.\n",
    "    \"\"\"\n",
    "    validation_examples = finbert.get_data('validation')\n",
    "    global_step = 0\n",
    "    finbert.validation_losses = []\n",
    "    \n",
    "    train_dataloader = finbert.get_loader(train_data, 'train') \n",
    "    model.train()\n",
    "    step_number = len(train_dataloader)\n",
    "    \n",
    "    i = 0\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    for epoch in trange(int(finbert.config.num_train_epochs), desc=\"Epoch\"):\n",
    "        model.train()\n",
    "        tr_loss = 0\n",
    "        nb_tr_examples, nb_tr_steps = 0, 0\n",
    "        \n",
    "        for step, batch in enumerate(tqdm(train_dataloader, desc='Iteration')):\n",
    "            \n",
    "            # Gradual unfreezing logic\n",
    "            if (finbert.config.gradual_unfreeze and i == 0):\n",
    "                for param in model.bert.parameters():\n",
    "                    param.requires_grad = False\n",
    "            \n",
    "            if (step % (step_number // 3)) == 0:\n",
    "                i += 1\n",
    "            \n",
    "            if (finbert.config.gradual_unfreeze and i > 1 and i < finbert.config.encoder_no):\n",
    "                for k in range(i - 1):\n",
    "                    try:\n",
    "                        for param in model.bert.encoder.layer[finbert.config.encoder_no - 1 - k].parameters():\n",
    "                            param.requires_grad = True\n",
    "                    except:\n",
    "                        pass\n",
    "            \n",
    "            if (finbert.config.gradual_unfreeze and i > finbert.config.encoder_no + 1):\n",
    "                for param in model.bert.embeddings.parameters():\n",
    "                    param.requires_grad = True\n",
    "            \n",
    "            batch = tuple(t.to(finbert.device) for t in batch)\n",
    "            input_ids, attention_mask, token_type_ids, label_ids, agree_ids = batch\n",
    "            \n",
    "            logits = model(input_ids, attention_mask, token_type_ids)[0]\n",
    "            weights = finbert.class_weights.to(finbert.device)\n",
    "            \n",
    "            if finbert.config.output_mode == \"classification\":\n",
    "                loss_fct = CrossEntropyLoss(weight=weights)\n",
    "                loss = loss_fct(logits.view(-1, finbert.num_labels), label_ids.view(-1))\n",
    "            elif finbert.config.output_mode == \"regression\":\n",
    "                loss_fct = MSELoss()\n",
    "                loss = loss_fct(logits.view(-1), label_ids.view(-1))\n",
    "            \n",
    "            if finbert.config.gradient_accumulation_steps > 1:\n",
    "                loss = loss / finbert.config.gradient_accumulation_steps\n",
    "            else:\n",
    "                loss.backward()\n",
    "            \n",
    "            tr_loss += loss.item()\n",
    "            nb_tr_examples += input_ids.size(0)\n",
    "            nb_tr_steps += 1\n",
    "            \n",
    "            if (step + 1) % finbert.config.gradient_accumulation_steps == 0:\n",
    "                if finbert.config.fp16:\n",
    "                    lr_this_step = finbert.config.learning_rate * warmup_linear(\n",
    "                        global_step / finbert.num_train_optimization_steps, \n",
    "                        finbert.config.warm_up_proportion\n",
    "                    )\n",
    "                    for param_group in finbert.optimizer.param_groups:\n",
    "                        param_group['lr'] = lr_this_step\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                finbert.optimizer.step()\n",
    "                finbert.scheduler.step()\n",
    "                finbert.optimizer.zero_grad()\n",
    "                global_step += 1\n",
    "                \n",
    "                # Log to W&B every N steps\n",
    "                if global_step % 10 == 0:\n",
    "                    wandb.log({\n",
    "                        'train_loss': tr_loss / nb_tr_steps,\n",
    "                        'learning_rate': finbert.optimizer.param_groups[0]['lr'],\n",
    "                        'epoch': epoch,\n",
    "                        'step': global_step\n",
    "                    })\n",
    "        \n",
    "        # Validation at end of epoch\n",
    "        validation_loader = finbert.get_loader(validation_examples, 'eval')\n",
    "        model.eval()\n",
    "        \n",
    "        valid_loss, valid_accuracy = 0, 0\n",
    "        nb_valid_steps, nb_valid_examples = 0, 0\n",
    "        \n",
    "        for input_ids, attention_mask, token_type_ids, label_ids, agree_ids in tqdm(validation_loader, desc=\"Validating\"):\n",
    "            input_ids = input_ids.to(finbert.device)\n",
    "            attention_mask = attention_mask.to(finbert.device)\n",
    "            token_type_ids = token_type_ids.to(finbert.device)\n",
    "            label_ids = label_ids.to(finbert.device)\n",
    "            agree_ids = agree_ids.to(finbert.device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                logits = model(input_ids, attention_mask, token_type_ids)[0]\n",
    "                \n",
    "                if finbert.config.output_mode == \"classification\":\n",
    "                    loss_fct = CrossEntropyLoss(weight=weights)\n",
    "                    tmp_valid_loss = loss_fct(logits.view(-1, finbert.num_labels), label_ids.view(-1))\n",
    "                elif finbert.config.output_mode == \"regression\":\n",
    "                    loss_fct = MSELoss()\n",
    "                    tmp_valid_loss = loss_fct(logits.view(-1), label_ids.view(-1))\n",
    "                \n",
    "                valid_loss += tmp_valid_loss.mean().item()\n",
    "                nb_valid_steps += 1\n",
    "        \n",
    "        valid_loss = valid_loss / nb_valid_steps\n",
    "        finbert.validation_losses.append(valid_loss)\n",
    "        \n",
    "        # Log validation metrics to W&B\n",
    "        wandb.log({\n",
    "            'val_loss': valid_loss,\n",
    "            'epoch': epoch,\n",
    "            'best_val_loss': min(finbert.validation_losses)\n",
    "        })\n",
    "        \n",
    "        print(f\"Epoch {epoch}: Validation loss = {valid_loss:.4f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if valid_loss == min(finbert.validation_losses):\n",
    "            try:\n",
    "                os.remove(finbert.config.model_dir / ('temporary' + str(best_model)))\n",
    "            except:\n",
    "                pass\n",
    "            torch.save({'epoch': str(epoch), 'state_dict': model.state_dict()},\n",
    "                       finbert.config.model_dir / ('temporary' + str(epoch)))\n",
    "            best_model = epoch\n",
    "            best_val_loss = valid_loss\n",
    "    \n",
    "    # Load best model\n",
    "    checkpoint = torch.load(finbert.config.model_dir / ('temporary' + str(best_model)))\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    \n",
    "    # Save final model\n",
    "    model_to_save = model.module if hasattr(model, 'module') else model\n",
    "    output_model_file = os.path.join(finbert.config.model_dir, 'pytorch_model.bin')\n",
    "    torch.save(model_to_save.state_dict(), output_model_file)\n",
    "    \n",
    "    # Clean up temporary files\n",
    "    try:\n",
    "        os.remove(finbert.config.model_dir / ('temporary' + str(best_model)))\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def calculate_metrics(results, finbert):\n",
    "    \"\"\"\n",
    "    Calculate comprehensive metrics for evaluation.\n",
    "    \"\"\"\n",
    "    from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "    \n",
    "    cs = CrossEntropyLoss(weight=finbert.class_weights)\n",
    "    loss = cs(\n",
    "        torch.tensor(list(results['predictions'])),\n",
    "        torch.tensor(list(results['labels']))\n",
    "    )\n",
    "    \n",
    "    accuracy = (results['labels'] == results['prediction']).sum() / results.shape[0]\n",
    "    \n",
    "    # Calculate per-class F1 scores\n",
    "    f1_scores = f1_score(results['labels'], results['prediction'], average=None)\n",
    "    f1_macro = f1_score(results['labels'], results['prediction'], average='macro')\n",
    "    \n",
    "    return {\n",
    "        'loss': loss.item(),\n",
    "        'accuracy': accuracy,\n",
    "        'f1_positive': f1_scores[0],\n",
    "        'f1_negative': f1_scores[1],\n",
    "        'f1_neutral': f1_scores[2],\n",
    "        'f1_macro': f1_macro\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"Training functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4c05f6",
   "metadata": {},
   "source": [
    "## 5. Initialize and Run Sweep\n",
    "\n",
    "This will start the hyperparameter sweep. W&B will automatically try different combinations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3487a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the sweep\n",
    "sweep_id = wandb.sweep(\n",
    "    sweep_config, \n",
    "    project=WANDB_PROJECT,\n",
    "    entity=WANDB_ENTITY\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
