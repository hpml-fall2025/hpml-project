{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# FinBERT LoRA Hyperparameter Sweep\n",
        "\n",
        "This notebook extends the standard FinBERT sweep to tune **LoRA-specific** hyperparameters:\n",
        "- LoRA rank (`lora_r`)\n",
        "- LoRA alpha (`lora_alpha`) \n",
        "- LoRA dropout (`lora_dropout`)\n",
        "- Target modules for LoRA adaptation\n",
        "- Higher learning rates suitable for LoRA training\n",
        "\n",
        "## Key Differences from Standard Sweep\n",
        "- `discriminate=False` and `gradual_unfreeze=False` are fixed (incompatible with LoRA)\n",
        "- Learning rate range is higher (1e-4 to 1e-3 vs 1e-5 to 5e-5)\n",
        "- Additional LoRA-specific hyperparameters in sweep space\n",
        "\n",
        "## Prerequisites\n",
        "```bash\n",
        "pip install wandb peft\n",
        "wandb login\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import shutil\n",
        "import os\n",
        "import logging\n",
        "import sys\n",
        "import numpy as np\n",
        "sys.path.append('..')\n",
        "\n",
        "from sklearn.metrics import classification_report, f1_score\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "import torch\n",
        "from torch.nn import CrossEntropyLoss, MSELoss\n",
        "\n",
        "from finbert.finbert import *\n",
        "import finbert.utils as tools\n",
        "\n",
        "# Weights & Biases\n",
        "import wandb\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "project_dir = Path.cwd().parent\n",
        "pd.set_option('max_colwidth', None)\n",
        "\n",
        "logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
        "                    datefmt = '%m/%d/%Y %H:%M:%S',\n",
        "                    level = logging.ERROR)\n",
        "\n",
        "print(\"Imports loaded successfully\")\n",
        "print(f\"Project directory: {project_dir}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Configuration\n",
        "\n",
        "Set up paths and W&B project name.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Paths\n",
        "cl_path = project_dir/'models'/'sentiment_lora'\n",
        "cl_data_path = project_dir/'data'/'sentiment_data'\n",
        "\n",
        "# W&B Configuration\n",
        "WANDB_PROJECT = \"finbert-lora-hyperparameter-sweep\"\n",
        "WANDB_ENTITY = None  # Set your W&B entity/username if needed\n",
        "\n",
        "print(f\"Model path: {cl_path}\")\n",
        "print(f\"Data path: {cl_data_path}\")\n",
        "print(f\"W&B Project: {WANDB_PROJECT}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Define LoRA Sweep Configuration\n",
        "\n",
        "This defines the hyperparameter search space specifically for LoRA fine-tuning.\n",
        "\n",
        "### Key LoRA Parameters:\n",
        "- **lora_r**: Rank of the low-rank matrices. Higher = more parameters, more capacity\n",
        "- **lora_alpha**: Scaling factor. Often set to 2*r or equal to r\n",
        "- **lora_dropout**: Dropout applied to LoRA layers\n",
        "- **lora_target_modules**: Which attention matrices to apply LoRA to\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sweep_config = {\n",
        "    'method': 'bayes',  # 'grid', 'random', or 'bayes'\n",
        "    'metric': {\n",
        "        'name': 'val_loss',\n",
        "        'goal': 'minimize'\n",
        "    },\n",
        "    'parameters': {\n",
        "        # LoRA-specific parameters\n",
        "        'lora_r': {\n",
        "            'values': [4, 8, 16, 32]\n",
        "        },\n",
        "        'lora_alpha': {\n",
        "            'values': [8, 16, 32, 64]\n",
        "        },\n",
        "        'lora_dropout': {\n",
        "            'distribution': 'uniform',\n",
        "            'min': 0.0,\n",
        "            'max': 0.2\n",
        "        },\n",
        "        'lora_target_modules': {\n",
        "            'values': [\n",
        "                ['query', 'value'],           # Standard: Q and V matrices\n",
        "                ['query', 'key', 'value'],    # All attention matrices\n",
        "                ['query', 'value', 'dense'],  # Q, V + output projection\n",
        "            ]\n",
        "        },\n",
        "        # Higher learning rates for LoRA (key difference from full fine-tuning)\n",
        "        'learning_rate': {\n",
        "            'distribution': 'log_uniform_values',\n",
        "            'min': 1e-4,\n",
        "            'max': 1e-3\n",
        "        },\n",
        "        # Standard training parameters\n",
        "        'num_train_epochs': {\n",
        "            'values': [5, 8, 10, 15]\n",
        "        },\n",
        "        'train_batch_size': {\n",
        "            'values': [16, 32, 64]\n",
        "        },\n",
        "        'warm_up_proportion': {\n",
        "            'distribution': 'uniform',\n",
        "            'min': 0.1,\n",
        "            'max': 0.3\n",
        "        },\n",
        "        'max_seq_length': {\n",
        "            'values': [48, 64, 96]\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "print(\"LoRA Sweep configuration created\")\n",
        "print(f\"  Method: {sweep_config['method']}\")\n",
        "print(f\"  Optimization metric: {sweep_config['metric']['name']}\")\n",
        "print(f\"  LoRA parameters being tuned:\")\n",
        "print(f\"    - lora_r: {sweep_config['parameters']['lora_r']['values']}\")\n",
        "print(f\"    - lora_alpha: {sweep_config['parameters']['lora_alpha']['values']}\")\n",
        "print(f\"    - lora_dropout: [{sweep_config['parameters']['lora_dropout']['min']}, {sweep_config['parameters']['lora_dropout']['max']}]\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Training Function with LoRA and W&B Integration\n",
        "\n",
        "This wraps the LoRA-enabled training code with W&B logging.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_with_lora_config(config=None):\n",
        "    \"\"\"\n",
        "    Training function that W&B will call with different LoRA hyperparameters.\n",
        "    \"\"\"\n",
        "    # Initialize W&B run\n",
        "    with wandb.init(config=config):\n",
        "        # Get hyperparameters from W&B\n",
        "        config = wandb.config\n",
        "        \n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(f\"Starting LoRA training run with config:\")\n",
        "        print(f\"  LoRA r: {config.lora_r}\")\n",
        "        print(f\"  LoRA alpha: {config.lora_alpha}\")\n",
        "        print(f\"  LoRA dropout: {config.lora_dropout:.4f}\")\n",
        "        print(f\"  LoRA target modules: {config.lora_target_modules}\")\n",
        "        print(f\"  Learning rate: {config.learning_rate:.6f}\")\n",
        "        print(f\"  Epochs: {config.num_train_epochs}\")\n",
        "        print(f\"  Batch size: {config.train_batch_size}\")\n",
        "        print(f\"  Warmup: {config.warm_up_proportion:.4f}\")\n",
        "        print(f\"  Max seq length: {config.max_seq_length}\")\n",
        "        print(f\"{'='*80}\\n\")\n",
        "        \n",
        "        # Clean previous model directory\n",
        "        model_path = project_dir / 'models' / 'lora_sweep' / f'sweep_{wandb.run.id}'\n",
        "        try:\n",
        "            shutil.rmtree(model_path)\n",
        "        except:\n",
        "            pass\n",
        "        \n",
        "        # Create BERT model\n",
        "        bertmodel = AutoModelForSequenceClassification.from_pretrained(\n",
        "            'bert-base-uncased', cache_dir=None, num_labels=3\n",
        "        )\n",
        "        \n",
        "        # Create FinBERT config with LoRA hyperparameters from sweep\n",
        "        finbert_config = Config(\n",
        "            data_dir=cl_data_path,\n",
        "            bert_model=bertmodel,\n",
        "            num_train_epochs=config.num_train_epochs,\n",
        "            model_dir=model_path,\n",
        "            max_seq_length=config.max_seq_length,\n",
        "            train_batch_size=config.train_batch_size,\n",
        "            learning_rate=config.learning_rate,\n",
        "            output_mode='classification',\n",
        "            warm_up_proportion=config.warm_up_proportion,\n",
        "            local_rank=-1,\n",
        "            # LoRA settings\n",
        "            use_lora=True,\n",
        "            lora_r=config.lora_r,\n",
        "            lora_alpha=config.lora_alpha,\n",
        "            lora_dropout=config.lora_dropout,\n",
        "            lora_target_modules=tuple(config.lora_target_modules),\n",
        "            # These must be OFF for LoRA\n",
        "            discriminate=False,\n",
        "            gradual_unfreeze=False\n",
        "        )\n",
        "        \n",
        "        # Initialize FinBERT\n",
        "        finbert = FinBert(finbert_config)\n",
        "        finbert.base_model = 'bert-base-uncased'\n",
        "        finbert.prepare_model(label_list=['positive', 'negative', 'neutral'])\n",
        "        \n",
        "        # Load data\n",
        "        train_data = finbert.get_data('train')\n",
        "        test_data = finbert.get_data('test')\n",
        "        \n",
        "        # Create model with LoRA\n",
        "        model = finbert.create_the_model()\n",
        "        \n",
        "        # Log trainable parameters info\n",
        "        total_params = sum(p.numel() for p in model.parameters())\n",
        "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "        wandb.log({\n",
        "            'total_params': total_params,\n",
        "            'trainable_params': trainable_params,\n",
        "            'trainable_percent': 100 * trainable_params / total_params\n",
        "        })\n",
        "        \n",
        "        # Train with W&B logging\n",
        "        trained_model = train_lora_with_wandb_logging(\n",
        "            finbert, train_data, model, test_data\n",
        "        )\n",
        "        \n",
        "        # Final evaluation\n",
        "        results = finbert.evaluate(examples=test_data, model=trained_model)\n",
        "        results['prediction'] = results.predictions.apply(lambda x: np.argmax(x, axis=0))\n",
        "        \n",
        "        # Calculate metrics\n",
        "        metrics = calculate_metrics(results, finbert)\n",
        "        \n",
        "        # Log final metrics to W&B\n",
        "        wandb.log({\n",
        "            'final_test_loss': metrics['loss'],\n",
        "            'final_test_accuracy': metrics['accuracy'],\n",
        "            'final_f1_positive': metrics['f1_positive'],\n",
        "            'final_f1_negative': metrics['f1_negative'],\n",
        "            'final_f1_neutral': metrics['f1_neutral'],\n",
        "            'final_f1_macro': metrics['f1_macro']\n",
        "        })\n",
        "        \n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(f\"Final Results:\")\n",
        "        print(f\"  Test Loss: {metrics['loss']:.4f}\")\n",
        "        print(f\"  Test Accuracy: {metrics['accuracy']:.4f}\")\n",
        "        print(f\"  Macro F1: {metrics['f1_macro']:.4f}\")\n",
        "        print(f\"  Trainable params: {trainable_params:,} ({100*trainable_params/total_params:.2f}%)\")\n",
        "        print(f\"{'='*80}\\n\")\n",
        "        \n",
        "        return metrics\n",
        "\n",
        "\n",
        "print(\"Main training function defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_lora_with_wandb_logging(finbert, train_data, model, test_data):\n",
        "    \"\"\"\n",
        "    Modified training loop with W&B logging for LoRA models.\n",
        "    Note: gradual_unfreeze is disabled for LoRA training.\n",
        "    \"\"\"\n",
        "    validation_examples = finbert.get_data('validation')\n",
        "    global_step = 0\n",
        "    finbert.validation_losses = []\n",
        "    \n",
        "    train_dataloader = finbert.get_loader(train_data, 'train') \n",
        "    model.train()\n",
        "    step_number = len(train_dataloader)\n",
        "    \n",
        "    best_val_loss = float('inf')\n",
        "    best_model_epoch = 0\n",
        "    \n",
        "    for epoch in trange(int(finbert.config.num_train_epochs), desc=\"Epoch\"):\n",
        "        model.train()\n",
        "        tr_loss = 0\n",
        "        nb_tr_examples, nb_tr_steps = 0, 0\n",
        "        \n",
        "        for step, batch in enumerate(tqdm(train_dataloader, desc='Iteration')):\n",
        "            batch = tuple(t.to(finbert.device) for t in batch)\n",
        "            input_ids, attention_mask, token_type_ids, label_ids, agree_ids = batch\n",
        "            \n",
        "            outputs = model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                token_type_ids=token_type_ids\n",
        "            )\n",
        "            logits = outputs.logits\n",
        "            weights = finbert.class_weights.to(finbert.device)\n",
        "            \n",
        "            if finbert.config.output_mode == \"classification\":\n",
        "                loss_fct = CrossEntropyLoss(weight=weights)\n",
        "                loss = loss_fct(logits.view(-1, finbert.num_labels), label_ids.view(-1))\n",
        "            elif finbert.config.output_mode == \"regression\":\n",
        "                loss_fct = MSELoss()\n",
        "                loss = loss_fct(logits.view(-1), label_ids.view(-1))\n",
        "            \n",
        "            if finbert.config.gradient_accumulation_steps > 1:\n",
        "                loss = loss / finbert.config.gradient_accumulation_steps\n",
        "            else:\n",
        "                loss.backward()\n",
        "            \n",
        "            tr_loss += loss.item()\n",
        "            nb_tr_examples += input_ids.size(0)\n",
        "            nb_tr_steps += 1\n",
        "            \n",
        "            if (step + 1) % finbert.config.gradient_accumulation_steps == 0:\n",
        "                torch.nn.utils.clip_grad_norm_(\n",
        "                    (p for p in model.parameters() if p.requires_grad), 1.0\n",
        "                )\n",
        "                finbert.optimizer.step()\n",
        "                finbert.scheduler.step()\n",
        "                finbert.optimizer.zero_grad()\n",
        "                global_step += 1\n",
        "                \n",
        "                # Log to W&B every N steps\n",
        "                if global_step % 10 == 0:\n",
        "                    wandb.log({\n",
        "                        'train_loss': tr_loss / nb_tr_steps,\n",
        "                        'learning_rate': finbert.optimizer.param_groups[0]['lr'],\n",
        "                        'epoch': epoch,\n",
        "                        'step': global_step\n",
        "                    })\n",
        "        \n",
        "        # Validation at end of epoch\n",
        "        validation_loader = finbert.get_loader(validation_examples, 'eval')\n",
        "        model.eval()\n",
        "        \n",
        "        valid_loss, valid_accuracy = 0, 0\n",
        "        nb_valid_steps, nb_valid_examples = 0, 0\n",
        "        \n",
        "        for input_ids, attention_mask, token_type_ids, label_ids, agree_ids in tqdm(validation_loader, desc=\"Validating\"):\n",
        "            input_ids = input_ids.to(finbert.device)\n",
        "            attention_mask = attention_mask.to(finbert.device)\n",
        "            token_type_ids = token_type_ids.to(finbert.device)\n",
        "            label_ids = label_ids.to(finbert.device)\n",
        "            agree_ids = agree_ids.to(finbert.device)\n",
        "            \n",
        "            with torch.no_grad():\n",
        "                outputs = model(\n",
        "                    input_ids=input_ids,\n",
        "                    attention_mask=attention_mask,\n",
        "                    token_type_ids=token_type_ids\n",
        "                )\n",
        "                logits = outputs.logits\n",
        "                \n",
        "                if finbert.config.output_mode == \"classification\":\n",
        "                    loss_fct = CrossEntropyLoss(weight=weights)\n",
        "                    tmp_valid_loss = loss_fct(logits.view(-1, finbert.num_labels), label_ids.view(-1))\n",
        "                elif finbert.config.output_mode == \"regression\":\n",
        "                    loss_fct = MSELoss()\n",
        "                    tmp_valid_loss = loss_fct(logits.view(-1), label_ids.view(-1))\n",
        "                \n",
        "                valid_loss += tmp_valid_loss.mean().item()\n",
        "                nb_valid_steps += 1\n",
        "        \n",
        "        valid_loss = valid_loss / nb_valid_steps\n",
        "        finbert.validation_losses.append(valid_loss)\n",
        "        \n",
        "        # Log validation metrics to W&B\n",
        "        wandb.log({\n",
        "            'val_loss': valid_loss,\n",
        "            'epoch': epoch,\n",
        "            'best_val_loss': min(finbert.validation_losses)\n",
        "        })\n",
        "        \n",
        "        print(f\"Epoch {epoch}: Validation loss = {valid_loss:.4f}\")\n",
        "        \n",
        "        # Save best model\n",
        "        if valid_loss == min(finbert.validation_losses):\n",
        "            try:\n",
        "                os.remove(finbert.config.model_dir / ('temporary' + str(best_model_epoch)))\n",
        "            except:\n",
        "                pass\n",
        "            torch.save({'epoch': str(epoch), 'state_dict': model.state_dict()},\n",
        "                       finbert.config.model_dir / ('temporary' + str(epoch)))\n",
        "            best_model_epoch = epoch\n",
        "            best_val_loss = valid_loss\n",
        "    \n",
        "    # Load best model\n",
        "    checkpoint = torch.load(finbert.config.model_dir / ('temporary' + str(best_model_epoch)))\n",
        "    model.load_state_dict(checkpoint['state_dict'])\n",
        "    \n",
        "    # Save final model\n",
        "    model_to_save = model.module if hasattr(model, 'module') else model\n",
        "    output_model_file = os.path.join(finbert.config.model_dir, 'pytorch_model.bin')\n",
        "    torch.save(model_to_save.state_dict(), output_model_file)\n",
        "    \n",
        "    # Clean up temporary files\n",
        "    try:\n",
        "        os.remove(finbert.config.model_dir / ('temporary' + str(best_model_epoch)))\n",
        "    except:\n",
        "        pass\n",
        "    \n",
        "    return model\n",
        "\n",
        "\n",
        "def calculate_metrics(results, finbert):\n",
        "    \"\"\"\n",
        "    Calculate comprehensive metrics for evaluation.\n",
        "    \"\"\"\n",
        "    cs = CrossEntropyLoss(weight=finbert.class_weights)\n",
        "    loss = cs(\n",
        "        torch.tensor(list(results['predictions'])),\n",
        "        torch.tensor(list(results['labels']))\n",
        "    )\n",
        "    \n",
        "    accuracy = (results['labels'] == results['prediction']).sum() / results.shape[0]\n",
        "    \n",
        "    # Calculate per-class F1 scores\n",
        "    f1_scores = f1_score(results['labels'], results['prediction'], average=None)\n",
        "    f1_macro = f1_score(results['labels'], results['prediction'], average='macro')\n",
        "    \n",
        "    return {\n",
        "        'loss': loss.item(),\n",
        "        'accuracy': accuracy,\n",
        "        'f1_positive': f1_scores[0],\n",
        "        'f1_negative': f1_scores[1],\n",
        "        'f1_neutral': f1_scores[2],\n",
        "        'f1_macro': f1_macro\n",
        "    }\n",
        "\n",
        "\n",
        "print(\"Helper functions defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Initialize and Run LoRA Sweep\n",
        "\n",
        "This will start the hyperparameter sweep. W&B will automatically try different LoRA configurations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize the sweep\n",
        "sweep_id = wandb.sweep(\n",
        "    sweep_config, \n",
        "    project=WANDB_PROJECT,\n",
        "    entity=WANDB_ENTITY\n",
        ")\n",
        "\n",
        "print(f\"LoRA Sweep initialized with ID: {sweep_id}\")\n",
        "print(f\"View at: https://wandb.ai/{WANDB_ENTITY or 'your-username'}/{WANDB_PROJECT}/sweeps/{sweep_id}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run the sweep\n",
        "# count: number of runs to execute (increase for more thorough search)\n",
        "wandb.agent(sweep_id, function=train_with_lora_config, count=15)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"LORA SWEEP COMPLETED\")\n",
        "print(\"=\"*80)\n",
        "print(f\"View results at: https://wandb.ai/{WANDB_ENTITY or 'your-username'}/{WANDB_PROJECT}/sweeps/{sweep_id}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Analyze Results\n",
        "\n",
        "After the sweep completes, examine the best configuration from the W&B dashboard.\n",
        "\n",
        "Key metrics to compare:\n",
        "- **val_loss**: Primary optimization target\n",
        "- **final_test_accuracy**: Test set performance\n",
        "- **final_f1_macro**: Balanced F1 across classes\n",
        "- **trainable_params**: Parameter efficiency\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Once you've identified the best config from the W&B dashboard, record it here:\n",
        "# Example (update with your actual best values from the sweep)\n",
        "best_lora_config = {\n",
        "    # LoRA parameters\n",
        "    'lora_r': 8,\n",
        "    'lora_alpha': 16,\n",
        "    'lora_dropout': 0.1,\n",
        "    'lora_target_modules': ['query', 'value'],\n",
        "    # Training parameters\n",
        "    'learning_rate': 5e-4,\n",
        "    'num_train_epochs': 10,\n",
        "    'train_batch_size': 32,\n",
        "    'warm_up_proportion': 0.2,\n",
        "    'max_seq_length': 64,\n",
        "}\n",
        "\n",
        "print(\"Best LoRA configuration (update after sweep):\")\n",
        "for k, v in best_lora_config.items():\n",
        "    print(f\"  {k}: {v}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Retrain with Best Config (Optional)\n",
        "\n",
        "Use the best configuration to train a final model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Uncomment to retrain with best config\n",
        "# train_with_lora_config(config=best_lora_config)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
