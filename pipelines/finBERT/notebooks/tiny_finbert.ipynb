{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# FinBERT Profiling: Baseline \n",
        "\n",
        "This notebook is intentionally **thin**: it reuses the profiling utilities in `pipelines/finBERT/finbert/` (especially `finbert/finbert_profile.py` and `finbert/profile_utils.py`) instead of copying large code blocks.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ]
        }
      ],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "from pathlib import Path\n",
        "import shutil\n",
        "import os\n",
        "import logging\n",
        "import sys\n",
        "sys.path.append('..')\n",
        "\n",
        "from textblob import TextBlob\n",
        "from pprint import pprint\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "from finbert.finbert import *\n",
        "from finbert.finbert_profile import *\n",
        "from finbert.profile_utils import get_model_size_mb, print_device_info, setup_nltk_data\n",
        "import finbert.utils as tools\n",
        "\n",
        "\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "project_dir = Path.cwd().parent\n",
        "pd.set_option('max_colwidth', None)\n",
        "\n",
        "import wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">training-distilled_bert-profiled</strong> at: <a href='https://wandb.ai/si2449-columbia-university/finbert-experiments/runs/e0dstvv0' target=\"_blank\">https://wandb.ai/si2449-columbia-university/finbert-experiments/runs/e0dstvv0</a><br> View project at: <a href='https://wandb.ai/si2449-columbia-university/finbert-experiments' target=\"_blank\">https://wandb.ai/si2449-columbia-university/finbert-experiments</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20251215_144426-e0dstvv0/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.23.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/srm2245/hpml-project/pipelines/finBERT/notebooks/wandb/run-20251215_144723-ljuktgop</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/si2449-columbia-university/finbert-experiments/runs/ljuktgop' target=\"_blank\">training-distilled_bert-profiled</a></strong> to <a href='https://wandb.ai/si2449-columbia-university/finbert-experiments' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/si2449-columbia-university/finbert-experiments' target=\"_blank\">https://wandb.ai/si2449-columbia-university/finbert-experiments</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/si2449-columbia-university/finbert-experiments/runs/ljuktgop' target=\"_blank\">https://wandb.ai/si2449-columbia-university/finbert-experiments/runs/ljuktgop</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/si2449-columbia-university/finbert-experiments/runs/ljuktgop?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7fe3c409f760>"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wandb.init(\n",
        "    entity=\"si2449-columbia-university\",\n",
        "    project=\"finbert-experiments\",\n",
        "    name=\"training-distilled_bert-profiled\",\n",
        "    group=\"knowledge-distillation\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "cl_path = project_dir/'models'/'student'\n",
        "cl_data_path = project_dir/'data'/'sentiment_data'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-mini and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    shutil.rmtree(cl_path) \n",
        "except:\n",
        "    pass\n",
        "\n",
        "bertmodel = AutoModelForSequenceClassification.from_pretrained(\"prajjwal1/bert-mini\", cache_dir=None, num_labels=3)\n",
        "\n",
        "\n",
        "config = Config(   data_dir=cl_data_path,\n",
        "                   bert_model=bertmodel,\n",
        "                   num_train_epochs=4,\n",
        "                   model_dir=cl_path,\n",
        "                   max_seq_length = 48,\n",
        "                   train_batch_size = 32,\n",
        "                   learning_rate = 2e-5,\n",
        "                   output_mode='classification',\n",
        "                   warm_up_proportion=0.2,\n",
        "                   local_rank=-1,\n",
        "                   discriminate=True,\n",
        "                   gradual_unfreeze=True,\n",
        "                   encoder_no = 4)\n",
        "\n",
        "config.profile_train_steps = 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "from contextlib import nullcontext\n",
        "from typing import Any\n",
        "\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "\n",
        "def timed_eval(\n",
        "    *, finbert: FinBert, model: torch.nn.Module, examples, use_amp: bool\n",
        ") -> tuple[pd.DataFrame, dict[str, Any]]:\n",
        "    \"\"\"Evaluation loop with optional CUDA autocast + timing (kept small for notebook use).\"\"\"\n",
        "    loader = finbert.get_loader(examples, phase=\"eval\")\n",
        "    device = finbert.device\n",
        "\n",
        "    model.eval()\n",
        "    preds: list[np.ndarray] = []\n",
        "    labels: list[int] = []\n",
        "\n",
        "    if device.type == \"cuda\":\n",
        "        torch.cuda.synchronize(device)\n",
        "\n",
        "    start = time.perf_counter()\n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            batch = tuple(t.to(device) for t in batch)\n",
        "            input_ids, attention_mask, token_type_ids, label_ids, _agree_ids = batch\n",
        "\n",
        "            logits = model(input_ids=input_ids, attention_mask=attention_mask)[0]\n",
        "\n",
        "            preds.extend(logits.detach().cpu().numpy())\n",
        "            labels.extend(label_ids.detach().cpu().numpy().tolist())\n",
        "\n",
        "    if device.type == \"cuda\":\n",
        "        torch.cuda.synchronize(device)\n",
        "\n",
        "    wall_s = time.perf_counter() - start\n",
        "    n = len(labels)\n",
        "\n",
        "    results_df = pd.DataFrame({\"predictions\": preds, \"labels\": labels})\n",
        "\n",
        "    timing = {\n",
        "        \"eval_wall_s\": float(wall_s),\n",
        "        \"eval_num_samples\": int(n),\n",
        "        \"eval_samples_per_s\": float(n / wall_s) if wall_s > 0 else float(\"inf\"),\n",
        "    }\n",
        "    return results_df, timing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "finbert = ProfiledFinBert(config)\n",
        "finbert.base_model = 'prajjwal1/bert-mini'\n",
        "finbert.config.discriminate=True\n",
        "finbert.config.gradual_unfreeze=True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "12/15/2025 14:47:29 - INFO - finbert.finbert -   device: cuda n_gpu: 1, distributed training: False, 16-bits training: False\n"
          ]
        }
      ],
      "source": [
        "finbert.prepare_model(label_list=['positive','negative','neutral'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_data = finbert.get_data('train')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = finbert.create_the_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "freeze = 2\n",
        "\n",
        "for param in model.bert.embeddings.parameters():\n",
        "    param.requires_grad = False\n",
        "    \n",
        "for i in range(freeze):\n",
        "    for param in model.bert.encoder.layer[i].parameters():\n",
        "        param.requires_grad = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "12/15/2025 14:48:15 - INFO - finbert.utils -   *** Example ***\n",
            "12/15/2025 14:48:15 - INFO - finbert.utils -   guid: train-1\n",
            "12/15/2025 14:48:15 - INFO - finbert.utils -   tokens: [CLS] after the reporting period , bio ##tie north american licensing partner so ##max ##on pharmaceuticals announced positive results with na ##lm ##efe ##ne in a pilot phase 2 clinical trial for smoking ce ##ssa ##tion [SEP]\n",
            "12/15/2025 14:48:15 - INFO - finbert.utils -   input_ids: 101 2044 1996 7316 2558 1010 16012 9515 2167 2137 13202 4256 2061 17848 2239 24797 2623 3893 3463 2007 6583 13728 27235 2638 1999 1037 4405 4403 1016 6612 3979 2005 9422 8292 11488 3508 102 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/15/2025 14:48:15 - INFO - finbert.utils -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/15/2025 14:48:15 - INFO - finbert.utils -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/15/2025 14:48:15 - INFO - finbert.utils -   label: positive (id = 0)\n",
            "12/15/2025 14:48:15 - INFO - finbert.finbert -   ***** Loading data *****\n",
            "12/15/2025 14:48:15 - INFO - finbert.finbert -     Num examples = 3488\n",
            "12/15/2025 14:48:15 - INFO - finbert.finbert -     Batch size = 32\n",
            "12/15/2025 14:48:15 - INFO - finbert.finbert -     Num steps = 48\n",
            "/home/srm2245/hpml-project/pipelines/finBERT/notebooks/../finbert/finbert_profile.py:138: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=self.config.use_amp)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\\n================================================================================\n",
            "Starting Profiled Training\n",
            "Device: cuda\n",
            "Profiling activities: [<ProfilerActivity.CPU: 0>, <ProfilerActivity.CUDA: 2>]\n",
            "================================================================================\\n\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch:   0%|          | 0/4 [00:00<?, ?it/s]/home/srm2245/hpml-project/pipelines/finBERT/notebooks/../finbert/finbert_profile.py:191: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=self.config.use_amp):\n",
            "/home/srm2245/hpml-project/pipelines/finBERT/notebooks/../finbert/finbert_profile.py:196: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=self.config.use_amp):\n",
            "Iteration:  17%|█▋        | 19/109 [00:00<00:02, 33.03it/s]\n",
            "Epoch:   0%|          | 0/4 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\\n================================================================================\n",
            "Profiling complete for first epoch (20 steps)\n",
            "Continuing full training without profiling...\n",
            "================================================================================\\n\n",
            "\\n================================================================================\n",
            "PROFILING RESULTS - Training\n",
            "================================================================================\\n\n",
            "\\nBy CPU Time:\n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                           forward_pass         7.96%      45.732ms        51.02%     293.213ms      14.661ms       0.000us         0.00%     125.152ms       6.258ms           0 B         -80 B       9.75 MB      -2.51 GB            20  \n",
            "                                       cudaLaunchKernel         5.39%      30.963ms        49.97%     287.224ms     132.728us       0.000us         0.00%       1.234ms       0.570us           0 B           0 B           0 B           0 B          2164  \n",
            "                       Runtime Triggered Module Loading        47.12%     270.812ms        47.12%     270.812ms       4.514ms       1.264ms         0.99%       1.264ms      21.067us           0 B           0 B           0 B           0 B            60  \n",
            "                                         optimizer_step         2.07%      11.908ms        24.73%     142.132ms       7.107ms       0.000us         0.00%       2.622ms     131.084us           8 B           0 B     -63.00 KB     -80.00 KB            20  \n",
            "                                           aten::linear         0.91%       5.215ms        14.61%      83.973ms     161.486us       0.000us         0.00%      93.932ms     180.638us           0 B           0 B       1.06 GB           0 B           520  \n",
            "                                            aten::addmm         4.76%      27.343ms        12.23%      70.306ms     135.204us      93.255ms        73.40%      93.932ms     180.638us           0 B           0 B       1.06 GB       1.06 GB           520  \n",
            "                              Optimizer.step#AdamW.step         1.98%      11.394ms        10.16%      58.403ms       2.920ms       0.000us         0.00%       1.630ms      81.507us           8 B        -160 B       7.00 KB     -70.00 KB            20  \n",
            "                                      aten::masked_fill         0.03%     166.068us         9.25%      53.171ms       2.659ms       0.000us         0.00%     183.455us       9.173us           0 B           0 B       5.62 MB           0 B            20  \n",
            "                                     aten::masked_fill_         0.04%     212.509us         9.09%      52.244ms       2.612ms      82.270us         0.06%      94.654us       4.733us           0 B           0 B           0 B           0 B            20  \n",
            "                                       loss_calculation         0.86%       4.950ms         7.93%      45.596ms       2.280ms       0.000us         0.00%     218.811us      10.941us           0 B           0 B      21.00 KB     -19.00 KB            20  \n",
            "enumerate(DataLoader)#_SingleProcessDataLoaderIter._...         1.97%      11.298ms         6.03%      34.679ms       1.734ms       0.000us         0.00%       0.000us       0.000us     730.00 KB     -27.26 KB           0 B           0 B            20  \n",
            "                                          backward_pass         2.80%      16.089ms         5.86%      33.664ms       1.683ms       0.000us         0.00%      61.150us       3.057us           0 B           0 B       7.55 MB       7.54 MB            20  \n",
            "                                               aten::to         0.14%     782.243us         5.46%      31.368ms      97.417us       0.000us         0.00%     615.219us       1.911us           0 B           0 B       7.76 MB           0 B           322  \n",
            "                                         aten::_to_copy         0.21%       1.207ms         5.32%      30.586ms     191.162us       0.000us         0.00%     615.219us       3.845us           0 B           0 B       7.76 MB           0 B           160  \n",
            "                                            aten::copy_         0.24%       1.376ms         4.97%      28.555ms     158.642us     666.933us         0.52%     704.020us       3.911us           0 B           0 B           0 B           0 B           180  \n",
            "                               aten::cross_entropy_loss         0.05%     273.165us         4.53%      26.059ms       1.303ms       0.000us         0.00%     205.307us      10.265us           0 B           0 B      30.00 KB           0 B            20  \n",
            "                                    aten::_foreach_sqrt         0.09%     509.445us         3.66%      21.009ms     525.229us     200.187us         0.16%     215.835us       5.396us           0 B           0 B      70.00 KB           0 B            40  \n",
            "                               aten::linalg_vector_norm         0.08%     464.954us         3.39%      19.461ms     973.051us     186.110us         0.15%     206.942us      10.347us           0 B           0 B      10.00 KB      10.00 KB            20  \n",
            "                                      aten::log_softmax         0.02%     126.334us         3.15%      18.125ms     906.227us       0.000us         0.00%      77.310us       3.865us           0 B           0 B      10.00 KB           0 B            20  \n",
            "                                     aten::_log_softmax         0.09%     535.723us         3.13%      17.983ms     899.153us      67.134us         0.05%      77.310us       3.865us           0 B           0 B      10.00 KB      10.00 KB            20  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 574.758ms\n",
            "Self CUDA time total: 127.042ms\n",
            "\n",
            "\\nBy CUDA Time:\n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                           forward_pass         0.00%       0.000us         0.00%       0.000us       0.000us     299.146ms       235.47%     299.146ms      14.957ms           0 B           0 B           0 B           0 B            20  \n",
            "                                           forward_pass         7.96%      45.732ms        51.02%     293.213ms      14.661ms       0.000us         0.00%     125.152ms       6.258ms           0 B         -80 B       9.75 MB      -2.51 GB            20  \n",
            "                                           aten::linear         0.91%       5.215ms        14.61%      83.973ms     161.486us       0.000us         0.00%      93.932ms     180.638us           0 B           0 B       1.06 GB           0 B           520  \n",
            "                                            aten::addmm         4.76%      27.343ms        12.23%      70.306ms     135.204us      93.255ms        73.40%      93.932ms     180.638us           0 B           0 B       1.06 GB       1.06 GB           520  \n",
            "                                  volta_sgemm_128x64_tn         0.00%       0.000us         0.00%       0.000us       0.000us      92.116ms        72.51%      92.116ms     191.909us           0 B           0 B           0 B           0 B           480  \n",
            "                                         optimizer_step         0.00%       0.000us         0.00%       0.000us       0.000us      72.653ms        57.19%      72.653ms       3.633ms           0 B           0 B           0 B           0 B            20  \n",
            "                              Optimizer.step#AdamW.step         0.00%       0.000us         0.00%       0.000us       0.000us      50.952ms        40.11%      50.952ms       2.548ms           0 B           0 B           0 B           0 B            20  \n",
            "                                       loss_calculation         0.00%       0.000us         0.00%       0.000us       0.000us      28.642ms        22.55%      28.642ms       1.432ms           0 B           0 B           0 B           0 B            20  \n",
            "                     aten::scaled_dot_product_attention         0.21%       1.198ms         1.70%       9.766ms     122.071us       0.000us         0.00%      11.868ms     148.349us           0 B      -1.25 KB     120.00 MB           0 B            80  \n",
            "          aten::_scaled_dot_product_efficient_attention         0.17%     948.360us         1.38%       7.942ms      99.270us       0.000us         0.00%      11.868ms     148.349us       1.25 KB           0 B     120.00 MB           0 B            80  \n",
            "                     aten::_efficient_attention_forward         0.32%       1.817ms         0.99%       5.714ms      71.425us      11.445ms         9.01%      11.868ms     148.349us       1.25 KB           0 B     120.00 MB           0 B            80  \n",
            "fmha_cutlassF_f32_aligned_64x64_rf_sm75(PyTorchMemEf...         0.00%       0.000us         0.00%       0.000us       0.000us      11.445ms         9.01%      11.445ms     143.063us           0 B           0 B           0 B           0 B            80  \n",
            "                                       aten::layer_norm         0.19%       1.073ms         2.24%      12.850ms      71.392us       0.000us         0.00%       7.536ms      41.866us           0 B           0 B     270.16 MB      -1.95 MB           180  \n",
            "                                aten::native_layer_norm         0.68%       3.909ms         2.05%      11.778ms      65.432us       7.412ms         5.83%       7.536ms      41.866us           0 B           0 B     272.11 MB           0 B           180  \n",
            "void at::native::(anonymous namespace)::vectorized_l...         0.00%       0.000us         0.00%       0.000us       0.000us       7.412ms         5.83%       7.412ms      41.180us           0 B           0 B           0 B           0 B           180  \n",
            "                                             aten::gelu         0.24%       1.403ms         1.53%       8.768ms     109.605us       4.428ms         3.49%       4.593ms      57.412us           0 B           0 B     480.00 MB     480.00 MB            80  \n",
            "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       4.428ms         3.49%       4.428ms      55.346us           0 B           0 B           0 B           0 B            80  \n",
            "                                          data_transfer         0.00%       0.000us         0.00%       0.000us       0.000us       2.981ms         2.35%       2.981ms     149.048us           0 B           0 B           0 B           0 B            20  \n",
            "                                          aten::dropout         0.20%       1.122ms         2.68%      15.416ms      77.081us       0.000us         0.00%       2.896ms      14.482us           0 B           0 B     270.62 MB     -67.66 MB           200  \n",
            "                                   aten::native_dropout         0.61%       3.487ms         2.49%      14.294ms      71.469us       2.857ms         2.25%       2.896ms      14.482us           0 B           0 B     338.28 MB           0 B           200  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 574.758ms\n",
            "Self CUDA time total: 127.042ms\n",
            "\n",
            "\\n================================================================================\\n\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch:   0%|          | 0/4 [00:00<?, ?it/s]/home/srm2245/hpml-project/pipelines/finBERT/notebooks/../finbert/finbert_profile.py:316: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=self.config.use_amp):\n",
            "Iteration: 100%|██████████| 109/109 [00:01<00:00, 94.15it/s]\n",
            "12/15/2025 14:48:22 - INFO - finbert.utils -   *** Example ***\n",
            "12/15/2025 14:48:22 - INFO - finbert.utils -   guid: validation-1\n",
            "12/15/2025 14:48:22 - INFO - finbert.utils -   tokens: [CLS] our in - depth expertise extends to the fields of energy , industry , urban & mobility and water & environment [SEP]\n",
            "12/15/2025 14:48:22 - INFO - finbert.utils -   input_ids: 101 2256 1999 1011 5995 11532 8908 2000 1996 4249 1997 2943 1010 3068 1010 3923 1004 12969 1998 2300 1004 4044 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/15/2025 14:48:22 - INFO - finbert.utils -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/15/2025 14:48:22 - INFO - finbert.utils -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/15/2025 14:48:22 - INFO - finbert.utils -   label: neutral (id = 2)\n",
            "12/15/2025 14:48:22 - INFO - finbert.finbert -   ***** Loading data *****\n",
            "12/15/2025 14:48:22 - INFO - finbert.finbert -     Num examples = 388\n",
            "12/15/2025 14:48:22 - INFO - finbert.finbert -     Batch size = 32\n",
            "12/15/2025 14:48:22 - INFO - finbert.finbert -     Num steps = 48\n",
            "Validating: 100%|██████████| 13/13 [00:00<00:00, 188.94it/s]\n",
            "Epoch:  25%|██▌       | 1/4 [00:01<00:04,  1.40s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation losses: [1.0584400983957143]\n",
            "No best model found\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Iteration: 100%|██████████| 109/109 [00:01<00:00, 67.71it/s]\n",
            "12/15/2025 14:48:24 - INFO - finbert.utils -   *** Example ***\n",
            "12/15/2025 14:48:24 - INFO - finbert.utils -   guid: validation-1\n",
            "12/15/2025 14:48:24 - INFO - finbert.utils -   tokens: [CLS] our in - depth expertise extends to the fields of energy , industry , urban & mobility and water & environment [SEP]\n",
            "12/15/2025 14:48:24 - INFO - finbert.utils -   input_ids: 101 2256 1999 1011 5995 11532 8908 2000 1996 4249 1997 2943 1010 3068 1010 3923 1004 12969 1998 2300 1004 4044 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/15/2025 14:48:24 - INFO - finbert.utils -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/15/2025 14:48:24 - INFO - finbert.utils -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/15/2025 14:48:24 - INFO - finbert.utils -   label: neutral (id = 2)\n",
            "12/15/2025 14:48:24 - INFO - finbert.finbert -   ***** Loading data *****\n",
            "12/15/2025 14:48:24 - INFO - finbert.finbert -     Num examples = 388\n",
            "12/15/2025 14:48:24 - INFO - finbert.finbert -     Batch size = 32\n",
            "12/15/2025 14:48:24 - INFO - finbert.finbert -     Num steps = 48\n",
            "Validating: 100%|██████████| 13/13 [00:00<00:00, 191.50it/s]\n",
            "Epoch:  50%|█████     | 2/4 [00:03<00:03,  1.67s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation losses: [1.0584400983957143, 0.9962026201761686]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Iteration: 100%|██████████| 109/109 [00:01<00:00, 61.63it/s]\n",
            "12/15/2025 14:48:26 - INFO - finbert.utils -   *** Example ***\n",
            "12/15/2025 14:48:26 - INFO - finbert.utils -   guid: validation-1\n",
            "12/15/2025 14:48:26 - INFO - finbert.utils -   tokens: [CLS] our in - depth expertise extends to the fields of energy , industry , urban & mobility and water & environment [SEP]\n",
            "12/15/2025 14:48:26 - INFO - finbert.utils -   input_ids: 101 2256 1999 1011 5995 11532 8908 2000 1996 4249 1997 2943 1010 3068 1010 3923 1004 12969 1998 2300 1004 4044 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/15/2025 14:48:26 - INFO - finbert.utils -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/15/2025 14:48:26 - INFO - finbert.utils -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/15/2025 14:48:26 - INFO - finbert.utils -   label: neutral (id = 2)\n",
            "12/15/2025 14:48:26 - INFO - finbert.finbert -   ***** Loading data *****\n",
            "12/15/2025 14:48:26 - INFO - finbert.finbert -     Num examples = 388\n",
            "12/15/2025 14:48:26 - INFO - finbert.finbert -     Batch size = 32\n",
            "12/15/2025 14:48:26 - INFO - finbert.finbert -     Num steps = 48\n",
            "Validating: 100%|██████████| 13/13 [00:00<00:00, 207.58it/s]\n",
            "Epoch:  75%|███████▌  | 3/4 [00:05<00:01,  1.83s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation losses: [1.0584400983957143, 0.9962026201761686, 0.9673957320360037]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Iteration: 100%|██████████| 109/109 [00:01<00:00, 61.84it/s]\n",
            "12/15/2025 14:48:28 - INFO - finbert.utils -   *** Example ***\n",
            "12/15/2025 14:48:28 - INFO - finbert.utils -   guid: validation-1\n",
            "12/15/2025 14:48:28 - INFO - finbert.utils -   tokens: [CLS] our in - depth expertise extends to the fields of energy , industry , urban & mobility and water & environment [SEP]\n",
            "12/15/2025 14:48:28 - INFO - finbert.utils -   input_ids: 101 2256 1999 1011 5995 11532 8908 2000 1996 4249 1997 2943 1010 3068 1010 3923 1004 12969 1998 2300 1004 4044 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/15/2025 14:48:28 - INFO - finbert.utils -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/15/2025 14:48:28 - INFO - finbert.utils -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/15/2025 14:48:28 - INFO - finbert.utils -   label: neutral (id = 2)\n",
            "12/15/2025 14:48:28 - INFO - finbert.finbert -   ***** Loading data *****\n",
            "12/15/2025 14:48:28 - INFO - finbert.finbert -     Num examples = 388\n",
            "12/15/2025 14:48:28 - INFO - finbert.finbert -     Batch size = 32\n",
            "12/15/2025 14:48:28 - INFO - finbert.finbert -     Num steps = 48\n",
            "Validating: 100%|██████████| 13/13 [00:00<00:00, 211.04it/s]\n",
            "Epoch: 100%|██████████| 4/4 [00:07<00:00,  1.82s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation losses: [1.0584400983957143, 0.9962026201761686, 0.9673957320360037, 0.9582578631547781]\n"
          ]
        }
      ],
      "source": [
        "start = time.perf_counter()\n",
        "trained_model = finbert.train(train_examples = train_data, model = model)\n",
        "train_wall_s = time.perf_counter() - start"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "12/15/2025 14:48:28 - INFO - finbert.utils -   *** Example ***\n",
            "12/15/2025 14:48:28 - INFO - finbert.utils -   guid: test-1\n",
            "12/15/2025 14:48:28 - INFO - finbert.utils -   tokens: [CLS] the bristol port company has sealed a one million pound contract with cooper specialised handling to supply it with four 45 - ton ##ne , custom ##ised reach stack ##ers from ko ##ne ##cr ##ane ##s [SEP]\n",
            "12/15/2025 14:48:28 - INFO - finbert.utils -   input_ids: 101 1996 7067 3417 2194 2038 10203 1037 2028 2454 9044 3206 2007 6201 17009 8304 2000 4425 2009 2007 2176 3429 1011 10228 2638 1010 7661 5084 3362 9991 2545 2013 12849 2638 26775 7231 2015 102 0 0 0 0 0 0 0 0 0 0\n",
            "12/15/2025 14:48:28 - INFO - finbert.utils -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0\n",
            "12/15/2025 14:48:28 - INFO - finbert.utils -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/15/2025 14:48:28 - INFO - finbert.utils -   label: positive (id = 0)\n",
            "12/15/2025 14:48:28 - INFO - finbert.finbert -   ***** Loading data *****\n",
            "12/15/2025 14:48:28 - INFO - finbert.finbert -     Num examples = 970\n",
            "12/15/2025 14:48:28 - INFO - finbert.finbert -     Batch size = 32\n",
            "12/15/2025 14:48:28 - INFO - finbert.finbert -     Num steps = 120\n",
            "12/15/2025 14:48:28 - INFO - finbert.finbert -   ***** Running evaluation ***** \n",
            "12/15/2025 14:48:28 - INFO - finbert.finbert -     Num examples = 970\n",
            "12/15/2025 14:48:28 - INFO - finbert.finbert -     Batch size = 32\n",
            "Testing: 100%|██████████| 31/31 [00:00<00:00, 181.40it/s]\n",
            "12/15/2025 14:48:29 - INFO - finbert.utils -   *** Example ***\n",
            "12/15/2025 14:48:29 - INFO - finbert.utils -   guid: test-1\n",
            "12/15/2025 14:48:29 - INFO - finbert.utils -   tokens: [CLS] the bristol port company has sealed a one million pound contract with cooper specialised handling to supply it with four 45 - ton ##ne , custom ##ised reach stack ##ers from ko ##ne ##cr ##ane ##s [SEP]\n",
            "12/15/2025 14:48:29 - INFO - finbert.utils -   input_ids: 101 1996 7067 3417 2194 2038 10203 1037 2028 2454 9044 3206 2007 6201 17009 8304 2000 4425 2009 2007 2176 3429 1011 10228 2638 1010 7661 5084 3362 9991 2545 2013 12849 2638 26775 7231 2015 102 0 0 0 0 0 0 0 0 0 0\n",
            "12/15/2025 14:48:29 - INFO - finbert.utils -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0\n",
            "12/15/2025 14:48:29 - INFO - finbert.utils -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/15/2025 14:48:29 - INFO - finbert.utils -   label: positive (id = 0)\n",
            "12/15/2025 14:48:29 - INFO - finbert.finbert -   ***** Loading data *****\n",
            "12/15/2025 14:48:29 - INFO - finbert.finbert -     Num examples = 970\n",
            "12/15/2025 14:48:29 - INFO - finbert.finbert -     Batch size = 32\n",
            "12/15/2025 14:48:29 - INFO - finbert.finbert -     Num steps = 120\n"
          ]
        }
      ],
      "source": [
        "test_data = finbert.get_data(\"test\")\n",
        "\n",
        "results = finbert.evaluate(examples=test_data, model=trained_model)\n",
        "\n",
        "eval_df, eval_timing = timed_eval(\n",
        "    finbert=finbert, model=trained_model, examples=test_data, use_amp=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "def report(df, cols=['label','prediction','logits']):\n",
        "    #print('Validation loss:{0:.2f}'.format(metrics['best_validation_loss']))\n",
        "    cs = CrossEntropyLoss(weight=finbert.class_weights)\n",
        "    loss = cs(torch.tensor(list(df[cols[2]])),torch.tensor(list(df[cols[0]])))\n",
        "    print(\"Loss:{0:.2f}\".format(loss))\n",
        "    print(\"Accuracy:{0:.2f}\".format((df[cols[0]] == df[cols[1]]).sum() / df.shape[0]) )\n",
        "    print(\"\\nClassification Report:\")\n",
        "    return_val = classification_report(df[cols[0]], df[cols[1]], output_dict=True)\n",
        "    print(return_val)\n",
        "    return_val[\"Loss\"] = loss\n",
        "    return_val[\"Accuracy\"] = (df[cols[0]] == df[cols[1]]).sum() / df.shape[0]\n",
        "    return return_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "results['prediction'] = results.predictions.apply(lambda x: np.argmax(x,axis=0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss:0.99\n",
            "Accuracy:0.58\n",
            "\n",
            "Classification Report:\n",
            "{'0': {'precision': 0.3971631205673759, 'recall': 0.20973782771535582, 'f1-score': 0.27450980392156865, 'support': 267.0}, '1': {'precision': 0.3007518796992481, 'recall': 0.625, 'f1-score': 0.40609137055837563, 'support': 128.0}, '2': {'precision': 0.7495559502664298, 'recall': 0.7339130434782609, 'f1-score': 0.7416520210896309, 'support': 575.0}, 'accuracy': 0.5752577319587628, 'macro avg': {'precision': 0.4824903168443513, 'recall': 0.5228836237312056, 'f1-score': 0.47408439852319173, 'support': 970.0}, 'weighted avg': {'precision': 0.593333469274423, 'recall': 0.5752577319587628, 'f1-score': 0.5687873455722356, 'support': 970.0}}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/tmp/ipykernel_368036/3181304599.py:4: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)\n",
            "  loss = cs(torch.tensor(list(df[cols[2]])),torch.tensor(list(df[cols[0]])))\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Accuracy</td><td>▁</td></tr><tr><td>Loss</td><td>▁</td></tr><tr><td>accuracy</td><td>▁</td></tr><tr><td>eval_num_samples</td><td>▁</td></tr><tr><td>eval_samples_per_s</td><td>▁</td></tr><tr><td>eval_wall_s</td><td>▁</td></tr><tr><td>model_size_mb</td><td>▁</td></tr><tr><td>profile_train_steps</td><td>▁</td></tr><tr><td>train_backward_pass_ms</td><td>▁</td></tr><tr><td>train_data_transfer_ms</td><td>▁</td></tr><tr><td>+6</td><td>...</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Accuracy</td><td>0.57526</td></tr><tr><td>Loss</td><td>0.9873</td></tr><tr><td>accuracy</td><td>0.57526</td></tr><tr><td>device</td><td>cuda</td></tr><tr><td>eval_num_samples</td><td>970</td></tr><tr><td>eval_samples_per_s</td><td>6618.10962</td></tr><tr><td>eval_wall_s</td><td>0.14657</td></tr><tr><td>model_dir</td><td>/home/srm2245/hpml-p...</td></tr><tr><td>model_size_mb</td><td>42.62306</td></tr><tr><td>profile_train_steps</td><td>20</td></tr><tr><td>+8</td><td>...</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">training-distilled_bert-profiled</strong> at: <a href='https://wandb.ai/si2449-columbia-university/finbert-experiments/runs/ljuktgop' target=\"_blank\">https://wandb.ai/si2449-columbia-university/finbert-experiments/runs/ljuktgop</a><br> View project at: <a href='https://wandb.ai/si2449-columbia-university/finbert-experiments' target=\"_blank\">https://wandb.ai/si2449-columbia-university/finbert-experiments</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20251215_144723-ljuktgop/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "wandb_report = report(results,cols=['labels','prediction','predictions'])\n",
        "\n",
        "summary = {\n",
        "        \"device\": str(finbert.device),\n",
        "        \"model_dir\": str(cl_path),\n",
        "        \"train_wall_s\": float(train_wall_s),\n",
        "        \"train_examples\": int(len(train_data)),\n",
        "        \"train_examples_per_s\": float((len(train_data) * finbert.config.num_train_epochs) / train_wall_s)\n",
        "        if train_wall_s > 0\n",
        "        else float(\"inf\"),\n",
        "        \"model_size_mb\": float(get_model_size_mb(trained_model)),\n",
        "        \"profile_train_steps\": finbert.config.profile_train_steps,\n",
        "        **(finbert.profile_results.get(\"training_summary\", {}) or {}),\n",
        "        **eval_timing,\n",
        "        **wandb_report,\n",
        "    }\n",
        "\n",
        "wandb.log(summary)\n",
        "wandb.finish()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
